% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% bvar_mapping_12_12.tex

\documentclass[11pt]{article} % use larger type; default would be 10pt


%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or\ldots .
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information


% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{url}
\usepackage{soulutf8}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another\ldots


%%% HEADERS & FOOTERS
%\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
%\pagestyle{fancy} % options: empty , plain , fancy




%%My Additional Packages
\usepackage{mathtext}          % русские буквы в формулах
\usepackage[T1, T2A]{fontenc}            % внутренняя кодировка  TeX
\usepackage[utf8]{inputenc}         % кодировка исходного текста
% \usepackage{cmap}          % русский поиск в pdf
\usepackage[russian, english]{babel} % локализация и переносы
\usepackage{amsmath} % Математические окружения AMS
\usepackage{amsfonts} % Шрифты AMS
\usepackage{amssymb} % Символы AMS
\usepackage{graphicx} % Вставить pdf- или png-файлы

\usepackage{euscript} % Красивый шрифт

\usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

\usepackage[colorinlistoftodos]{todonotes} % [colorinlistoftodos,prependcaption,textsize=tiny]

\usepackage{indentfirst} % Отступ в первом абзаце.
\usepackage{pdflscape} %Переворачивает страницы, удобно для широких таблиц


\usepackage{footnote} %сноски в таблицах
\makesavenoteenv{tabular}
\makesavenoteenv{table}


\usepackage[bibencoding = auto, backend = biber, babel = other,
style = bwl-FU, citestyle = bwl-FU]{biblatex}
\AtEveryBibitem{\clearlist{language}}
\usepackage{csquotes}
\setcounter{page}{3}

\usepackage{microtype}

\addbibresource{bibliobase2.bib}

\usepackage{wrapfig} % Обтекание рисунков текстом

\usepackage{hyperref} % Гиперссылки

\DeclareMathOperator{\etr}{etr}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathbb{E}}
%\DeclareMathOperator{\L}{\mathbb{L}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Var}{\mathbb{V}\mathrm{ar}}
\DeclareMathOperator{\Loss}{\mathbb{L}}

\DeclareMathOperator{\chol}{chol}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cIW}{\mathcal{IW}}
\newcommand{\lag}{\EuScript{L}}

\newcommand{\prior}{\underline}
\newcommand{\post}{\overline}

\newcommand{\eng}[1]{\begin{otherlanguage}{english}#1\end{otherlanguage}}


\let\vec\relax
\DeclareMathOperator{\vec}{vec}


%\usepackage[toc,page]{appendix}


%\renewcommand{\appendixtocname}{Приложения}
%\renewcommand{\appendixpagename}{Приложения}
%\renewcommand{\appendixname}{Приложение}
%%% END Article customizations

%%% The "real" document content comes below\ldots

\title{Картографирование BVAR}
\author{Демешев Б.Б и Малаховская О.А.}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed

\begin{document}

\selectlanguage{russian}

%\maketitle
\section{Введение}

Значение точных прогнозов для проведения макроэкономической политики трудно переоценить. Существование лагов политики приводит к тому, что решения, принятые сегодня, повлияют на экономику только через некоторое время, поэтому фискальным и монетарным властям при принятии решений приходится ориентироваться не на текущие, а на ожидаемые показатели. Точный прогноз макроэкономических показателей, таким образом, является одним из ключевых факторов успешной политики.

В настоящее время основной моделью для прогнозирования макроэкономических временных рядов является модель векторной  авторегрессии (VAR) и её модификации. Использование векторных авторегрессий в макроэкономическом анализе явилось следствием критики активно использовавшихся прежде традиционных эконометрических моделей. В частности, \eng{\cite{sims_1980}}~обратил внимание на необоснованность ограничений, вводимых в рамках традиционных моделей,\footnote{Под традиционными моделями имеются в виду модели, построенные в рамках подхода комиссии Коулза. Их прогнозная способность резко ухудшилась в начале 1970-х годов, т.е. примерно тогда же, когда «исчезла» базовая кривая Филлипса (подробнее об этом см. \eng{\cite{favero_2001}}~и  \cite{malakhovskaya_pekarsky_rus_2012})}~и предложил использовать более простую по построению динамическую модель, основанную на разложении Вольда и не требующую введения никаких ограничений на взаимную динамику переменных "--- VAR.

 Модели этого класса стали широко использоваться как для прогнозирования, так и для структурного анализа благодаря своей логичности и относительной простоте.   Однако для того чтобы правильно отражать динамику фактических временных рядов, VAR часто требуется большое количество лагов, что, в свою очередь, может привести к высоким ошибкам прогноза. Проблема усугубляется тем, что в реальности при проведении политики центральные банки развитых стран ориентируются на большое количество показателей, и VAR малой размерности не может отразить всей информации, доступной центральным банкам, что  означает, что использование моделей высокой размерности потенциально может улучшить качество прогноза. При этом увеличение числа переменных в VAR приводит к тому, что количество оцениваемых параметров, растет нелинейно. Это усугубляет проблему неэффективности оценивания и высоких ошибок прогноза.
Одним из решений этой проблемы стало использование априорной информации относительно распределения параметров и ковариационной матрицы ошибок, т. е. переход от обычных\footnote{В англоязычной литературе обычные VAR (без наложения априорных распределений) называются частотными "--- \eng{frequentist}.} VAR к Байесовским (\eng{Bayesian VAR, BVAR}).

 Исследователи выделяют два преимущества BVAR по отношению к обычным. Во-первых, этот класс моделей предлагает решение проблемы избыточной параметризации и благодаря этому позволяет включать в модель большее количество переменных. При этом априорные веры позволяют снизить неопределенность в распределении параметров модели и улучшить её прогнозные способности. Во-вторых, распространенные в настоящее время  априорные распределения отражают современные представления о  долгосрочной динамике переменных, не проявляющиеся в коротких выборках, обычно используемых для анализа. Это, в свою очередь, также улучшает точность полученных прогнозов. Кроме того, современные компьютеры осуществляют симуляции настолько быстро, что исследователи  более не ограничены необходимостью использования только сопряженных распределений (т.е. распределений, при котором  априорное распределение, функция правдоподобия и апостериорное распределение принадлежат одному классу), позволяющих получить явное аналитическое решение, что, безусловно, увеличивает привлекательность байесовского подхода и способствует его быстрому распространению, в частности, в макроэкономическом анализе (\eng{\cite{karlsson_2012}}).


Наиболее часто цитируемый недостаток байесовского подхода "--- субъективность, с нашей точки зрения, не является существенным. Действительно, изменение априорного распределения влияет на  результаты анализа\footnote{Т.к. плотность апостериорного распределения представляет собой комбинацию плотности априорного распределения и функции правдоподобия. Этот вопрос подробно освещен в следующем разделе.}. Однако обычная частотная VAR (как и любая другая эконометрическая модель) является в неменьшей степени отражением субъективных представлений исследователя. Выбор весьма ограниченного набора переменных в модели, разделение их на эндогенные и экзогенные, определение числа лагов в модели\footnote{На практике для определения количества лагов в частотной VAR, как правило, исследователи ориентируются на информационные критерии. Однако довольно часто разные информационные критерии дают противоречивые результаты. В этом случае предпочтение определенного критерия всем остальным "--- есть тоже субъективное решение.} и т. д. в любом случае отражают собственные представления исследователя о правильной спецификации модели. При этом, в отличие от частотной VAR, в BVAR эта субъективность задается в явном виде с помощью априорных распределений.

К сожалению, несмотря на широкое распространение BVAR в академических статьях, количество практических обзоров этого метода  весьма ограничено. Существующие обзоры \eng{\cite{karlsson_2012}}, \eng{\cite{delnegro_schorfheide_2011}}~и изложение в учебнике \eng{\cite{canova_2007}}~сильно математизированы и едва ли доступны для экономистов без специальной математической подготовки.
 При этом ни в одном из них не содержится достаточно подробной классификации априорных распределений и к большинству из них не прилагается инструкций для реализации предложенных методов в эконометрическом пакете.
Исключениями являются обзоры \eng{\cite{koop_korobilis_2010}}~и \eng{\cite{blake_mumtaz_2012}}, к которым прилагаются коды в среде MATLAB.\footnote{Находящиеся в открытом доступе известные нам коды в различных пакетах для работы с BVAR описаны в Приложении 1.}
Однако \eng{\cite{koop_korobilis_2010}}~не рассматривают ставший весьма популярным метод задания априорного распределения через добавление дополнительных наблюдений, в т. ч. априорное распределение суммы коэффициентов (\eng{sum-of-coefficients prior}) и априорное распределение начального наблюдения (\eng{initial observation prior}).
\eng{\cite{blake_mumtaz_2012}}~используют терминологию, несколько отличающуюся от других работ, а код фактически содержит  пример построения BVAR только с использованием семплирования по Гиббсу. При этом ни в одном из указанных обзоров не рассмотрен достаточно подробно вопрос о прогнозировании с помощью BVAR (а именно это и является обычно целью их построения, по крайней мере, BVAR в сокращенной форме). В российской же академической литературе на данный момент, насколько нам известно, обзоров BVAR вообще нет.

Данный обзор, содержащий подробную классификацию априорных распределений, наиболее популярных при проведении макроэкономических исследований, может быть полезен для экономистов, обладающих ограниченным опытом в области байесовского анализа. Кроме того, к обзору прилагается код в R, в котором используются те же обозначения, что и в тексте работы, и который может быть использован как в учебных, так и в научных целях.

Мы оставляем за рамками данного обзора построение структурных BVAR (SBVAR), BVAR c меняющимися параметрами (TVP-BVAR), BVAR со стохастической волатильностью, а также проблемы выбора переменных при построении BVAR. %К этим вопросам мы вернемся в своих будущих работах. %План обзора следующий: ...





\section{Оценивание BVAR}


\subsection{Байесовская VAR: формулировка модели}
Рассмотрим переменные\footnote{Для удобства читателя обозначения также вынесены в Приложение 2.} $y_{it}$, объединенные в~вектор  $y_{t}=(y_{1t},y_{2t},\ldots, y_{mt})'$ размерности $m$. Векторная авторегрессия в сокращенной форме записывается в~виде:
\begin{equation}
y_t =\Phi_{const}+ \Phi_1 y_{t-1} + \Phi_2 y_{t-2} +\ldots + \Phi_p y_{t-p} + \varepsilon_t,\quad \varepsilon_t\sim \cN(0,\Sigma)
\end{equation}
где $\Phi_{const}=(c_1,\ldots ,c_m)'$ "--- вектор констант размерности $m$, $\Phi_l$ --– авторегрессионные матрицы размерности $m\times m$ при $l=1, \ldots, p$. Вектор $\varepsilon_t$ "--- $m$-мерный вектор ошибок, %с ковариационной матрицей $\E\varepsilon_t \varepsilon _t'=\Sigma$,
некоррелированный с объясняющими переменными.
Группируя матрицы параметров в общую матрицу $\Phi=[\Phi_1 \ldots \Phi_p \; \Phi_{const}]'$ % здесь нужно транспонирование!
и определяя новый вектор $x_t=[ y'_{t-1} \ldots  y'_{t-p} \; 1]'$, получаем VAR записанную в более компактном виде:

\begin{equation}
y_t=\Phi' x_t+\varepsilon_t
\end{equation}
Если же сгруппировать переменные  и шоки следующим образом: $Y=[y_1, y_2,\ldots, y_T]'$, $X=[x_1, x_2,\ldots, x_T]'$, $E=[\varepsilon_1, \varepsilon_2,\ldots, \varepsilon_T]'$, то VAR можно записать как:
\begin{equation}
Y=X\Phi+E\label{var}
\end{equation}

Эта же модель может быть записана в векторизованном виде\footnote{Уравнение (\ref{var2}) системы следует из тождества: $\vec(ABC)=(C\otimes A)\vec (B)$}:
\begin{gather}
%Y=X\Phi+E\\
\vec(Y)=\vec(X\Phi I)+\vec(E)\Leftrightarrow \\
%\vec(Y)=(I_m\otimes X)\vec(Phi)+\vec(E
%\intertext{либо}
y=(I_M\otimes X)\phi + \varepsilon \label{var2}
\end{gather}
где $\varepsilon  \sim \cN(0,\Sigma\ \otimes\ I_T)$ и вектор $\phi=\vec{\Phi}$ имеет размерность $km\times 1$.

Задача байесовского оценивания заключается в~поиске апостериорных распределений параметров $p(\Phi, \Sigma|Y)$ с~использованием функции максимального правдоподобия $p(Y|\Phi, \Sigma)$ и~заданного априорного распределения, $p(\Phi, \Sigma|Y)$. Для этого используется правило Байеса:
\begin{equation}
p(\Phi, \Sigma|Y)=\frac{p(\Phi,\Sigma) p(Y|\Phi,\Sigma)}{p(Y)}
\end{equation}
Т.к. $p(Y)$ не зависит от $\Phi$ и $\Sigma$, то можно записать:
\begin{equation}
p(\Phi, \Sigma|Y)\propto p(\Phi,\Sigma) p(Y|\Phi,\Sigma)
\end{equation}\\
Так как $\varepsilon_t\sim \cN(0,\Sigma)$, то функция правдоподобия задается как:\footnote{Здесь и далее $\etr(\cdot)=\exp(\tr(\cdot))$.}
\begin{equation}
p(Y|\Phi, \Sigma) \propto |\Sigma|^{-T/2}\etr\left\lbrace -\frac{1}{2}  \left[\Sigma^{-1}(Y-X\Phi)'(Y-X\Phi)\right]\right\rbrace\label{likelihood}
\end{equation}
Другая форма записи функции правдоподобия:
\begin{equation}
p(Y|\Phi, \Sigma) \propto |\Sigma|^{-T/2}\etr\left\lbrace -\frac{1}{2}  \left[\Sigma^{-1} \hat E' \hat E\right]\right\rbrace \times \etr\left\lbrace -\frac{1}{2}  \left[\Sigma^{-1}(\Phi-\hat\Phi)'X'X(\Phi-\hat\Phi)\right]\right\rbrace,
\end{equation}
где $\hat E=Y-X\hat\Phi$ и $\hat \Phi=(X'X)^{-1}X'Y$.

В~двух следующих разделах будут разобраны наиболее известные априорные и построенные на их основе апостериорные распределения.


\subsection{Классификация популярных априорных распределений}

Детально каждое из априорных распределений будет описано отдельно, а в~этом разделе будет дана общая схема связи априорных распределений.
Среди самых популярных априорных распределений можно назвать:

\begin{enumerate}

\item Независимое нормальное-обратное Уишарта априорное распределение (\eng{Independent Normal-inverse Wishart prior})


\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma \sim \cIW(\prior S; \prior\nu) \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}


В общем случае выборку из  апостериорного распределения можно получить
по схеме Гиббса.


Частными случаями независимого нормального-обратного Уишарта являются:


\begin{enumerate}

\item Априорное распределение Миннесоты (\eng{Minnesota prior})


\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma = const \\
% \phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}


Получается из независимого нормального-обратного Уишарта при $\prior
S=(\prior \nu-m-1) \cdot \Sigma$ и $\prior \nu \to \infty$. В этом случае
апостериорное распределение выписывается в явной форме. Можно
использовать алгоритм Монте-Карло, сразу генерирующий случайную
выборку из апостериорного распределения, без необходимости периода
«прожига».


Более того, алгоритм симуляции упрощается если матрица $\prior \Xi$
имеет структуру кронекерова произведения $\prior \Xi =  \Sigma \otimes
\prior \Omega $. В этом случае распределение Миннесоты также становится
частным случаем сопряжённого нормального-обратного Уишарта (см. ниже).


\item Независимое нормальное-Джеффриса априорное распределение (\eng{Independent normal Jeffreys prior})


\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma \sim |\Sigma|^{-(m+1)/2} \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}




Получается из независимого нормального-обратного Уишарта при $\prior S
= \prior \nu^{1/m} \cdot I$ и  $\prior \nu \to 0$. Функция плотности обратного Уишарта распределения имеет вид:
\[
p(\Sigma) = \frac{1}{\Gamma_m(\prior\nu/2)}
|\prior S|^{v/2} |\Sigma|^{-(\prior\nu+m+1)/2}2^{-\prior\nu m/2}
\etr\left( -\frac{1}{2} \prior S\Sigma^{-1} \right)
\]

Если $\prior S= \nu^{1/m} \cdot I$ и $\prior \nu \to 0$, то одновременно $|\prior S|^{\nu} \to 1$ и $\etr\left( -\frac{1}{2} \prior S\Sigma^{-1} \right) \to 1$. Поэтому:

\[
p(\Sigma) \to const \cdot |\Sigma|^{-(m+1)/2}
\]


В данном случае распределение Джеффриса является несобственным, то есть интеграл под всей функцией плотности невозможно отнормировать так, чтобы он равнялся единице. Тем не менее апостериорное распределение будет собственным при достаточном ($T>m-1$) количестве наблюдений  (\eng{\cite{alvarez_2014}}).


Для получения выборки из~апостериорного распределения можно
использовать схему Гиббса. Необходимые формулы для гиперпараметров
апостериорного распределения получаются из общего случая просто
подстановкой $\prior S=0$, $\prior \nu =0$.

Распределение Миннесоты и независимое нормальное-Джеффриса являются
противоположными крайностями независимого-обратного Уишарта. В
распределении Миннесоты матрица $\Sigma$ предполагается известной, а в
нормальном-Джеффриса матрица $\Sigma$ имеет «размытое» неинформативное
распределение.



Частным случаем независимого нормального-Джеффриса распределения является:


\begin{enumerate}

\item Неинформативное-Джеффриса априорное распределение (\eng{Diffuse Jeffreys prior})

\begin{equation}
\begin{cases}
\phi \sim 1 \\
\Sigma \sim |\Sigma|^{-(m+1)/2} \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}

\end{enumerate}

При задании этого априорного распределения не нужно указывать ни одного
гиперпараметра. Оно получается из независимого нормального-Джеффриса при $\prior\phi=0$ и $\prior\Xi= a\cdot I$ и $a\to \infty$ и является также частным случаем сопряженного нормального-Джеффриса априорного распределения.


Для получения выборки из~апостериорного распределения можно
использовать прямое симулирование по схеме Монте-Карло без алгоритма
Гиббса. % Необходимые формулы для гиперпараметров апостериорного
Распределение получается из общего случая просто подстановкой $\prior
S=0$, $\prior \nu =0$, $\prior \Xi^{-1}=0$, $\prior\phi=0$. При этом
формулы существенно упрощаются, в частности исчезает необходимость
обращать матрицу размера $km\times km$.


\end{enumerate}

\item Сопряженное нормальное-обратное Уишарта распределение (\eng{Conjugate normal-inverse Wishart prior})


\begin{equation}
\begin{cases}
\phi|\Sigma\sim \cN (\prior \phi, \Sigma\otimes\prior \Omega)\\
\Sigma\sim \cIW(\prior S, \prior \nu)
\end{cases}
\end{equation}


Для сопряженного нормального-обратного Уишарта распределения нет
необходимости использовать алгоритм Гиббса, так как существуют выведенные формулы
параметров апостериорного распределения. Это позволяет использовать алгоритм
Монте-Карло, сразу генерирующий случайную выборку из апостериорного
распределения, что исключает период «прожига», необходимый для сходимости
алгоритма Гиббса.


Частными случаями сопряжённого нормального-обратного Уишарта
распределения являются:

\begin{enumerate}

\item Распределение Миннесоты при  $\prior \Xi =  \Sigma \otimes
\prior \Omega $.



При $\prior \Xi =  \Sigma \otimes \prior \Omega $ распределение
Миннесоты является одновременно и частным случаем независимого
нормального-обратного Уишарта, и сопряженного нормального-обратного
Уишарта. Для получения выборки из апостериорного распределения можно
использовать и схему Гиббса для независимого нормального-обратного
Уишарта, и алгоритм Монте-Карло без «прожига».



\item Сопряжённое нормальное-Джеффриса априорное распределение (\eng{Conjugate normal Jeffreys prior})


\begin{equation}
\begin{cases}
\phi | \Sigma \sim \cN(\prior\phi; \Sigma \otimes \prior\Omega ) \\
\Sigma \sim |\Sigma|^{-(m+1)/2}
\end{cases}
\end{equation}




Это распределение является частным случаем сопряжённого нормального-обратного Уишарта при $\prior S
= \prior \nu^{1/m}\cdot I$ и  $\prior \nu \to 0$.  Формулы для
гиперпараметров апостериорного распределения получаются подстановкой
$\prior\nu=0$, $\prior S=0$.


Частным случаем сопряжённого нормального-Джеффриса является

\begin{enumerate}

\item Неинформативное-Джеффриса (\eng{Diffuse Jeffreys prior})


Оно получается из сопряженного нормального-Джеффриса при $\prior\phi=0$ и
$\prior\Omega= a\cdot I$ и $a\to \infty$. Формулы для гиперпараметров
апостериорного распределения получаются подстановкой $\prior\phi=0$,
$\prior\Omega^{-1}=0$, $\prior\nu=0$, $\prior S=0$.

\end{enumerate}



\end{enumerate}


\end{enumerate}


В нашей работе мы всегда явно указываем, идёт ли речь о независимом
или сопряжённом априорном распределении. Однако при чтении многих
других работ надо быть внимательным, зачастую авторы говорят о
«нормальном-обратном Уишарта» распределении, не уточняя, какое  именно имеется
в виду.

\subsection{Оценка моделей с различными априорными распределениями}
\subsubsection{Априорное распределение Миннесоты}


Решение проблемы избыточной идентификации на основе байесовских методов было предложено в работе \eng{\cite{litterman_1979}}, автор которой показал, что введение ограничений в форме априорных распределений параметров увеличивает точность оценок и прогнозов. Априорное распределение, получившее название «априорное распределение Миннесоты>> было предложено в работе  \eng{\cite{litterman_1986}}~и (с некоторыми модификациями) в \eng{\cite{doan_al_1984}}.

%Хотя априорное распределение Миннесоты и является частным случаем независимого нормального-обратного Уишарта распределения, имеет смысл рассмотреть его отдельно. Оно было предложено в работе  \eng{\cite{litterman_1986_forecasting}}~и \eng{\cite{doan_litterman_sims_1984_forecasting}}.

%Изначально \cite{litterman_1986_forecasting} предложил использовать априорное распределение Миннесоты, при котором в явном виде учитывается предпосылка о нестационарности большинства макрорядов.

Априорное распределение параметров предполагается многомерным нормальным, зависящим от нескольких гиперпараметров. Параметры предполагаются независимыми, следовательно, их ковариационная матрица $\Xi$ диагональна. Ковариационная матрица вектора $\varepsilon_t (\Sigma)$  также предполагается диагональной и постоянной.
Тогда вектор $\phi$ не зависит от $\Sigma$:
\begin{equation}
\phi\sim \cN(\prior \phi, \prior \Xi)
\end{equation}

Априорная плотность распределения $\phi$  может быть записана как:
\begin{equation}
p(\phi)=\frac{1}{(2\pi)^{km/2}|\prior\Xi|^{1/2}} \exp \left\lbrace-\frac{1}{2}(\phi-\prior \phi)'\prior\Xi^{-1}(\phi-\prior \phi ) \right\rbrace.
\end{equation}

Комбинируя её с функцией правдоподобия \eqref{likelihood}, получаем, что апостериорное распределение параметров задается в следующем виде:
\begin{equation}
\phi|Y\sim \cN(\post{\phi},\post \Xi)
\end{equation}

где
\begin{align*}
&\post \Xi=[\prior \Xi^{-1}+\Sigma^{-1}\otimes(X'X)]^{-1}\\
& \post \phi=\post{\Xi}[\prior \Xi^{-1}\prior \phi+(\Sigma^{-1}\otimes X')y].
\end{align*}

Если $\prior\Xi$ имеет структуру кронекерова произведения: $\prior\Xi = \Sigma \otimes \prior{\Omega}$, то формулы можно существенно упростить и обойтись обращением матриц меньшей размерности:

\begin{multline}
\post \Xi=[\prior \Xi^{-1}+\Sigma^{-1}\otimes(X'X)]^{-1}=[(\Sigma \otimes \prior\Omega)^{-1}+\Sigma^{-1}\otimes(X'X)]^{-1}=\\
=[\Sigma^{-1}\otimes  \prior\Omega^{-1} +\Sigma^{-1}\otimes(X'X)]^{-1}=\Sigma \otimes (\prior\Omega^{-1}+ X'X)^{-1} = \Sigma \otimes \post \Omega
\end{multline}

В результате получаем:

\[
\Phi | Y \sim \cN (\post\Phi, \Sigma \otimes \post\Omega)
\]

На практике в качестве матрицы $\Sigma$ используют её оценку $\hat\Sigma$, диагональные элементы которой равны: $\hat\sigma_{1}^2, \hat\sigma_{2}^2,\ldots,\hat\sigma_{m}^2 $, где  $\hat\sigma_{i}^2$ "--- оценка дисперсии случайной составляющей в $AR(p)$ модели для ряда $i$. При этом некоторые авторы для подсчета оценки дисперсии используют AR(1) модель, даже если сама VAR имеет большее количество лагов.

Математическое ожидание априорного распределения параметров может быть записано с помощью матрицы $\prior \Phi = \E (\Phi)$ размерности $k\times m$, где $\prior\Phi=[\prior\Phi_1 \ldots \prior\Phi_p \; \prior\Phi_{const}]'$ и $\prior \phi = \vec{ \prior \Phi}$.


\begin{equation}
(\prior\Phi_l)_{ij}=
\begin{cases}
\delta_i\; \mbox{ если } i=j, l=1;\\
0,\;\text{ в остальных случаях }
\end{cases}\label{minnesota_exp}
\end{equation}

%%% здесь про дельта i и кто их как берет

Распределение Миннесоты было задумано таким образом, чтобы учесть нестационарность многих макроэкономических временных рядов. В этом случае $\delta_{i}$ принимают значение единица.

В настоящее время широкое распространение получила практика назначать $\delta_{i}=1$ для нестационарных рядов и $\delta_{i}<1$ для стационарных.

Распределение Миннесоты предполагает, что априорная ковариационная матрица параметров $\prior \Xi$ диагональна. Диагональ матрицы $\prior \Xi$ разбивается на блоки  $\prior \Xi_1,\prior \Xi_2, \ldots, \prior \Xi_m$ размерности $k\times k$. В свою очередь, каждый блок $\prior \Xi_i$, $i=1,\ldots,m$ может быть разбит на диагональные подблоки размерности $m\times m$: $\prior \Xi_{i,lag=l}, l=1,\ldots, p$ c константой $\prior \Xi_{i,const}$  в конце главной диагонали:

% $\prior \Xi_i$ обозначает блок $\prior \Xi$, размера $k\times k$, связанный с коэффициентами уравнения $i$, т.е.

\begin{equation*}
\prior \Xi=\begin{pmatrix}
\prior \Xi_1&0_{k\times k}&\cdots&0_{k\times k}&0_{k\times k}\\
0_{k\times k}& \prior \Xi_2& \cdots &0_{k\times k}&0_{k\times k}\\
\vdots &\vdots& \ddots&\vdots& \vdots\\
0_{k\times k}&0_{k\times k}&\cdots&\prior\Xi_{m-1} & 0_{k\times k}\\
0_{k\times k}&0_{k\times k}&\cdots&0_{k\times k}&\prior \Xi_m
\end{pmatrix}
\quad
\prior \Xi_i=\begin{pmatrix}
\prior \Xi_{i,lag=1}&0_{m\times m}&\cdots&0_{m\times m}&0_{m\times 1}\\
0_{m\times m}& \prior \Xi_{i,lag=2}& \cdots &0_{m\times m}&0_{m\times 1}\\
\vdots &\vdots& \ddots& \vdots&\vdots\\
0_{m\times m}&0_{m\times m}&\cdots&\prior \Xi_{i,lag=p}&0_{m\times 1}\\
0_{1\times m}&0_{1\times m}&\cdots&0_{1\times m}&\prior \Xi_{i,const}
\end{pmatrix}
\end{equation*}

Тогда  диагональные элементы  $\prior \Xi_{i,lag=l}$ определяются по формулам:
\begin{equation}
(\prior \Xi_{i,lag=l})_{jj}
=\begin{cases}
\left(\frac{\lambda_{tight}}{l^{\lambda_{lag}}}\right)^2,\ j=i\\
\left(\frac{\lambda_{tight}\cdot\lambda_{kron}\sigma_i}{l^{\lambda_{lag}}\sigma_j}\right)^2, \; j\ne i
\end{cases}\\
\prior \Xi_{i,const}
=\lambda_{tight}^2\lambda_{const}^2\sigma_i^2,
\label{minnesota_variance}
\end{equation}

%\begin{itemize}
%\item $\frac{\lambda_1}{l^2}$ "--- для переменной $i$ и лага $l$
%\item $\frac{\lambda_1\lambda_2\sigma^2_i}{l^2\sigma^2_j}$ "--- для переменной $j\neq i$ и лага $l$
%\item $\lambda_3\sigma_i^2 \label{minnesota_variance}$ "--- для коэффициентов при экзогенных переменных
%\end{itemize}

Как можно видеть из приведенной выше формулы \eqref{minnesota_variance} априорная дисперсия параметров зависит от нескольких гиперпараметров, задаваемых исследователем. Гиперпараметры имеют следующую интерпретацию:\footnote{Соотношение гиперпараметров, определяющих дисперсию в различных работах, можно найти в Приложении 3.} $\lambda_{tight}$ (параметр регуляризации) отражает общую «жесткость» априорного распределения.

Если $\lambda_{tight}\to 0$, то априорное распределение совпадает с апостериорным распределением, и данные не играют никакой роли при оценке параметров. В этом случаем мы считаем, что доподлинно знаем параметры, то есть:

\[
\Phi \sim \cN (\prior \Phi, 0), \; \Phi |Y \sim \cN (\prior \Phi, 0)
\]


Если $\lambda_{tight}\to \infty$, то апостериорное математическое ожидание параметров сходится к МНК-оценке. В этом случае $\prior \Xi^{-1} = 0$, поэтому
\[
\post \Xi = (\Sigma^{-1} \otimes (X'X))^{-1} = \Sigma \otimes (X'X)^{-1}
\]

Отсюда
\begin{multline}
\post\phi = 0 + (\Sigma \otimes (X'X)^{-1}) \cdot (\Sigma^{-1} \otimes X' )\cdot y=
(I \otimes (X'X)^{-1}X') \cdot \vec(Y) =\\
= \vec((X'X)^{-1}X'Y\cdot I')= \vec((X'X)^{-1}X'Y)
\end{multline}

%\todo[inline]{У нас $\lambda_{const}$ не домножается на $\lambda_{tight}$, поэтому косяк вообще-то говоря!}

Параметр кросс-регуляризации $\lambda_{kron}$ добавляет дополнительную жесткость лагам других переменных по сравнению с лагами зависимой переменной. Если  $\lambda_{kron}<1$, то собственные лаги зависимой переменной помогают предсказывать значение переменной лучше, чем лаги других переменных, поэтому коэффициенты при лагах других переменных оказываются жестче регуляризованы к нулю.

При $\lambda_{kron}=1$ матрица $\prior \Xi$ имеет структуру кронекерова произведения и представима в виде:
\[
\prior \Xi = \Sigma \otimes \prior \Omega,
\]
где $\prior \Omega$ "--- матрица размера $k\times k$, соответствующая отдельному уравнению. Кронекерово домножение слева на матрицу $\Sigma$ для $i$-го уравнения означает домножение дисперсий, указанных в матрице $\prior \Omega$, на коэффициент $\sigma^2_i$. Сама матрица $\prior \Omega$ представима в виде:


\begin{equation}
\prior \Omega=\begin{pmatrix} \label{prior_omega1}
\prior \Omega_{lag=1}&0_{m\times m}&\cdots&0_{m\times m}&0_{m\times 1}\\
0_{m\times m}& \prior\Omega_{lag=2}& \cdots &0_{m\times m}&0_{m\times 1}\\
\vdots &\vdots& \ddots&\vdots& \vdots\\
0_{m\times m}&0_{m\times m}&\cdots&\prior\Omega_{lag=p} & 0_{m\times 1}\\
0_{1\times m}&0_{1\times m}&\cdots&0_{1\times m}&\prior \Omega_{const}
\end{pmatrix}
\end{equation}

При этом матрица $\prior \Omega_{lag=l}$ имеет размерность $m\times m$, и её диагональные элементы определяются по формулам:
\begin{equation}
(\prior \Omega_{lag=l})_{jj} \label{prior_omega2}
=\left(\frac{\lambda_{tight}}{l^{\lambda_{lag}}\sigma_j}\right)^2
\quad
\prior \Omega_{const}=\lambda_{tight}^2\lambda_{const}^2
\end{equation}

Параметр $\lambda_{const}$ отражает относительную жесткость распределения константы, а параметр $\lambda_{lag}$ отвечает за то, насколько быстро убывает априорная дисперсия с увеличением номера лага.

Как уже говорилось выше, при использовании априорного распределения Миннесоты, нет необходимости применять алгоритм Гиббса для получения апостериорного распределения. Алгоритм генерации случайной выборки непосредственно из апостериорного распределения происходит методом Монте-Карло: \label{alg:alg1}
\begin{enumerate}
\item На $s$-ом шаге сгенерировать очередную итерацию согласно:
\begin{equation}
\phi^{[s]}\sim \cN(\post \phi; \post \Xi)
\end{equation}
\item Увеличить $s$ на единицу и перейти к пункту 1
\end{enumerate}

Если $\prior \Xi$ имеет структуру кронекерова произведения: $\prior \Xi = \Sigma \otimes \prior \Omega$, то вместо вектора $\phi^{[s]}$ можно генерировать матрицу $\Phi^{[s]}$ численно более простым алгоритмом:


\begin{enumerate}
\item Сгенерировать матрицу $V$ размера $k\times m$  из независимых стандартных нормальных величин
\item Посчитать матрицу $\Phi^{[s]}$ по формуле:
\begin{equation}
\Phi^{[s]} = \post \Phi + \chol(\post\Omega) \cdot V \cdot \chol(\Sigma^{[s]})',
\end{equation}
где $\chol(\post\Omega)$ и  $\chol(\Sigma^{[s]})$ "--- верхнетреугольные матрицы, полученные путем разложения Холецкого матриц $\post\Omega$ и $\Sigma^{[s]}$ соответственно.
\end{enumerate}

Можно выделить несколько преимуществ априорного распределения. Прежде всего, оно просто задается, Кроме того, оно успешно применялось в литературе для решения различных задач. И наконец, получившееся апостериорное распределение является нормальным, а значит, легко можно получить значение любой функции параметров с помощью методов Монте-Карло. Однако существенным недостатком этого распределения является то, что оно не предполагает использования байесовской процедуры для оценки ковариационной матрицы $\Sigma$.




\subsubsection{Сопряженное нормальное-обратное Уишарта априорное распределение}

Недостатка априорного распределения Миннесоты, состоящего в отсутствии байесовской процедуры для  оценки матрицы $\Sigma$, можно избежать, если рассматривать сопряженное априорное распределение. При известной ковариационной матрицы остатков функция правдоподобия может быть разбита на две части: часть, соответствующую нормальному распределению, и часть, соответствующую обратному распределению Уишарта. Поэтому сопряженным априорным распределением для рассматриваемой модели будет также нормальное-обратное Уишарта распределение.

Сопряженное нормальное-обратное Уишарта априорное распределение может быть записано как:
\begin{equation}
\begin{cases} \label{conjugate_prior}
\Sigma\sim \cIW(\prior S, \prior \nu) \\
\phi|\Sigma\sim \cN (\prior \phi, \Sigma\otimes\prior \Omega)
\end{cases}
\end{equation}

В отличие от распределения Миннесоты, система \eqref{conjugate_prior} записана для случая, когда ковариационная матрица параметров имеет кронекерову структуру, то есть $\lambda_{kron}$ полагается равной единице.

%Хотя строго, говоря, это предположение не обязательно, оно существенно ускоряет расчеты, поэтому, как правило, используется в макроэкономических  приложениях.

Гиперпараметры вектора математического ожидания $(\prior \phi)$ и ковариационной матрицы $\prior \Omega$ условного априорного распределения  могут быть заданы точно так же, как и в случае распределения Миннесоты для  случая $\lambda_{kron} =1$ (см. \eqref{minnesota_exp},\eqref{prior_omega1} и\eqref{prior_omega2}).
$\prior S$ выбирается так, чтобы среднее $\Sigma$ совпадало с фиксированной ковариационной матрицей $\Sigma$ в априорном распределении Миннесоты. Т.к. математическое ожидание и дисперсия параметров имеют вид\footnote{Правая часть \ref{var3} следует из того, что:
%\begin{align}
$\Var(\phi)=\Var(\E(\phi|\Sigma))+\E(\Var(\phi|\Sigma))=\Var(\prior\phi)+\E(\Sigma\otimes\prior\Omega)=\\=\E(\Sigma\otimes\prior\Omega)=\E(\Sigma)\otimes\prior\Omega=(\nu-m-1)^{-1}\prior S\otimes \prior\Omega$
%\end{align}
}:
\begin{equation}
\E(\phi)=\prior \phi \quad \Var(\phi)=(\prior \nu - m -1)^{-1}(\prior S\otimes \prior \Omega),\label{var3}
\end{equation}
то диагональные элементы $\prior S$ выбираются следующим образом:
\begin{equation}
(\prior S)_{ii}= (\prior \nu- m- 1) \hat\sigma^2_{i}
\end{equation}
Выбор степеней свободы обратного Уишарта распределения $\prior\nu$ в соответствии с:
\begin{equation}
\prior \nu \geq\max\lbrace m+2, m+2h-T\rbrace
\end{equation}
обеспечивает существование как априорной дисперсии параметров, так и апостериорной дисперсии прогнозов на горизонте $h$ (см. \eng{\cite{kadiyala_karlsson_1997}}).

Можно показать, что с учетом функции правдоподобия \eqref{likelihood} апостериорное распределение принадлежит тому же классу (см, например, \eng{\cite{zellner_1996}}):
\begin{equation}
\begin{cases}
\Sigma|Y \sim \cIW(\post S, \post \nu) \\
\Phi|\Sigma,Y\sim \cN (\post \Phi, \Sigma\otimes\post \Omega)
\end{cases}
\end{equation}

где гиперпараметры апостериорного распределения задаются как:

\begin{align*}
\post\nu &=\prior \nu+T\\
\post{\Omega}&=(\prior \Omega^{-1}+X'X)^{-1}\\
\post \Phi&=\post{\Omega}\cdot (\prior \Omega^{-1}\prior \Phi+X'Y)\\
\post S&=\prior S +\hat E'\hat E+\hat \Phi'
 X'X \hat \Phi +\prior \Phi'\prior\Omega^{-1}\prior \Phi-\post \Phi'\post\Omega^{-1}\post \Phi\\&=\prior S +\hat E'\hat E + (\prior \Phi - \hat \Phi)'(\prior \Omega + (X'X)^{-1})^{-1}(\prior \Phi - \hat \Phi), \text{ где:} \\
&\hat\Phi=(X'X)^{-1}X'Y \text{ и } \hat E=Y-X\hat\Phi
\end{align*}


Существует достаточно популярный альтернативный подход для подсчёта гиперпараметров апостериорного распределения.


Мы обнуляем матрицы  $\prior S$ и $\prior \Omega^{-1}$, при этом матрица $(\prior \Omega + (X'X)^{-1})^{-1}$ оказывается равной $0$, а матрица $\prior \Phi$ исчезает из формул. Чтобы компенсировать разницу добавляем дополнительные наблюдения в матрицу $X$ и в матрицу $Y$:

\[
X^*=\begin{bmatrix}
X^{+} \\
X
\end{bmatrix}, \;
Y^*=\begin{bmatrix}
Y^{+} \\
Y
\end{bmatrix}
\]



При добавлении наблюдений матрицы скалярных произведений $X^{*\prime}X^*$ и $X^{*\prime}Y^*$ разлагаются в сумму: $X^{*\prime}X^*=X^{+\prime}X^+ + X'X$, $X^{*\prime}Y^*=X^{+\prime}Y^+ + X'Y$. В частности, добавление нулевых искусственных наблюдений никак не изменяет матрицы $X'X$, $X'Y$ и $Y'Y$. Отметим, что матрицы $X$ и $Y$ входят в гиперпараметры апостериорного распределения только в составе матриц $X'X$, $X'Y$ и $Y'Y$, поэтому абсолютно не важно, в каком порядке добавлять искусственные наблюдения и каким образом по отношению к матрицам $X$ и $Y$. Можно их добавить в конец матриц $X$ и $Y$, можно в начало, можно посередине.

Получим новые формулы для апостериорных гиперпараметров:

\begin{align*}
&\post\nu =\prior \nu+T\\
&\post{\Omega}=(X^{*\prime}X^*)^{-1}=(X^{+\prime}X^+ + X'X)^{-1}\\
&\post \Phi=\post{\Omega}\cdot (X^{*\prime}Y^*)=\post{\Omega}\cdot (X^{+\prime}Y^+ + X'Y)= (X^{*\prime}X^*)^{-1}X^{*\prime}Y^*\\
&\post S=\hat E^{*\prime}\hat E^*\\
&\hat E^*=Y^*-X^*\post\Phi
\end{align*}

%\begin{align*}
%&\post\nu = \prior \nu  + T \\
%&\post S= E^{*\prime}E^*, \text{ где } E^*=Y^*-X^*\Phi \\
%&\post \Xi = ( \Sigma^{-1}\otimes X^{*\prime}X^*)^{-1} \\
%&\post \phi = \post \Xi \cdot ( \vec(X^{*\prime}Y^*\Sigma^{-1}))
%\end{align*}

Наблюдения добавляются так, чтобы гиперпараметры апостериорных наблюдений не изменились. Для этого необходимо, чтобы

\begin{equation}
\begin{cases}
X^{+\prime}X^+=\prior \Omega^{-1} \\
X^{+\prime}Y^+=\prior \Omega^{-1}\prior \Phi \\
(Y^+-X^+\prior \Phi)'(Y^+-X^+ \prior \Phi) = \prior S
\end{cases}
\end{equation}

Заметим, что взаимосвязь «новых» формул  c матрицами, определяющими априорное и апостериорное распределения, может быть представлена в виде таблицы:

%\begin{comment}
\begin{center}
\begin{table}[h!]
\begin{tabular}{lll}
\toprule
& Интерпретация& Формула\\
\midrule
 $\prior \Phi$ & оценки коэффициентов регрессий $Y^+$ на $X^+$&
 $(X^{+\prime}X^+)^{-1}\cdot (X^{+\prime}Y^+)= \prior \Phi$ \\
$\prior S$ &скалярные произведения остатков этих регрессий&
$\hat E^{+\prime} \hat E^+, \text{ где } \hat E^+=Y^+-X^+\prior \Phi
$\\
$\prior \Omega^{-1}$ & скалярные произведения регрессоров из $X^+$&
$X^{+\prime}X^+$\\
\midrule
$\post \Phi$ & оценки коэффициентов регрессий  $Y^*$ на $X^*$&
$(X^{*\prime}X^*)^{-1}\cdot (X^{*\prime}Y^*)= \post \Phi$\\
$\post S$ & скалярные произведения остатков этих регрессий&
$\hat E^{*\prime} \hat E^*, \text{ где } \hat E^*=Y^*-X^*\post\Phi$\\
$\post \Omega^{-1}$ &скалярные произведения регрессоров из $X^*$&
$X^{*\prime}X^*$\\
\bottomrule

\end{tabular}
\end{table}
\end{center}

%\end{comment}

Указанные условия будут выполнены если добавить наблюдения по схеме\footnote{Аналогичные формулы, приведенные в работах \eng{\cite{banbura_al_2010}}, \eng{\cite{berg_henzel_2013}}~для задания сопряженного нормального-обратного Уишарта априорного распределения являются частным случаем \eqref{NIW_observations} для $\lambda_{lag}=1$ и $\lambda_{const}\to\infty$.}:

%По аналогии с  работами  \eng{\cite{banbura_al_2010}}, \eng{\cite{berg_henzel_2013}}~соответствующее априорное распределение вводится путем добавления искусственных наблюдений
\begin{equation}
Y^{NIW}=\begin{bmatrix}
\frac{\diag(\delta_1\sigma_1,\ldots, \delta_m\sigma_m)}{\lambda_{tight}}\\
0_{m(p-1)\times m}\\
\diag(\sigma_1,\ldots,\sigma_m)\\
0_{1\times m}
\end{bmatrix}
\quad
%
X^{NIW}=\begin{bmatrix}
\frac{\diag(1,2^{\lambda_{lag}},\ldots, p^{\lambda_{lag}})\otimes \diag(\sigma_1,\ldots,\sigma_m)}{\lambda_{tight}} & 0_{mp\times 1} \\
0_{m\times mp}&0_{m\times 1}\\
0_{1\times mp}&\frac{1}{\lambda_{tight}\lambda_{const}}
\end{bmatrix}\label{NIW_observations}
\end{equation}



%При этом математические ожидания и дисперсии априорного распределения параметров могут быть заданы по тому же принципу, что и в априорном распределении Миннесоты (см. \eqref{minnesota_exp}-\eqref{minnesota_variance}).
%При этом априорные распределения параметров, указанные в \eqref{minnesota_exp}-\eqref{minnesota_variance}), предполагают наличие  нулевой априорной корреляции между коэффициентами.



\subsubsection*{Модификации априорного распределения}

В работах \eng{\cite{doan_al_1984}}~и \eng{\cite{sims_1993}}~было предложено добавить к априорному распределению дополнительную характеристику, введение которой обуславливается возможным наличием во временных рядах единичных корней и коинтеграционных  соотношений. Это позволяет исключить появление неправдоподобно большой доли внутривыборочной дисперсии, объясняемой экзогенными переменными (\eng{\cite{carriero_al_2015}}). %%% ???

Априорное распределение суммы коэффициентов (\eng{sum-of-coefficients prior}) было предложено в работе  \eng{\cite{doan_al_1984}}. Это распределение отражает следующую идею: если переменные в VAR имеют единичный корень, то можно учесть эту информацию, задав априорное распределение, в котором сумма коэффициентов при всех лагах зависимой переменной равна единице (см. \eng{\cite{robertson_tallman_1999}}~и \eng{\cite{blake_mumtaz_2012}}). Другими словами, среднее значение лагированных значений какой либо переменной является хорошим прогнозом  для будущих значений этой переменной.



Внедрение этого априорного распределения производится путем добавления искусственных дамми-наблюдений по следующей схеме:
%\begin{gather*}
%y_d(i,j)=\begin{cases}
%\post y_{0i}/\lambda_3, \quad \text{если }i=j\\
%0 \quad \text{в обратном случае}
%\end{cases}
%x_d(i,s)=\begin{cases}
%\post y_{0i}/\lambda_3,\quad \text{если }i=j, s<km\\
%0 \quad \text{в обратном случае,}
%\end{cases}
%\end{gather*}
%где $i,j=1,\ldots,m$, $s=1, \ldots, km$. Когда $\lambda_3\to 0$, модель стремится к виду, предполагающему запись в разностях, т.е. единичных корней становится столько же, сколько переменных, и нет коинтеграции.


\begin{gather}
Y^{SC}=\frac{1}{\lambda_{sc}}\begin{bmatrix}\diag(\delta_1\mu_1,\ldots,\delta_m\mu_m)\end{bmatrix}\\
X^{SC}=\frac{1}{\lambda_{sc}}\begin{bmatrix}(1_{1\times p})\otimes \diag(\delta_1\mu_1,\ldots,\delta_m\mu_m) &0_{m\times 1}\end{bmatrix},
\end{gather}

где $(1_{1\times p})$ "--- вектор-строка из единиц длиной $p$, $\mu_i$ есть $i$-ая компонента вектора $\mu$, который состоит из средних начальных значений всех переменных\footnote{Некоторые авторы для  расчета $\mu$ усредняют все наблюдения в выборке: $\mu=\frac{1}{T}\sum_{t=1}^T y_t$ (см. \eng{\cite{banbura_al_2010}} и \eng{\cite{carriero_al_2015}}). Однако, в соответствии с работой \eng{\cite{sims_zha_1998}}~для расчета среднего следует использовать только первые $p$ наблюдений.}: $\mu=\frac{1}{p}\sum_{t=1}^p y_t$

Априорное распределение начального наблюдения (\eng{dummy initial observation prior}), предложенное в работе \eng{\cite{sims_1993}}~отражает априорную веру в то, что переменные имеют общий стохастический тренд. Для этого вводится единственное дамми-наблюдение, т. ч. все значения всех переменных равны соответствующему среднему начальных значений $\mu_i$ с точностью до коэффициента масштаба $\lambda_{io}$:
\begin{gather}
Y^{IO}=\frac{1}{\lambda_{io}}\begin{bmatrix}
\delta_1\mu_1,\ldots,\delta_m\mu_m
\end{bmatrix}\\
X^{IO}=\frac{1}{\lambda_{io}}\begin{bmatrix}
(1_{1\times p})\otimes (\delta_1\mu_1,\ldots,\delta_m\mu_m) &1
\end{bmatrix},
\end{gather}

Это априорное распределение предполагает, что среднее по каждой переменной есть линейная комбинация всех остальных средних.
%%% Непонятно, нужны ли здесть дельты. У Карьеро их нет, но у него и в sum-of-coefficients их нет, а у Банбуры в sum-of-coefficients есть, f initial ibservation они не вводят



%\begin{gather*}
%y_d(j)=\begin{cases}\post y_{0i}/\lambda_4 \qquad
%\post y_{0i}/\lambda_4\quad \text{если }s<km\\
%1/\lambda_4 \quad \text{в обратном случае,}
%\end{cases}
%\end{gather*}
%где $j=1,\ldots m, s=1,\ldots km$.

Гиперпараметр $\lambda_{io}$ отражает жесткость указанного априорного распределения. Когда $\lambda_{io}\to 0 $,  модель принимает вид, в котором либо все переменные стационарны со средним, равным выборочному среднему начальных условий, либо нестационарны без дрейфа и коинтегрированны.


Как и в случае распределения Миннесоты, необходимости использовать алгоритм Гиббса нет, можно генерировать случайную выборку непосредственно из апостериорного распределения. Например, можно применять такой алгоритм \label{alg:alg2}:


\begin{enumerate}
\item На $s$-ом шаге сгенерировать очередную итерацию согласно:
\begin{align*}
&\Sigma^{[s]} \sim \cIW(\post S, \post \nu) \\
&\phi^{[s]}\sim \cN(\post \phi; \Sigma^{[s]} \otimes \post \Omega)
\end{align*}
\item Увеличить $j$ на единицу и перейти к пункту 1.
\end{enumerate}

На практике вместо генерирования вектора $\phi^{[s]}$ генерируют сразу матрицу $\Phi^{[s]}$ в два шага:

\begin{enumerate}
\item Сгенерировать матрицу $V$ размера $k\times m$  из независимых стандартных нормальных величин
\item Посчитать матрицу $\Phi^{[s]}$ по формуле:\footnote {Указанная формула следует из того, что генерация вектора $\Phi\sim N(\bar\Phi,\Sigma\otimes \bar\Omega)$: происходит с помощью разложения Холецкго указанной ковариационной матрицы: $\vec{\Phi}=\vec{\bar \Phi}+\chol (\Sigma\otimes \bar \Omega)\times \nu$}
\[
\Phi^{[s]} = \post \Phi + \chol(\post\Omega) \cdot V \cdot \chol(\Sigma^{[s]})'
\]
\end{enumerate}

%Два ключевых недостатка априорного распределения Миннесоты (отсутствие байесовского оценивания $\Sigma$ обязательная апостериорная независимость отдельных уравнений системы) решаются при использовании сопряженного нормального-обратного Уишарта распределения.

Желание задать сопряженное нормальное Уишарта априорное распределение с помощью нескольких гиперпараметров приводит к тому, что моменты априорных распределений параметров для разных уравнений оказываются зависимыми друг от друга. В частности, все коэффициенты при первом лаге зависимой переменной априорно имеют одну и ту же дисперсию $\lambda_{tight}^2$  Хотя обычно эта предпосылка не является слишком ограничивающей, в реальности легко встретиться с задачами, в которых ковариационная матрица априорного распределения не должна быть симметрично сформирована для разных уравнений. Например, довольно известным в литературе является следующий пример (см, \eng{\cite{kadiyala_karlsson_1997})}. Положим, исследователь хочет  учесть в VAR наличие нейтральности денег. При построении модели эта предпосылка может быть учтена наложением такого априорного распределения, в котором все коэффициенты при лагах денег в уравнении для выпуска имеют нулевое математическое ожидание и низкую дисперсию. Однако это означает, что и в других уравнениях дисперсия коэффициентов в априорном распределении будет относительно низкой. Это характеристика может быть нежелательной, и, чтобы этого избежать,   априорное распределение можно задать как независимое-обратное Уишарта.

\subsubsection{Независимое нормальное-обратное Уишарта распределение}

Этот тип распределений предполагает, что ковариационная матрица параметров может быть произвольной формы:

\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma \sim \cIW(\prior S; \prior\nu) \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}


В этом случае можно показать (\eng{\cite{karlsson_2012}}), что условные апостериорные распределения имеют вид:

\begin{equation}
\begin{cases}
\phi | \Sigma, Y \sim \cN(\post \phi; \post \Xi ) \\
\Sigma | \phi, Y \sim \cIW(\post S; \post\nu) \\
\end{cases}
\end{equation}
где гиперпараметры апостериорного распределения задаются как:
%\footnote{Другой вариант записи для $\post\phi$ имеет вид: $\post \phi = \post \Xi \cdot (\prior \Xi^{-1} \prior \phi + (\Sigma^{-1}\otimes(X'X))\vec{\hat\Phi})$ }
\begin{align*}
&\post\nu = \prior \nu + T \\
&\post S=\prior S + E'E, \text{ где } E=Y-X\Phi \\
&\post \Xi = (\prior \Xi^{-1} + \Sigma^{-1}\otimes X'X)^{-1} \\
&\post \phi = \post \Xi \cdot (\prior \Xi^{-1} \prior \phi + \vec(X'Y\Sigma^{-1}))=\\
&=\post \Xi \cdot (\prior \Xi^{-1} \prior \phi + (\Sigma^{-1}\otimes(X'X))\vec{\hat\Phi})
\end{align*}



Гиперпараметры априорного распределения могут быть выбраны точно так же, как и в априорном распределении Миннесоты (см. \eqref{minnesota_exp} и \eqref{minnesota_variance}). При необходимости неинформативное априорное распределение для коэффициентов при детерминированных переменных переменных можно задать, обнулив соответствующее значение в матрице $\prior\Xi^{-1}$.
Использование произвольной ковариационной матрицы приводит к тому, что исследователю оказываются известны только условные апостериорные распределения для $\phi$ и $\Sigma$. Это обуславливает необходимость использования алгоритма Гиббса для получения реализаций из совместного апостериорного распределения.

Получить Марковскую цепь, сходящуюся к апостериорному распределению можно, например, так \label{alg:alg3}:
\begin{enumerate}
\item Сгенерировать произвольно стартовую матрицу $\Sigma^{[0]}$, например, единичную
\item На $s$-ом шаге сгенерировать очередную итерацию согласно:

\begin{align}
&\phi^{[s]}\sim \cN(\post \phi^{[s-1]}; \post \Xi^{[s-1]}),
\text{ где }  \post \phi^{[s-1]} \text{ и } \post \Xi^{[s-1]} \text{ рассчитываются через  } \Sigma^{[s-1]}\\
&\Sigma^{[s]} \sim \cIW(\post S^{[s]}; \post\nu),
\text{ где } \post S^{[s]}  \text{ рассчитываются с помощью } \phi^{[s]}
\end{align}

\item Увеличить $s$ на единицу и перейти к пункту 2.
\end{enumerate}






\subsection{Эндогеннное определение гиперпараметров: оценка BVAR большой размерности}

В некоторых случаях гиперпараметры априорного распределения определяются эндогенно. В частности, это происходит при оценке модели большой размерности.
Для решения ряда прикладных макроэкономических задач, связанных как с прогнозированием, так и со структурным анализом, исследователям требуется работать с выборками большой размерности.  И хотя BVAR широко использовались в моделях низкой размерности практически с момента их появления в начале 1980-х годов, их применение  в моделях высокой размерности до недавнего времени было весьма ограничено. Причина заключалась в существовании консенсуса о том, что байесовская регуляризация сама по себе недостаточна для решения проблемы избыточной параметризации в моделях большой размерности и требует введения дополнительных небайесовских ограничений.

Ключевую роль в развитии подхода сыграла работы \eng{\cite{demol_al_2008}}~и \eng{\cite{banbura_al_2010}}, в которых было показано, что BVAR вполне могут быть применены к выборкам большой размерности без наложения дополнительных ограничений. Однако использование большого количества временных рядов требует уменьшения параметра $\lambda_{tight}$ с увеличением размерности выборки, что означает наложение более жесткого априорного распределения. На данный момент в литературе используется два подхода к определению оптимальной величины $\lambda_{tight}$.

%\subsubsection{Метод регуляризации в соответствии с \eng{\cite{banbura_al_2010}}}
Первый алгоритм был предложен в работе \eng{\cite{banbura_al_2010}}~и он основан на идее о том, что регуляризация должна быть настолько жесткой, чтобы исключить возможность избыточной параметризации модели, при этом предполагается, что трехмерная VAR "--- достаточно простая  модель, не содержащая слишком большого количества параметров, и поэтому она не требует дополнительной регуляризации. Это означает, что гиперпараметр  $\lambda$ должен быть выбран таким образом, чтобы модель демонстрировала такую же внутривыборочную подгонку, как и VAR c тремя переменными. Другими словами,  каждая модель регуляризуется до размера простой неограниченной VAR.
%Детальное описание процедуры приведено в работах \eng{\cite{banbura_al_2010}}, \eng{\cite{koop_2013}}, \cite{demeshev_malakhovskaya_2015}.

Детальное описание процедуры приведено ниже. Обозначим фактическое значение переменной $var$ в момент $T+h$ как $y_{var,T+h}$, а прогноз переменной $var$, осуществленный в момент на горизонт $h$ в модели с $m$ переменными  и параметром жесткости $\lambda$  как: $y_{var, T+h|T}^{\lambda,m}$.
Схема выбора $\lambda$ состоит из следующих этапов:


\begin{enumerate}
\item На первом этапе c помощью BVAR строятся внутривыборочные однопериодные прогнозы на обучающей выборке и рассчитывается среднеквадратичная ошибка прогноза для $M$ переменных, объединенных в набор $(\EuScript{M})$ и представляющих особенный интерес. \footnote{В базовый набор переменных, представляющих особенный интерес $(\EuScript{M})$ включается индекс промышленного производства, индекс потребительских цен и межбанковская процентная ставка, т.е.  $M=3$. Для проверки робастности было проведено исключение процентной ставки из этого набора, что практически не изменило вектор оптимальных  $\lambda$.}:
\begin{equation}
MSFE_{var,1}^{\lambda,m}=\frac{1}{T_0 - p}\sum_{t=p}^{T_0-1} \left(y_{var,t+1|t}^{\lambda,m}-y_{var,t+1}\right)^2,
%\quad var=\lbrace{ip,p,r\rbrace},
\end{equation}
\noindent где коэффициенты BVAR получены в результате оценки на обучающей выборке: $t=p+1,\ldots, T_0$ и $T_0$ "--- последнее наблюдение обучающей выборки: $T_0=p+120$.
\item Аналогично рассчитываются однопериодные прогнозы в соответствии с моделью случайного блуждания с дрейфом \footnote{MSFE для  моделей BVAR и  VAR нормализуются на MSFE, полученные с помощью модели RW, чтобы учесть тот факт, что различные временные ряды имеют разные единицы измерения. Для указания на модель RW используется надстрочный индекс 0, потому что RW может рассматриваться как частный случай BVAR для $\lambda=0$ и $\delta_i=1, i=1,\ldots,k$.} для тех же самых переменных $\left(MSFE_{var,1}^0\right)$, и кроме того рассчитывается новый индикатор $FIT^{\lambda,m}$:
\begin{equation}
FIT^{\lambda,m}=\frac{1}{M} \sum_{var\in \EuScript{M}} \frac{MSFE_{var,1}^{\lambda,m}}{MSFE_{var,1}^{0}}
%FIT^{\lambda,m}=\frac{1}{2}\cdot\frac{MSFE_y^{\lambda,m}}{MSFE^0_y} + \frac{1}{2}\cdot\frac{MSFE_{\pi}^{\lambda,m}}{MSFE^0_{\pi}}
\end{equation}
\item Оценивается трехмерная VAR для  тех же самых $M$ переменных, представляющих особый интерес при прогнозировании,\footnote{Для указания на неограниченную VAR используется надстрочный индекс $\infty$, так как неограниченная VAR является частным случаем BVAR при $\lambda\to \infty$. В этом случае апостериорное распределение совпадает с функцией правдоподобия.}  и считаются MSFE  и индикатор  $FIT^{\infty,M}$:

\begin{equation}
FIT^{\infty,M}=\frac{1}{M} \sum_{var\in \EuScript{M}} \frac{MSFE_{var,1}^{\infty,M}}{MSFE_{var,1}^{0}}
\end{equation}
\item Оптимальным $\lambda$ считается значение, минимизирующее разницу между $FIT^{\lambda,m}$ и $FIT^{\infty,M}$:

\begin{equation}
\lambda^*_m=\argmin_{\lambda} |FIT^{\lambda, m}-FIT^{\infty,M}|
\end{equation}
\end{enumerate}

После того как выбрано оптимальное $\lambda$ для каждой модели, происходит построение вневыборочных прогнозов на оценивающей выборке.

\subsubsection{Регуляризация с помощью однопериодного прогноза }

Второй алгоритм предложен в работе \eng{\cite{doan_al_1984}}~и представляет собой выбор такого параметра $\lambda_{tight}$, который бы максимизировал точность однопериодного вневыборочного прогноза на обучающей выборке. Этот выбор сводится к максимизации функции предельной плотности:
\begin{equation}
\lambda^*=\argmax_{\lambda} \ln p(Y)
\end{equation}

При этом функция предельной плотности может быть получена путем интегрирования коэффициентов модели:
% \footnote{Т.к. интегрирование происходит по  всем коэффициентам, но не по гиперпараметрам априорного распределения ($\lambda_1,\lambda_2,\lambda_3$)  и не по числу лагов $p$, то предельная плотность является функцией $\lambda_j,j=1\ldots 3$ и $p$.}
\begin{equation}
p(Y)=\int p(Y|\phi)p(\phi) d \phi
\end{equation}
Если априорное распределение является нормальным-обратным Уишарта, то маргинальная плотность $p(Y)$ может быть посчитана аналитически (\eng{\cite{zellner_1996}}; \eng{\cite{bauwens_al_2000}}; \eng{\cite{carriero_al_2012}}):

%\todo[inline]{Нормальные ссылки в жанре cite}

%\begin{equation}
\begin{multline}
p(Y)=\pi^{-\frac{Tm}{2}}\times +\left |(I+X\prior{\Omega} X')^{-1}\right|^{\frac{N}{2}}\times |\prior{S}|^{\frac{\prior{\nu}}{2}}\times \frac{\Gamma_N(\frac{\prior{\nu}+T}{2})}{\Gamma_N(\frac{\prior{\nu}}{2})}\times \\ \times
\left|\prior{S}+(Y-X\prior{\Phi})'(I+X\prior{\Omega} X')^{-1}(Y-X\prior{\Phi})\right|^{-\frac{\prior{\nu}+T}{2}},\label{marginal_density}
\end{multline}
где $\Gamma_N(\cdot)$ обозначает $N$-мерную гамма функцию.
Выбор числа лагов происходит аналогично путем максимизации по $p$ функции предельной плотности \eqref{marginal_density}:
\begin{equation}
p^*=\argmax_{p} \ln p(Y)
\end{equation}


\subsubsection{Особенности кодирования}

При практической реализации алгоритма Гиббса или Монте-Карло часто
приходится обращать положительно определённые симметричные матрицы. Некоторые из этих матриц могут иметь определитель близкий к нулю, что мешает практическому обращению этих матриц на компьютере.
В таком случае можно использовать следующий способ:


\begin{enumerate}

\item Получить разложение Холецкого для заданной матрицы $A$

\[
A=U'U,
\]

где $U$ "--- верхнетреугольная матрица

\item Обратить матрицу $U$. Существуют специальные алгоритмы обращения
верхнетреугольных матриц.

\item Получить $A^{-1}$ по формуле

\[
A^{-1}=U^{-1}U^{-1\prime}
\]

\end{enumerate}


Однако даже данный способ сопряжён с численными трудностями, если
обращаемая матрица плохо обусловлена. В таком случае оправданно использовано
псевдообратную матрицу Мура-Пенроуза.

Если дополнительно известно, что обращаемая матрица $A$ имеет вид $A=X'X$, то изложенный алгоритм лучше не применять, а сразу получить обратную к $A$, без вычисления $A$:

\begin{enumerate}
\item Получить сингулярное разложение матрицы $X$, $X=U\Sigma V'$
\item Получить обратную к $A=X'X$ по формуле $A^{-1}=V\Sigma^{-2} V'$
\end{enumerate}


%
%
%При определении параметра жесткости распределения по методу \eng{\cite{banbura_al_2010}}~необходимо выделить период, на котором происходит определение $\lambda$. Мы определяем $\lambda$ на самом раннем доступном промежутке: с января 1996г. по декабрь 2005г (первые 12 месяцев используются в качестве лаговых значений переменных для этой регрессии). Количество лагов определяется путем минимизации информационных критериев для обычной VAR на той же самой подвыборке. (что делать с вар по 24 рядам??) Далее количество лагов и параметр жесткости фиксируются и используются для построения прогнозов на всех остальных подвыборках.
%При определении параметра жесткости путем максимизации предельной плотности данных (\eng{marginal data density}) при оценке каждой BVAR происходит совместный поиск на сетке по $\lambda$ и по $p$, и выбираются такие значения, для которых \eqref{marginal_density} максимально.
%*\
\section{Прогнозирование с помощью BVAR}

\subsection{Апостериорная прогнозная плотность}
Основная цель оценки неструктурных Байесовских VAR "--- это построение прогнозов. BVAR позволяют строить точечные прогнозы, интервальные прогнозы и  прогнозы  плотности. Если задача состоит не только в построении прогноза по определённой модели, но и в оценке качества этого прогноза, то модель оценивается на некоторой исторической выборке и прогноз строится на те моменты времени, по которым фактические данные уже есть. Оценка модели может происходить как по сдвигающейся выборке (\eng{«rolling window scheme»}), так и по растущей выборке (\eng{«recursive regression»}). В первом случае оценка происходит по фиксированному количеству наблюдений, но на каждом шаге начало выборки и её окончание сдвигаются на один шаг вперёд. Прогнозы строятся на каждом шаге на выбранный горизонт.  Так происходит до тех пор, пока не будут исчерпаны все наблюдения, по которым можно сопоставить прогнозы и фактические данные. В случае растущей выборки фиксируется начало выборки, и на каждом шаге её длина увеличивается на одно наблюдение.

Ключевой концепцией при построении прогноза с помощью байесовской модели является функция апостерионой прогнозной плотности (\eng{posterior predictive density}). Вслед за \eng{\cite{karlsson_2012}}~мы обозначим прогнозную плотность с помощью $p(y_{T+1:T+H}|Y_{T})$. Эта запись означает, что прогноз строится на все моменты времени, начиная с $T+1$-го и заканчивая моментом $T+H$-ым при известных наблюдениях с момента $1$ по момент $T$. Здесь матрица $y_{T+1:T+H}=(y'_{T+1}\ldots, y'_{T+H} )'$ "--- это все будущие наблюдения, а матрица $Y_T=(y'_{1}\ldots, y'_{T} )'$ "--- это все наблюдения, по которым оценивалась модель. Функция апостериорной прогнозной плотности может быть представлена как:

\begin{equation}\label{predictive_density}
p(y_{T+1:T+H}|Y_{T})=\int p(y_{T+1:T+H}|Y_{T},\phi) p(\phi |Y_T) d\phi,
\end{equation}
где $p(y_{T+1:T+H}|Y_{T},\phi)$ "--- функция плотности будущих наблюдений при условии фиксированных параметров $\phi$ и данных вплоть до периода $T$, а $p(\phi |Y_T)$ "--- апостериорная плотность параметров.

Как правило, функция апостериорной прогнозной плотности не выражается аналитически для прогнозирования дальше, чем на один период вперёд. Однако её расчет возможен в соответствии с формулой \eqref{predictive_density} c помощью численных методов. Для этого необходимо для каждой реализации вектора параметров из апостериорного распределения $p(\theta |Y_T)$ посчитать прогнозные значения переменных $\tilde y_{T+h}$ на горизонте для $h=1,\ldots, H$ c помощью функции условной плотности $p(y_{T+1:T+H}|Y_{T},\theta)$. При этом при построении прогноза на горизонт $\tilde h$ прогнозы, построенные на $h<\tilde h$ становятся как будто известными значениями. Повторение описанной процедуры большое число раз позволяет получить выборку из апостериорного прогнозного распределения для каждого $h$.
%(как написать, что тета при этом игнорируется?).
\todo[inline]{тильда??? --- обозначение для прогнозов}

Другими словами, для BVAR построение прогнозной плотности происходит по следующей схеме (\cite{karlsson_2012}, c. 8, 18):
\begin{enumerate}
\item Сгенерировать параметры апостериорного распределения. Для априорного распределения Миннесоты можно использовать алгоритм на стр. \pageref{alg:alg1}, для сопряженного нормального-обратного Уишарта  "---  алгоритм на стр. \pageref{alg:alg2}, для независимого нормального-обратного Уишарта "--- алгоритм на стр. \pageref{alg:alg3}.

%\item На $s$-ом шаге сгенерировать очередную итерацию $\Sigma^{[s]}$ из маржинального апостериорного распределения $\Sigma|Y_{T}\sim IW (\bar S, \bar\nu)$, используя, например, алгоритм Geweke (1988).

%\item Cгенерировать $\Phi^[s]$ из условного апостериорного распределения: $\Phi|\Sigma^{[s]},Y_T\sim$%N(\bar \Phi,\Sigma^{[s]}\kron \bar\Omega)$

\item Сгенерировать $\varepsilon^{[s]}_{T+1},\ldots,\varepsilon^{[s]}_{T+H}$ из $\varepsilon_t\sim N(0,\Sigma^{[s]})$ (в случае априорного распределения Миннесоты $\Sigma^{[s]}=\Sigma$) и посчитать рекурсивно:

\begin{equation}
\tilde y_{T+h}^{[s]}=\Phi_{ex}^{[s]}+\sum_{i=1}^{h-1} \Phi_{i}^{[s]} \tilde y_{T+h-i}^{[s]}+\sum_{i=h}^{p} \Phi_{i}^{[s]} y_{T+h-i}^{[s]} +\varepsilon_{T+h}^{[s]}
\end{equation}

\item Увеличить $s$ на единицу и перейти к шагу 1.
\end{enumerate}

Далее следует «забыть» о том, что разные прогнозы были сделаны для разных значений параметров.
Для априорного распределения Миннесоты и сопряженного нормального-обратного Уишарта  следует  рассматривать $\lbrace \tilde y_{T+1}^{[s]},\ldots,\tilde y_{T+H}^{[s]}\rbrace_{s=1}^{S}$ как выборку независимых реализаций из совместного прогнозного распределения. В случае  независимого нормального-обратного Уишарта распределения генерация $\varepsilon^{[s]}$ происходит только при $s$ больше некоторого заранее заданного $B$, т.к. первые $B$ реализаций апостериорных параметров используются для сходимости цепи (т.н. период прожига, \eng{burn-in}). Соответственно,  прогнозные реализации также рассматриваются только для $s>B$: $\lbrace \tilde y_{T+1}^{[s]},\ldots,\tilde y_{T+H}^{[s]}\rbrace_{s=B+1}^{S}$.

Стоит заметить, что в случае сопряженного нормального априорного распределения процесс генерации параметров можно ускорить. Для этого матрицу параметров $\Phi$ можно посчитать как:
\begin{equation}
\Phi=\bar \Phi+\chol (\bar \Omega)\times V\times \chol(\Sigma)',
\end{equation}

%В случае если используется нормальное-неинформативное или независимое нормальное-Уишарта, то апостериорное предсказательное распределение находится с помощью семплирования Гиббса:
%\begin{enumerate}
%\item Сгенерировать параметры апостериорного распределения в соответствии с алгоритмом на стр.\pageref{alg:alg3}
%\end{enumerate}
%\subsubsection{Построение прогноза \todo{проверить Карьеро, с. 54}}
При этом прогноз на один шаг вперёд представляет собой линейную функцию параметров, и поэтому апостериорная прогнозная плотность для $h=1$ может быть задана аналитически. Она принимает вид матричного матричного $t$-распределения (MT), параметры которого зависят от конкретного используемого априорного распределения.  Например, для случая сопряженного нормального распределения (\eng{\cite{carriero_al_2015}}~с. 54) прогноз имеет многомерное $t$-распределение с параметрами:

\begin{equation}
y'_{T+1}|x'_{T+1}\sim MT \left(x'_{T+1}\bar\Phi, \left(x'_{T+1}\bar\Omega x_{T+1}\right)^{-1},\bar S,\bar\nu\right)
\end{equation}

\todo[inline]{Зачем снова уточняется, что это для сопр норм? Ведь уже выше сказано, что это только в этом случае? Какая тогда свобода в используемом априорном распределении?}

Прогноз на большее количество периодов требует численной процедуры, описанной выше.

После построения случайной выборки из апостериорного прогнозного распределения можно рассчитывать точечные и интервальные прогнозы. Выбор конкретного типа точечного прогноза  (например, мода или медиана прогнозной плотности) может быть сделан на основании функции потерь. Можно считать, что существует функция потерь $\Loss(a, y_{T+1:T+H})$, определяющая, какой вектор значений $a$ будет выбран в качестве точечного прогноза. Вектор значений $a$ выбирается таким образом, чтобы минимизировать ожидаемые потери при условии доступных данных $Y_T$ (\eng{\cite[p.~3]{karlsson_2012}}):

\begin{equation}
\E[\Loss(a,y_{T+1:T+H})|Y_T]=\int \Loss(a, y_{T+1:T+H}) p(y_{T+1:T+H}|Y_T) dy_{T+1:T+H}
\end{equation}

При заданных функции потерь и прогнозной плотности решение задачи минимизации есть функция только от доступных данных, $a(Y_T)$. Для частных случаев функции потерь решение принимает простую форму. Например, для квадратичной функции потерь решение принимает вид условного математического ожидания: $a(Y_T)=\E(y_{T+1:T+H}|Y_T)$, а для функции потерь в виде абсолютного значения  решение представляет собой моду прогнозного распределения.

\todo[inline]{Хм, несмотря на коррелированность срабатывает? я знаю этот результат для скалярного точечного прогноза, а для целого вектора у меня сомнения}

В прикладных работах, использующих BVAR,  строятся точечные прогнозы и прогнозы плотности.  При этом использование альтернативных моделей для построения прогноза позволяет оценивать указанные модели с точки зрения их точности.
%Для прогноза на несколько шагов вперёд не существует аналитического решения, и они рассчитываются с помощью итераций с использованием алгоритма Монте Карло:
%\begin{enumerate}
%\item Получить выборку из $\Sigma$ и параметров $\Phi$
%\item Для каждой реализации $j$ посчитать траекторию $\hat y_{\tau+h}^{var}$
%\end{enumerate}
%Для того чтобы избежать длительных расчетов, можно воспользоваться более эффективным алгоритмом, котрый значительно ускоряет расчеты как раз для случая матричного t-распределения. В этом случае параметры расчитываются как:
%\begin{equation}
%\vec{\Phi}=\vec{\bar \Phi}+\chol (\Sigma\otimes \bar \Omega)\times \nu,
%\end{equation}
%или:
%\begin{equation}
%\Phi=\bar \Phi+\chol (\bar \Omega)\times V\times \chol(\Sigma)',
%\end{equation}
%где $\nu$ "--- нормальный стандартный вектор размерности $[MN\times 1]$ и $\nu=\vec V$, и $V$ имеет размерность $[M\times N]$.
%Такое разложение возможно, только если ковариационная матрица параметров априорного распределения имеет кронеккерову структуру.

\subsection{Оценка точности точечного прогноза: одномерный случай}

%\subsubsection{Оценка точности точечного прогноза: одномерный случай}

Для оценки точности точечного прогноза чаще всего используют среднеквадратическую ошибку прогноза (\eng{MSFE, mean squared forecasting error}) или квадратный корень из нее (\eng{RMSFE, root mean squared forecasting error}):

\begin{equation}
MSFE_{var,h}^{M}=\frac{1}{N_h}\sum_{T} (y_{var,T+h|T}^{M}-y_{var,T+h|T})^2,
\end{equation}

\begin{equation}
RMSFE_{var,h}^{M}=\sqrt{MSFE_{var,h}^{M}},
\end{equation}

где $y_{var,T+h|T}^{M}$ "--- прогноз переменной $var$, сделанный в момент $T$ на $h$ шагов вперёд на основе модели $M$. Число $N_h$ отражает количество таких прогнозов для горизонта $h$, сделанных для разных $T$.


Альтернативной мерой точности прогноза служит значение средней абсолютной ошибки прогноза (\eng{MAE, mean absolute forecast error}): % да, MAE, F не пишется!


\begin{equation}
MAFE_{var,h}^{M}=\frac{1}{N_h}\sum_{T} |y_{var,T+h|T}^{M}-y_{var,T+h|T}|
\end{equation}

\todo[inline]{тильды у прогноза? здесь пропали :)}

\subsection{Оценка точности точечного прогноза: многомерный случай}

В многомерном случае, когда с помощью одного индикатора требуется описать точность прогноза сразу по нескольким переменным, используют две статистики, предложенные в работе \eng{\cite{adolfson_al_2007}}.
Это след и логарифм определителя матрицы  среднеквадратических ошибок $\Sigma_M(h)$, рассчитываемой как:

\begin{equation}
\Sigma_A(h)=\frac{1}{N_h}\sum_{t=T}^{T+N_h-1} \tilde \varepsilon_{t+h|t}\tilde \varepsilon_{t+h|t}',
\end{equation}

где  $\tilde \varepsilon_{t+h|t}= A^{-1/2} \varepsilon_{t+h|t}$, а $\varepsilon_{t+h|t}$ "--- ошибка прогноза на горизонте $h$ и $A$ "--- произвольная положительно определенная матрица.
Ранжирование прогнозов с помощью логарифма определителя не зависит от выбора матрицы $A$, а с помощью следа "--- зависит, так как $\ln |\Sigma_A(h)| = \ln|\Sigma_I(h)|-\ln|A|$, а  $\tr|\Sigma_A(h)| = \tr|A^{-1}\Sigma_I(h)|$.

В работе \eng{\cite{adolfson_al_2007}} в качестве матрицы $A$ была взята диагональная матрица выборочных дисперсий прогнозируемых переменных. В этом случае след матрицы равен взвешенному среднему среднеквадратичных ошибок прогнозов индивидуальных временных рядов. Однако в указанной работе было утверждается, что ранжирование точечных прогнозов с помощью многомерных индикаторов может вводить в заблуждение. Дело в том, что на значение этих статистик особенно сильно влияет возможность прогнозирования переменных, которые сложнее всего поддаются прогнозированию. Плохо прогнозируемые переменные обычно включают в себя большую часть последней главной компоненты. При этом эти переменные не обязательно представляют особый интерес для исследователя (\eng{\cite{adolfson_al_2007}}).




\subsection{Оценка точности апостериорной прогнозной плотности}

\subsubsection{Использование апостериорной функции распределения}

Точечный прогноз представляет собой  вероятное будущее значение переменной и не несёт никакой информации о степени неопределенности прогноза. В отличие от точечного прогноза, прогноз плотности отражает всю меру неопределенности прогноза. Прогноз плотности можно изобразить, например, с помощью веерных диаграмм (\eng{fan charts}).  Веерные диаграммы часто используются центральными банками для анонсирования своих прогнозов по инфляции или другим ключевым макроиндикаторам. В отличие от стандартных доверительных интервалов, прогноз плотности в общем случае не является симметричным относительного точечного прогноза.
Основная сложность при оценивании прогноза плотности является то, что истинная плотность является ненаблюдаемой, а наблюдаемыми являются лишь отдельные фактические значения прогнозируемых переменных, по одному на каждый момент времени.

Похожесть апостериорного прогнозного распределения на фактические данные называется калибровкой («соответствие между вероятностными прогнозами и фактическим поведением прогнозируемой величины», \cite{tsyplakov_2012}, стр. 118). Оценивание точности калибровки рассматривается, например, в работах \eng{\cite{gneiting_al_2007}}, \eng{\cite{mitchell_wallis_2011}}.
Естественно, возможные несоответствия между прогнозом и фактическими реализациями могут по-разному восприниматься исследователем. Формально говоря, это определяется его функцией потерь, о которой мы писали выше. Распространенный метод оценки калибровки функции потерь, PIT, состоит в том, чтобы подставить фактические значения переменных в апостериорную прогнозную функцию распределения\footnote{По английски этот метод называется вероятностным интегральным преобразованием, (\eng{probability integral transform}, PIT), в силу того, что апостериорная функция распределения получается взятием интеграла от апостериорной функции плотности. В русском языке подобная терминология не используется.} (см., например, \eng{\cite{diebold_al_1998}},
\eng{\cite{gerdrup_al_2009}}, \eng{\cite{gonzalez-rivera_sun_2015}}).


Предположим, что прогноз на момент $\tau$ имеет апостериорную функцию распределения $F$, а фактическое значение прогнозируемой переменной равно $y_{\tau}$. Если фактические реализации действительно представляют собой выборку из указанного распределения, то значения функции распределения $F(y_{\tau})$ имеют равномерное распределение на промежутке $[0, 1]$. Визуальная оценка качества прогноза происходит с помощью гистограммы. При совпадении фактического и предполагаемого апостериорного распределения гистограмма должна соответствовать равномерному распределению, то есть иметь столбики примерно одинаковой высоты.

В отличие от анализа функции плотности прогноза (\eng{predictive density}), метод  PIT является частотным, т.к. сравнивает распределение фактических значений с распределением, которое они имели бы, если бы процесс генерации данных совпадал с тем, что используется моделью. Некоторые формальные тесты на основе PIT можно найти в \eng{\cite{geweke_amisano_2010}}.

Можно выделить две проблемы, связанные с применением PIT. Во-первых, на практике условие равномерности распределения PIT сложно проверить: даже если рассматриваемая модель действительно является процессом генерации данных для некоторой переменной, прогнозная плотность не обязательно будет с ней совпадать из-за неопределенности относительно параметров.
\todo[inline]{не понял мысль, мы же всё выинтегрировали, что мешало?}
Во-вторых,  в работе \eng{\cite{gneiting_al_2007}}~c~помощью симуляций было показано, что на основе PIT невозможно однозначно выбрать истинную модель среди нескольких конкурирующих. Поэтому в этой же работе было предложено максимизировать «остроту прогнозной плотности при заданной калибровке», где под остротой понимается концентрация распределений вокруг фактических реализаций, что, в свою очередь, можно оценить с помощью диаграмм размаха (\eng{box plots}) или скоринговых правил.
\todo[inline]{Ежели мы чаго максимизируем, то как это оценивает точность прогноза? см. название раздела? Да и по каким параметрам? может добавить слово иногда невозможно выбрать?}



\subsubsection{Скоринговые правила}

Скоринговые правила представляют собой функции, в соответствии с которыми прогнозу начисляется определенное количество скоринговых баллов в зависимости от реализации прогнозируемой величины. Подробный обзор скоринговых правил можно найти в (\cite{tsyplakov_2012}). Если речь идет о серии прогнозов, полученных по разным моделям, то наиболее точной признается та модель, которая получила наивысший скоринговый балл.

Наиболее часто в литературе используется скоринговое правило для логарифма прогнозной плотности (\eng{log predictive density scores}), предложенное в работе \eng{\cite{good_1952}}~и описанное в \eng{\cite{geweke_amisano_2010}}. Свежие примеры его использования можно найти в \eng{\cite{adolfson_al_2007}}, \eng{\cite{christoffel_al_2010}}~и \eng{\cite{carriero_al_2015}}.

Скоринговый балл рассчитывается как:

\begin{equation}
s_h=\sum_{t=T}^{T+N_h-1} \ln p_t(y_{t+h})
\end{equation}

\todo[inline]{что такое $p_t()$? почему нет внутри $|Y_T$?}

Скоринговый балл для прогноза на один шаг может быть выражен с помощью функции маргинального правдоподобия (\eng{\cite{adolfson_al_2007}}, с. 324-325):
\begin{multline}
s_1 =\ln [p_T(y_{T+1})\cdots p_{T+N_1-1}(y_{T+N_1})]= \\
=\ln[p_T(y_{T+1},\ldots,y_{T+N_1})]=\ln m(T+N_1)-\ln m(T),
\end{multline}

\todo[inline]{опять же, что такое $p_t()$ и какие внутри условия явно не написаны?}

где $m(t)=p_0(y_1,\ldots,y_t)=\int p(y_1,\ldots,y_t|\phi) p(\phi) d\phi$ и $m(t)$ "--- функция маргинального правдоподобия данных вплоть до момента времени $t$, $p(\theta)$ "--- априорная плотность. Важно отметить, что внутри интеграла находится только априорная плотность параметров $p(\phi)$ и функция правдоподобия $p(y_1, \ldots, y_t | \phi)$. Следовательно, при подсчёте маргинального правдоподобия не используются фактические данные,  что позволяет интерпретировать маргинальное правдоподобие как меру точности вневыборочного прогноза плотности, а не меру внутривыборочной подгонки.

Подсчет $s_h$ для горизонта $h>1$ является более сложным, т.к. функция плотности $p(y_{t+h})$ не имеет явного аналитического выражения. Один из подходов состоит в оценке функции плотности $p(y_{t+h})$ на основе реализаций прогнозов с помощью ядерной оценки плотности (\eng{kernel density estimator}). На практике ядерная оценка функции плотности может быть построена, только если количество переменных в модели не очень велико. В случае большого количества переменных  \eng{\cite{adolfson_al_2007}}~предлагают предположить, что $p(y_{t+h})$ "--- есть плотность многомерного нормального распределения и оценить вектор математических ожиданий этого распределения и его ковариационную матрицу из прогнозной выборки.

\todo[inline]{Косячное обозначение $p(y_{t+h})$? Нет ли условия по $Y_T$?}

Предположение нормальности плотности используется, например, в работе \eng{\cite{carriero_al_2015}}. В этом случае применяется следующий алгоритм  построения скорингового балла:

\begin{enumerate}

\item Сгенерировать выборку из прогнозов для переменной $var$ на горизонт $h$

\item Рассчитать оценку логарифма прогнозной плотности по формуле:

\begin{multline}
s_t\left(y_{t+h}^{var}\right) =\log p(y_{t+h}|Y_t,m)=\\
=-0.5\left[ \ln(2\pi)+\ln(V^{var}_{\tau+h|\tau})+(y_{t+h}^{var}-\bar y_{\tau+h|\tau})^2/V_{\tau+h|\tau}^{var} \right]
\end{multline}

где $p(y_{t+h}|Y_t,m)$ "--- маргинальная прогнозная условная плотность для $y_{t+h},\ldots,y_{t+h}$, зависящая от наблюдаемых данных $Y_t=\lbrace{y_1,\ldots,y_T\rbrace}$,  а вектор $\bar y_{\tau+h|\tau}^{\tau}$ и матрица $V_{\tau+h|\tau}^{var}$ обозначают апостериорное математическое ожидание и дисперсию  распределения прогнозов для переменной $var$ и прогнозного окна $h$.

\item Рассчитать скоринговый балл как среднее арифметическое полученных оценок для каждого прогнозного горизонта:
$s_t\left(y_{t+h}^{var}\right)$:
\begin{equation}
\bar s_{var,h}^{M}=\frac{1}{N_h}\sum s_t\left(y_{t+h}^{var}\right),
\end{equation}
где $N_h$ "--- количество построенных прогнозов на горизонт $h$.
\end{enumerate}



\section{Заключение}

Представленный обзор посвящен механизму оценки байесовских неструктурных векторных авторегрессий и их применению для построения прогнозов. Мы подробно останавливаемся на наиболее часто используемых в макроэкономических прикладных работах априорных распределениях и строим для них «карту», содержащую подробное представление их взаимосвязей.

Отдельный параграф работы посвящен заданию сопряженного нормального-обратного Уишарта распределения с помощью искусственных наблюдений "--- способу, часто применяемому в литературе, но не затронутому во многих из существующих обзоров метода. В разделе, посвященном прогнозированию с помощью BVAR, рассмотрены как точечные прогнозы, так и прогнозы плотности.

%В дальнейшем планируется продолжение работы в направлении рассмотрения более сложных моделей BVAR, таких как иерархические BVAR и BVAR c меняющимися параметрами.

\newpage
\printbibliography

\newpage

%\begin{appendices}
%\section*{Приложения}

\section*{Приложение 1. Доступные реализации кода}

\begin{center}
\begin{tabular}{p{4cm}cccccc}
\toprule
Источник&  Среда&Min &  Conj N-IW & Ind N-IW & SoC &IO\\
\midrule
% тут базовые %
Carriero&Matlab&-&?&-&+&+ \\
Blake Mumtaz&Matlab&-&+&+&+&+ \\
Koop Korobilis&Matlab  & + & +  & + &- &- \\
Zha&Matlab&&?&&+&+\\
Le Sage&Matlab&?&&&&\\
Sims&Matlab&&?&?&+&+\\
Canova&Matlab&&?&&&\\
BMR&R& + & - & + & - & -  \\
MSBVAR&R & - & + & - & + & +  \\
bvarr&R  & + & + & + & + & +  \\
Sims&R&&?&?&+&+\\
Встроенная функция&EViews& + & + & - &+ &+ \\
Встроенная функция&Dynare& ? & +& ?& +&+  \\
\bottomrule
\end{tabular}
\end{center}
\begin{enumerate}
\item Carriero: Дамми-наблюдения вводятся как для conj NIW, при этом делает Gibbs sampling, при этом $\post\Phi$ одно и то же, а $\Sigma$ пересчитывается на каждом шаге в зависимости от предыдущего $\Phi$. При плохо обусловленной матрице $X'X$ используется псевдо-обратная. Имеется странный хак, повторно генерирующий VAR коэффициенты, если собственные числа за пределами единичного круга. \url{http://cremfi.econ.qmul.ac.uk/efp/info.php}
\item Blake Mumtaz: Называет Миннесотой Ind NIW.  Код для conj NIW построен так же, как у Carriero через Gibbs Sampling: разница --- добавляет две одинаковых строки для дамми при определении коэффициента при константе. \url{http://www.bankofengland.co.uk/education/Pages/ccbs/technical_handbooks/techbook4.aspx}
\item Koop Korobilis: Код крайне негибкий. Без правки кода нет возможности прогнозировать больше чем на один шаг, базовые conj N-IW и ind N-IW априорное распределения не содержит  гиперпараметров и задаются через фиксированные матрицы.\url{https://sites.google.com/site/dimitriskorobilis/matlab}\\
\item Zha: в соответствии со статьей ограничения накладываются на структурную форму VAR, соответственно дается другая интерпретация для некоторых гиперпараметров
\item Sims: Недостаточно подробное описание. Нужно прочитать весь код, чтобы что-то модифицировать.\url{http://sims.princeton.edu/yftp/VARtools/}
\item BMR: Симуляции реализованы в C++.  оценивает DSGE.  Реализует TVPBVAR. Отличная документация. \url{ http://bayes.squarespace.com/bmr/}
\item MSBVAR: Симуляции реализованы на фортране и C++. Реализует также марковские BVAR с переключением.  \url{https://cran.r-project.org/web/packages/MSBVAR/ }
\item bvarr: Сопряжённое нормальное-Уишарта реализовано максимально гибко.  При плохо обусловленной матрице $X'X$ используется псевдо-обратная. Код для Миннесоты и независимого нормального Уишарта является переводом кода Koops-Korobilis и потому крайне негибкий. \url{https://github.com/bdemeshev/bvarr }
\item bvarsv: TVP, \url{https://github.com/FK83/bvarsv}
\item Eviews: Код игнорирует тот факт, что коэффициенты были оценены байесовскими методами, прогнозы делаются точно так же, как вобычной модели. Коэффициент $\lambda_{kron}$ равен по умолчанию 0,99 и не может быть изменен. Большую свободу представляет прямое задание ковариационной матрицы. Матожидания коэффициентов при первых лагах могут быть заданы только одинаковыми для всех переменных.
\item Dynare: Функция позиционируется как BVAR \`a la Sims. Оценка возможна только «комплектом», но можно поменять априорное распределение для априорного распределения ковариационной матрицы  \url{http://www.dynare.org/}
\end{enumerate}
\newpage

\section*{Приложение 2. Таблица обозначений}

\begin{center}
\begin{tabular}{ccp{6cm}l}
\toprule
Буква & Размер &  Описание & Формула \\
\midrule
$p$ & скаляр & количество лагов & \\
$m$ & скаляр & количество эндогенных переменных & \\
$d$ & скаляр & количество экзогенных переменных & \\
$k$ & скаляр & количество параметров в одном уравнении & $k=mp+d$ \\
$T$ & скаляр & количество наблюдений &  \\
\midrule
$z_t$ & $d \times 1$ & вектор экзогенных переменных (считая константу) & \\
$y_t$ & $m \times 1$ & вектор эндогенных переменных  & $y_t=\Phi' x_t+\varepsilon_t$ \\
$x_t$ & $k \times 1$ & вектор всех регрессоров & $x_t=[ y'_{t-1} \ldots  y'_{t-p} \; z'_t ]'$ \\
$\varepsilon_t$ & $m \times 1$ & вектор случайных ошибок  & $y_t=\Phi' x_t+\varepsilon_t$\\
$Y$ & $T \times m$ & все эндогенные переменные & $Y=[y_1, y_2,\ldots, y_T]'$ \\
$X$ & $T \times k$ & матрица регрессоров& $X=[x_1, x_2,\ldots, x_T]'$ \\
$E$ & $T \times m$ & матрица ошибок & $E=[\varepsilon_1, \varepsilon_2,\ldots, \varepsilon_T]'$ \\
$y$&$mT\times 1$& векторизация $Y$&$y=\vec(Y)$\\
$\varepsilon$&$mT\times 1$ &векторизация $E$&$\varepsilon=\vec(E)$\\
\midrule
$\Phi_1$, \ldots & $m \times m$ & коэффициенты VAR & $y_t= \Phi_1 y_{t-1} + \ldots + \Phi_{const}+\varepsilon_t$ \\
$\Phi_{const}$ & $m \times d$ & вектор свободных членов& $y_t= \Phi_1 y_{t-1} + \ldots + \Phi_{const}+\varepsilon_t$ \\
$\Phi$ & $k \times m$ & упаковка матриц $\Phi_1$, \ldots & $\Phi=[ \Phi_1 \ldots \Phi_p \; \Phi_{const}]'$ \\
$\phi$ & $km \times 1$ & вектор из матрицы $\Phi$ & $\phi = \vec{\Phi}$ \\
\midrule % здесь к независимому без кронекерова произведения
$\prior \Phi$ & $k \times m$ & априорное математическое ожидание $\Phi$ &  \\
$\prior \phi$ & $km \times 1$ & вектор из матрицы $\prior \Phi$  & $\prior \phi = \vec{ \prior \Phi}$ \\
$\post \Phi$ & $k \times m$ & апостериорное математическое ожидание $\Phi$ & \\
$\post \phi$ & $km \times 1$ & вектор из матрицы $\post \Phi$  & $\post \phi = \vec{ \post \Phi}$ \\
\midrule
$\prior \Xi $ & $km \times km$  & Априорная ковариационная матрица $\Phi$&\\
$\post \Xi $ & $km \times km$  & Апостериорная ковариационная матрица $\Phi$& $\post \Xi = (\prior \Xi^{-1} + \Sigma^{-1}\otimes X'X)^{-1}$ \\
$\prior \nu $ & скаляр & априорное количество степеней свободы & \\
$\post \nu $ & скаляр & апостериорное количество степеней свободы& $\post \nu = T + \prior \nu$ \\
\midrule % здесь к сопряженному с кронекеровым произведением
\end{tabular}
\end{center}

\newpage
\begin{center}
\begin{tabular}{ccp{6cm}l}
\midrule
$\prior \Omega$ & $k \times k$ & Матрица априорных масштабирующих коэффициентов ковариационной матрицы $\Phi$& $\prior \Xi = \Sigma \otimes \prior \Omega$ \\
$\post \Omega$ & $k \times k$ & Матрица апостериорных масштабирующих коэффициентов ковариационной матрицы $\Phi$&  $\post \Omega = (\prior\Omega^{-1}+ X'X)^{-1}$,  $\post \Xi = \Sigma \otimes \post \Omega$\\
$\Sigma$ & $m \times m$ &Ковариационная матрица ошибок& $\E\varepsilon_t \varepsilon _t'=\Sigma$\\
\bottomrule
\end{tabular}
\end{center}
\newpage

\section*{Приложение 3. Соответствие гиперпараметров в разных работах}


\begin{center}
\begin{tabular}{cccc}
\toprule
DM15 &  CCM15 &  BGR10, BH13 & KK97  \\
\midrule
% тут базовые %
$\lambda_{tight}$ & $\lambda_1$ & $\lambda$ & $\sqrt{\pi_1}$ \\
$\lambda_{kron}$ & $\lambda_2=1$ & $\vartheta=1$ &  $\sqrt{\pi_2/\pi_1}$\\
$\lambda_{lag}$ & $1$ & $1$ & $0.5$\\
$\lambda_{const}$  & $\lambda_0/\lambda_1$ & $\infty$ &$ \sqrt{\pi_3/\pi_1}$\\
$\lambda_{exo}$ & NA & NA & NA\\
\midrule
$\lambda_{sc}$ & $\lambda_3$ & $\tau$ & \\
$\lambda_{io}$ & $\lambda_4$ & NA & \\
\bottomrule
\end{tabular}
\end{center}


DM15 "--- данная работа, CCM15 "--- \eng{\cite{carriero_al_2015}}, BGR10 "--- \eng{\cite{banbura_al_2010}},
BH13 "--- \eng{\cite{berg_henzel_2013}}, KK97 "--- \eng{\cite{kadiyala_karlsson_1997}}


\end{document}
