% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{article} % use larger type; default would be 10pt


%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or\ldots .
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information


% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{url}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another\ldots


%%% HEADERS & FOOTERS
%\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
%\pagestyle{fancy} % options: empty , plain , fancy




%%My Additional Packages
\usepackage{mathtext}          % русские буквы в формулах
\usepackage[T2A]{fontenc}            % внутренняя кодировка  TeX
\usepackage[utf8]{inputenc}         % кодировка исходного текста
% \usepackage{cmap}          % русский поиск в pdf
\usepackage[english,russian]{babel} % локализация и переносы
\usepackage{amsmath} % Математические окружения AMS
\usepackage{amsfonts} % Шрифты AMS
\usepackage{amssymb} % Символы AMS
\usepackage{graphicx} % Вставить pdf- или png-файлы

\usepackage{euscript} % Красивый шрифт

\usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

\usepackage{indentfirst} % Отступ в первом абзаце.

\usepackage[backend=biber, style=authoryear, citestyle=authoryear]{biblatex}
\addbibresource{bvar.bib}



\usepackage{wrapfig} % Обтекание рисунков текстом

\usepackage{hyperref} % Гиперссылки

\DeclareMathOperator{\etr}{etr}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Var}{\mathbb{V}\mathrm{ar}}
\DeclareMathOperator{\chol}{chol}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cIW}{\mathcal{IW}}
\newcommand{\lag}{\EuScript{L}}

\newcommand{\prior}{\underline}
\newcommand{\post}{\overline}

\let\vec\relax
\DeclareMathOperator{\vec}{vec}


%\usepackage[toc,page]{appendix}


%\renewcommand{\appendixtocname}{Приложения}
%\renewcommand{\appendixpagename}{Приложения}
%\renewcommand{\appendixname}{Приложение}
%%% END Article customizations

%%% The "real" document content comes below\ldots

\title{BVAR in Russia}
\author{я и Боря}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed

\begin{document}
\maketitle

\section{Обзор литературы}

Построение точных  макроэкономических прогнозов является ключевым условием проведения верной политики центральными  банками.  Хорошо известно, что центральные банки развитых государств опираются на большое число макроиндикаторов при проведении политики ((\cite{stock_watson_2002_forecasting} или (\cite{bernanke_al_2005}) для США., какая-нибудь аналогичная работа для ЕЗ). Однако обычная векторно-авторегрессионная модель, ставшая наиболее часто встречающимся инструментом для построения прогнозов, не может учесть большое количество переменных, так как количество параметров, подлежащих оценке, растет нелинейно с увеличением числа уравнений.  При этом неучтенная при построении VAR информация может приводить к смещенным оценкам и  неверным выводам  как относительно прогнозируемых значений, так и виде функций импульсных откликов.  Основные способы учета большого числа переменных – это использование DF и Байсовских VAR.
Динамические факторы были предложены в работах \cite{forni_hallin_lippi_generalized} и \cite{stock_watson_2002_forecasting}.  В указанных работах предполагается, что дисперсия большого количества временных рядов может быть описана с помощью нескольких, искусственно построенных (common factors)  с помощью метода главных компонент.  Расширением метода\cite{stock_watson_2002_forecasting} служит FAVAR, предложенная в статье (\cite{bernanke_al_2005}). В рамках FAVAR несколько динамических факторов  добавляются как дополнительные переменные в обычную VAR.
Цель данной работы состоит в построении  прогноза основных макроиндикаторов (выпуска, инфляции и др.) для российской экономики.  Задача осложняется отсутствием  большого количества длинных временных рядов, что не позволяет провести построение DFM.
Байесовские модели зарекомендовали себя как хороший инструмент построения прогнозов. В ряде работ было показано, что они обеспечивают более низкую ошибку прогноза, чем, например, обычные VAR и VECM.
(?) две стратегии – использовать shrinkage, который становится уже с ростом числа переменных
По Bloor and Matheson надо бы переписать.


Модификация Litterman prior была предложена в работах \cite{doan_litterman_sims_1984_forecasting}  (sum of coefficients prior) и \cite{sims_1993_nine}    (co-persistence prior).  Комбинация этих трех априорных распределений была использована в работе \cite{robertson_tallman_1999_vector}  для предсказания безработицы, темпа роста  и инфляции. В работе показано, что смешанное априорное распределение получает получить более точные прогнозы, чем априорные распределения \cite{litterman1986forecasting} и \cite{sims_zha_1998_bayesian} (про эту работу пока ничего не писала).


Ключевую роль в развитии подхода сыграла статья (\cite{de_mol_al_2008_forecasting}). В этой работе на основе асимптотического анализа было показано, что если данные характеризуются высокой мультиколлинеарностью (что характерно для выборок макрорядов большой размерности) сужение априорного распределения при увеличении количества переменных дает больший вес нескольким первым главным компонентам.  Это означает, что если данные характеризуются факторной структурой, то наложение более узких априорных распределений с увеличением размерности модели  не  приводит к потере важной информации, т.к. для описания данных достаточно небольшого количества первых факторов.  Эта точка зрения  была подтверждена и развита в статье \cite{banbura_al_2010_large}, в которой авторы  строят VAR модели для 3, 7, 20 и 131 переменных и показывают, что модели с большей размерностью демонстрируют лучшие прогнозные способности, чем модели малой размерности и даже FAVAR (? Проверить, об этом пишет Бошеман). Интересно отметить, что хорошая прогнозная способность достигается уже в модели с 20  переменными, поэтому как для прогнозирования, так и для структурного анализа достаточно сконцентрироваться на агрегированных данных.
Аналогичная модель для Новой Зеландии была построена в работе \cite{bloor_matheson_2010_analysing}, в которой они использовали метод условного прогнозного оценивания (\cite{waggoner_zha_1999_conditional}), что позволило им сравнить сценарии , основанные на различной условной информации.  Строят три модели (с 9, 13 и 35 переменными), делают вывод, что BVAR обладает более высокой предсказательной способностью, чем AR и обычная VAR модель.  Хотя результаты варьируют по разным переменным, в общем и целом, BVAR c большим числом переменных характеризуется более высокой точностью прогноза.
Из Beauchemin
Koop (2010) расширил результаты \cite{banbura_al_2010_large}  и показал, что BVAR c большой размерностью обладают лучшей прогнозной способностью даже по отношению  к более сложным моделям (???)
Тот же метод построения априорного распределения (естественно-сопряженная версия Миннесоты-распределения (\cite{kadiyala_karlsson_1997_numerical},  \cite{sims_zha_1998_bayesian}- проверить, они ли предложили? ), что в работах \cite{banbura_al_2010_large} , \cite{bloor_matheson_2010_analysing} и Koop (2010) был применен в работе \cite{beauchemin_zaman_2011_medium}. Они показывают, что BVAR с 16 переменными может быть с успехом использована как для прогнозов, так и для структурного анализа (трансмиссии монетарного шока (?)/cтруктурного анализа монетарной политики. Аналогичное построение априорного распределения используется в работе  \cite{alessandri_mumtaz_2014}, где с помощью линейной и нелинейной BVAR (?) модели показано, что учет финансовых индикаторов позволяет улучшить прогноз выпуска и инфляции, в т. ч. в кризисные периоды.

Во всех работах гиперпараметр, контролирующий жесткость (?) , выбирается таким образом, чтобы максимизировать функцию правдоподобия  данных ( это максимизирует точность вневыборочного прогноза ?). В работе \cite{geweke_whiteman_2006_bayesian} было показано, что такой выбор гиперпараметра минимизирует ошибки прогноза на один период.


Сам Литтерман в своей работе показал, что использование априорного распределения (Bayesian shrinkage) в BVAR с  не менее  чем шестью переменными  улучшает прогнозную силу модели. Однако до~последнего времени считалось, что при использовании достаточно большого числа временных рядов уточнения правдоподобия только с помощью априорного распределения недостаточно.  Это приводило к необходимости задавать дополнительные ограничения.

\section{Методология}

\subsection{Удобная табличка}

\begin{center}
\begin{tabular}{ccp{6cm}l}
\toprule
Буква & Размер &  Описание & Формула \\
\midrule
$p$ & скаляр & количество лагов & \\
$m$ & скаляр & количество эндогенных переменных & \\
$d$ & скаляр & количество экзогенных переменных & \\
$k$ & скаляр & количество параметров в одном уравнении & $k=mp+d$ \\
$T$ & скаляр & количество наблюдений &  \\
\midrule
$z_t$ & $d \times 1$ & вектор экзогенных переменных (считая константу) & \\
$y_t$ & $m \times 1$ & вектор эндогенных переменных  & $y_t=\Phi' x_t+\varepsilon_t$ \\
$x_t$ & $k \times 1$ & вектор всех регрессоров & $x_t=[ y'_{t-1} \ldots  y'_{t-p} \; z'_t ]'$ \\
$\varepsilon_t$ & $m \times 1$ & вектор случайных ошибок  & \\
$Y$ & $T \times m$ & все эндогенные переменные & $Y=[y_1, y_2,\ldots, y_T]'$ \\
$X$ & $T \times k$ & матрица регрессоров& $X=[x_1, x_2,\ldots, x_T]'$ \\
$E$ & $T \times m$ & матрица ошибок & $E=[\varepsilon_1, \varepsilon_2,\ldots, \varepsilon_T]'$ \\
\midrule
$\Phi_1$, \ldots & $m \times m$ & коэффициенты VAR & $y_t= \Phi_1 y_{t-1} + \ldots + \Phi_{ex} z_t$ \\
$\Phi_{ex}$ & $m \times d$ & коэффициенты при экзогенных переменных & \\
$\Phi$ & $k \times m$ & упаковка матриц $\Phi_1$, \ldots & $\Phi=[ \Phi_1 \ldots \Phi_p \; \Phi_{ex}]'$ \\
$\phi$ & $km \times 1$ & вектор из матрицы $\Phi$ & $\phi = \vec{\Phi}$ \\
\midrule % здесь к независимому без кронекерова произведения
$\prior \Xi $ & $km \times km$  & Априорная ковариационная матрица $\Phi$& \\
$\prior \Phi$ & $k \times m$ & априорное математическое ожидание $\Phi$ & \\
$\prior \phi$ & $km \times 1$ & вектор из матрицы $\prior \Phi$  & $\prior \phi = \vec{ \prior \Phi}$ \\
$\post \Xi $ & $km \times km$  & Апостериорная ковариационная матрица $\Phi$& \\
$\post \Phi$ & $k \times m$ & апостериорное математическое ожидание $\Phi$ & \\
$\post \phi$ & $km \times 1$ & вектор из матрицы $\post \Phi$  & $\post \phi = \vec{ \post \Phi}$ \\
$\prior \nu $ & скаляр & & \\
$\post \nu $ & скаляр & & $\post \nu = T + \prior \nu$ \\
\midrule % здесь к сопряженному с кронекеровым произведением
$\prior \Omega$ & $k \times k$ & Матрица априорных масштабирующих коэффициентов ковариационной матрицы $\Phi$& $\prior \Xi = \Sigma \otimes \prior \Omega$ \\
$\post \Omega$ & $k \times k$ & Матрица апостериорных масштабирующих коэффициентов ковариационной матрицы $\Phi$&  $\post \Omega = (\prior\Omega^{-1}+ X'X)^{-1}$,  $\post \Xi = \Sigma \otimes \post \Omega$\\
$\Sigma$ & $m \times m$ &Ковариационная матрица ошибок& $\E\varepsilon_t \varepsilon _t'=\Sigma$\\
\bottomrule
\end{tabular}
\end{center}





\subsection{Байесовская VAR}
Рассмотрим переменные $y_{it}$, объединенные в~вектор  $y_{t}=(y_{1t},y_{2t},\ldots, y_{mt})'$ размерности $m$. Векторная авторегрессия в сокращенной форме записывается в~виде:
\begin{equation}
y_t =\Phi_{ex}+ \Phi_1 y_{t-1} + \Phi_2 y_{t-2} +\ldots + \Phi_p y_{t-p} + \varepsilon_t,\quad \varepsilon\sim \cN(0,\Sigma)
\end{equation}
где $\Phi_{ex}=(c_1,\ldots ,c_m)'$ --- вектор констант размерности $m$, $\Phi_l$ --– авторегрессионные матрицы размерности $m\times m$ при $l=1, \ldots, p$. Вектор $\varepsilon_t$ --- $m$-мерный вектор ошибок с ковариационной матрицей $\E\varepsilon_t \varepsilon _t'=\Sigma$, некоррелированный с объясняющими переменными.
Группируя матрицы параметров в общую матрицу $\Phi=[\Phi_1 \ldots \Phi_p \; \Phi_{ex}]'$ % здесь нужно транспонирование!
и определяя новый вектор $x_t=[ y'_{t-1} \ldots  y'_{t-p} \; 1]'$, получаем VAR записанную в более компактном виде:

\begin{equation}
y_t=\Phi' x_t+\varepsilon_t
\end{equation}
Если же сгруппировать переменные  и шоки следующим образом: $Y=[y_1, y_2,\ldots, y_T]'$, $X=[x_1, x_2,\ldots, x_T]'$, $E=[\varepsilon_1, \varepsilon_2,\ldots, \varepsilon_T]'$ то VAR можно записать как:
\begin{equation}
Y=X\Phi+E\label{var}
\end{equation}

Задача байесовского оценивания заключается в~поиске апостериорных распределений параметров $p(\Phi, \Sigma|Y)$ с~использованием функции максимального правдоподобия, $L(Y|\Phi, \Sigma)$, и~заданного априорного распределения, $p(\Phi, \Sigma|Y)$. Для этого используется правило Байеса:
\begin{gather}
p(\Phi, \Sigma|Y)=\frac{p(\Phi,\Sigma) L(Y|\Phi,\Sigma)}{p(Y)}
\text{Т.к. }p(Y) \text{ не зависит от } \Phi \text{ и }\Sigma, \text{ то можно записать: }\\
p(\Phi, \Sigma|Y)\propto p(\Phi,\Sigma) L(Y|\Phi,\Sigma)
\end{gather}\\
Так как $\varepsilon_t\sim \cN(0,\Sigma)$, то функция правдоподобия задается как:\footnote{ Другая форма записи функции правдоподобия: $L(Y|\Phi, \Sigma) \propto |\Sigma|^{-T/2}\etr\left\lbrace -\frac{1}{2}  \left[\Sigma^{-1} \hat E' \hat E\right]\right\rbrace \times \etr\left\lbrace -\frac{1}{2}  \left[\Sigma^{-1}(\Phi-\hat\Phi)'X'X(\Phi-\hat\Phi)\right]\right\rbrace$, где $\hat E=Y-X\hat\Phi$ и $\hat \Phi=(X'X)^{-1}X'Y$. Здесь $\etr()=\exp(\tr())$. }
\begin{equation}
L(Y|\Phi, \Sigma) \propto |\Sigma|^{-T/2}\etr\left\lbrace -\frac{1}{2}  \left[\Sigma^{-1}(Y-X\Phi)'(Y-X\Phi)\right]\right\rbrace\label{likelihood}
\end{equation}

\subsection{Классификация популярных априорных распределений}

Детально каждое из априорных распределений будет описано отдельно, а в этом разделе будет дана общая схема связи априорных распределений. Среди самых популярных априорных распределений можно назвать:
\begin{enumerate}
\item Независимое нормальное-обратное Уишарта распределение

\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma \sim \cIW(\prior S; \prior\nu) \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}

В общем случае выборку из  апостериорного распределения можно получить по схеме Гиббса. 

Частными случаями независимого нормального-обратного Уишарта являются:

\begin{enumerate}
\item Распределение Миннесоты

\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma = const \\
% \phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}

Получается из независимого нормального-обратного Уишарта при $\prior S=(\prior \nu-m-1) \cdot \Sigma$ и $\prior \nu \to \infty$. Апостериорное распределение выписывается в явной форме. Можно использовать алгоритм Монте-Карло, сразу генерирующий случайную выборку из апостериорного распределения, без необходимости периода «прожига». 

Более того, алгоритм симуляции упрощается если матрица $\prior \Xi$ имеет структуру кронекерова произведения $\prior \Xi =  \Sigma \otimes \prior \Omega $. В этом случае распределение Миннесоты становится частным случаем сопряжённого нормального-обратного Уишарта. 


\item Нормальное-Джеффри

\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma \sim |\Sigma|^{-(m+1)/2} \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}



Получается из независимого нормального-обратного Уишарта при $\prior S = \prior \nu^{1/m}\cdot I$ и  $\prior \nu \to 0$.  

как тут с плотностью растущей к необратимым матрицам ???????????
\url{http://www.tc.umn.edu/~nydic001/docs/unpubs/Wishart_Distribution.pdf} --- тут пишут, что при $\nu <...$ получается необратимая матрица, а тут вроде как обратимая????


Для получения выборки из~апостериорного распределения можно использовать схему Гиббса. Необходимые формулы для гиперпараметров апостериорного распределения получаются из общего случая просто подстановкой $\prior S=0$, $\prior \nu =0$.


Распределение Миннесоты и нормальное-Джеффри являются противоположными крайностями независимого-обратного Уишарта. В распределении Миннесоты матрица $\Sigma$ предполагается известной, а в нормальном-Джеффри матрица $\Sigma$ имеет «размытое» неинформативное распределение.


Частным случаем нормального-Джеффри распределения является:

\begin{enumerate}
\item Неинформативное-Джеффри
\begin{equation}
\begin{cases}
\phi \sim 1 \\
\Sigma \sim |\Sigma|^{-(m+1)/2} \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}
\end{enumerate}

Априорное распределение для ленивых. Не нужно указывать ни одного гиперпараметра!

Для получения выборки из~апостериорного распределения можно использовать схему Гиббса. Необходимые формулы для гиперпараметров апостериорного распределения получаются из общего случая просто подстановкой $\prior S=0$, $\prior \nu =0$, $\prior \Xi^{-1}=0$, $\prior\phi=0$. При этом формулы существенно упрощаются, в частности исчезает необходимость обращать матрицу размера $km\times km$ исчезает. 

\end{enumerate}
\item Сопряженное нормальное-обратное Уишарта распределение

\begin{equation}
\begin{cases}
\Sigma\sim \cIW(\prior S, \prior \nu) \\
\phi|\Sigma\sim \cN (\prior \phi, \Sigma\otimes\prior \Omega)
\end{cases}
\end{equation}

Для сопряженного нормального-обратного Уишарта распределения нет необходимости использовать алгоритм Гиббса, так как есть явные формулы для апостериорных распределений. Можно использовать алгоритм Монте-Карло, сразу генерирующий случайную выборку из апостериорного распределения. Исчезает период «прожига», необходимый для сходимости алгоритма Гиббса. 

Частным случаем сопряжённого нормального-обратного Уишарта распределения оказывается:

\begin{enumerate}
\item Распределение Миннесоты при  $\prior \Xi =  \Sigma \otimes \prior \Omega $.
\end{enumerate}

При $\prior \Xi =  \Sigma \otimes \prior \Omega $ распределение Миннесоты является и частным случаем независимого нормального-обратного Уишарта, и сопряженного нормального-обратного Уишарта. Для получения выборки из апостериорного распределения можно использовать и схему Гиббса для независимого нормального-обратного Уишарта, и алгоритм Монте-Карло без «прожига».


\end{enumerate}

В нашей работе мы всегда явно указываем, идёт ли речь о независимом или сопряжённом априорном распределении. Однако при чтении многих других работ надо быть внимательным, зачастую авторы говорят о «нормальном-обратном Уишарта» распределении, не уточняя, какое имеется ввиду.

\subsection{Независимое нормальное-обратное Уишарта сопряжение}

Априорно предполагается, что:

\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma \sim \cIW(\prior S; \prior\nu) \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}


В этом случае можно показать (ссылка), что условные апостериорные распределения имеют вид:

\begin{equation}
\begin{cases}
\phi | \Sigma, Y \sim \cN(\post \phi; \post \Xi ) \\
\Sigma | \phi, Y \sim \cIW(\post S; \post\nu) \\
\end{cases}
\end{equation}
где 
\begin{align*}
&\post\nu = \prior \nu + T \\
&\post S=\prior S + E'E, \text{ где } E=Y-X\Phi \\
&\post \Xi = (\prior \Xi^{-1} + \Sigma^{-1}\otimes X'X)^{-1} \\
&\post \phi = \post \Xi \cdot (\prior \Xi^{-1} \prior \phi + \vec(X'Y\Sigma^{-1}))
\end{align*}







Гиперпараметры априорного распределения традиционно выбираются следующим образом:

....


Получить Марковскую цепь, сходящуюся к апостериорному распределению можно, например, так:

\begin{enumerate}
\item Сгенерировать произвольно стартовую матрицу $\Sigma^{[0]}$, например, единичную
\item На $j$-ом шаге сгенерировать очередную итерацию согласно:

\begin{align}
&\phi^{[j]}\sim \cN(\post \phi^{[j-1]}; \post \Xi^{[j-1]}), 
\text{ где }  \post \phi^{[j-1]} \text{ и } \post \Xi^{[j-1]} \text{ рассчитываются через  } \Sigma^{[j-1]}\\
&\Sigma^{[j]} \sim \cIW(\post S^{[j]}; \post\nu), 
\text{ где } \post S^{[j]}  \text{ рассчитываются через } \phi^{[j]}
\end{align}

\item Увеличить $j$ на единицу и перейти к пункту два
\end{enumerate}



\subsection{Априорное распределение Миннесоты}

Хотя априорное распределение Миннесоты и является частным случаем независимого нормального-обратного Уишарта распределения, имеет смысл рассмотреть его отдельно. Оно было предложено в работе  \cite{litterman_1986_forecasting} и \cite{doan_litterman_sims_1984_forecasting}.

Изначально \cite{litterman_1986_forecasting} предложил использовать априорное распределение Миннесоты, при котором в явном виде учитывается предпосылка о нестационарности большинства макрорядов.


Априорное распределение параметров предполагается многомерным нормальным, зависящим от нескольких гиперпараметров. Параметры предполагаются независимыми, следовательно, их ковариационная матрица диагональна. Ковариационная матрица вектора $\varepsilon_t$ также предполагается диагональной. Запишем задачу $\eqref{var}$ в векторизованном виде\footnote{Третье уравнение системы следует из тождества: $\vec(ABC)=(C\otimes A)\vec (B)$}:
\begin{gather*}
Y=X\Phi+E\\
\vec(Y)=\vec(X\Phi I)+\vec(E)\Leftrightarrow \\
%\vec(Y)=(I_m\otimes X)\vec(Phi)+\vec(E
%\intertext{либо}
y=(I_M\otimes X)\phi + \varepsilon
\end{gather*}
где $\varepsilon  \sim \cN(0,\Sigma\ \otimes\ I_T)$ и вектор $\phi=\vec{\Phi}$ имеет размерность $km\times 1$.

Тогда:
\begin{equation}
\phi\sim \cN(\prior \phi, \prior \Xi)
\end{equation}

Априорная плотность распределения $\phi$ может быть записана как:
\begin{equation}
p(\phi)=\frac{1}{(2\pi)^{km/2}|\Xi|^{1/2}} \exp \left\lbrace-\frac{1}{2}(\phi-\prior \phi)'\Xi^{-1}(\phi-\prior \phi ) \right\rbrace.
\end{equation}

Комбинируя её с функцией правдоподобия \eqref{likelihood}, получаем, что апостериорное распределение параметров задаются в следующем виде:
\begin{equation}
\phi|Y\sim \cN(\post{\phi},\post \Xi)
\end{equation}

где
\begin{align*}
&\post \Xi=[\prior \Xi^{-1}+\Sigma^{-1}\otimes(X'X)]^{-1}\\
& \post \phi=\post{\Xi}[\prior \Xi^{-1}\prior \phi+(\Sigma^{-1}\otimes X')y].
\end{align*}

????? почему тут игрек мелкий????

Если $\prior\Xi$ имеет структуру кронекерова произведения, $\prior\Xi = \Sigma \otimes \prior{\Omega}$, то формулы можно существенно упростить и обойтись обращением матриц меньшей размерности:

\begin{multline}
\post \Xi=[\prior \Xi^{-1}+\Sigma^{-1}\otimes(X'X)]^{-1}=[(\Sigma \otimes \prior\Omega)^{-1}+\Sigma^{-1}\otimes(X'X)]^{-1}=\\
=[\Sigma^{-1}\otimes  \prior\Omega^{-1} +\Sigma^{-1}\otimes(X'X)]^{-1}=\Sigma \otimes (\prior\Omega^{-1}+ X'X)^{-1} = \Sigma \otimes \post \Omega
\end{multline}

....

В результате получаем 

\[
\Phi | Y \sim \cN (\post\Phi, \Sigma \otimes \post\Omega
\]





На практике в качестве матрицы $\Sigma$ используют её оценку $\hat\Sigma$, диагональные элементы которой равны: $\hat\sigma_{1}^2, \hat\sigma_{2}^2,\ldots,\hat\sigma_{m}^2 $, где  $\hat\sigma_{i}^2$ --- оценка дисперсии случайной составляющей в $AR(p)$ модели для ряда $i$.

Априорное распределение Миннесоты было задумано таким образом, чтобы учесть нестационарность многих макроэкономических временных рядов. В этом априорном распределении предполагается, что диагональные элементы матрицы первого лага $\Phi_1$ имеют матожидание единица, а остальные элементы матрицы первого лага и все элементы остальных матриц равны нулю, т.е.:

\begin{equation}
\prior{\Phi}=\E[(\Phi_l)_{ij}|\Sigma]=
\begin{cases}
1,& i=j, l=1;\\
0,&\text{ в остальных случаях }
\end{cases}\label{minnesota_exp}
\end{equation}

%%% здесь про дельта i и кто их как берет


Однако это априорное распределение может быть обобщено до случая, когда все диагональные элементы матрицы первого лага имеют матожидание $\phi_{ii}$, где $\phi_{ii}$ принимает значение единица для нестационарных рядов и меньше единицы для стационарных (рядов с высокой степенью персистентности).

Априорное распределение Миннесоты предполагает, что априорная ковариационная матрица $\prior \Xi$ диагональна.
Пусть $\prior \Xi_i$ обозначает блок $\prior \Xi$, размера $k\times k$, связанный с коэффициентами уравнения $i$. Тогда  диагональные элементы  $\prior \Xi_i$ определяются по формулам:

\begin{equation}
\Var((\Phi_l)_{ij})=\begin{cases}
\frac{\lambda_1^2}{l^2}, \; \text{ для переменной  } i \text{ и лага } l,\\
\frac{\lambda_1^2\lambda_2^2\sigma^2_i}{l^2\sigma^2_j}, \; \text{ для переменной } j\neq i \text{ и лага } l, \\
\lambda_3^2\sigma_i^2, \; \text{ для коэффициентов при экзогенных переменных }
\end{cases}
\label{minnesota_variance}
\end{equation}

%\begin{itemize}
%\item $\frac{\lambda_1}{l^2}$ --- для переменной $i$ и лага $l$
%\item $\frac{\lambda_1\lambda_2\sigma^2_i}{l^2\sigma^2_j}$ --- для переменной $j\neq i$ и лага $l$
%\item $\lambda_3\sigma_i^2 \label{minnesota_variance}$ --- для коэффициентов при экзогенных переменных
%\end{itemize}

Как можно видеть из приведенной выше формулы \eqref{minnesota_variance} априорная дисперсия параметров зависит от нескольких гиперпараметров, задаваемых исследователем. Гиперпараметры имеют следующую интерпретацию: $\lambda_1$ (параметр регуляризации) отражает общую <<жесткость>> априорного распределения. Если $\lambda_1\to 0$, то априорное распределение полностью определяет апостериорное распределение, и данные не играют никакой роли при оценке параметров. Наоборот, если $\lambda_1\to \infty$, то априорное распределение перестает влиять и оценка параметров сходится к обычной оценке МНК. Параметр $\lambda_2$ (параметр кросс-регуляризации) добавляет дополнительную жесткость лагам других переменных по сравнению с лагами зависимой переменной. Если  $\lambda_2<1$, то собственные лаги зависимой переменной помогают предсказывать значение переменной лучше, чем лаги других переменных. Параметр $\lambda_3$ отражает относительную жесткость распределения константы.

При $\lambda_2=1$ матрица $\prior \Xi$ имеет структуру кронекерова произведения и представима в виде
\[
\prior \Xi = \Sigma \otimes \prior \Omega,
\]
где $\prior \Omega$ --- матрица размера $k\times k$, соответствующая отдельному уравнению. Кронекерово домножение слева на матрицу $\Sigma$ для $i$-го уравнения означает домножение дисперсий, указанных в матрице $\prior \Omega$, на коэффициент $\sigma^2_i$.  Начало диагонали матрицы $\prior{\Omega}$ соответствует коэффициентам для лага равного $1$, затем коэффициентам  для лага равного $2$ и т.д. Конец диагонали матрицы $\prior{\Omega}$ соответствует экзогенным переменным. Таким образом диагональ матрицы $\prior{\Omega}$ состоит из вектора $\left(\frac{\lambda_1^2}{l^2 \sigma_1^2}, \frac{\lambda_1^2}{l^2 \sigma_2^2}, \ldots, \frac{\lambda_1^2}{l^2 \sigma_m^2}   \right)$, повторенного $p$ раз, и в конце стоит число $\lambda_3^2$.


Преимущества априорного распределения Миннесоты хорошо известны. Во-первых, оно просто задается, во-вторых, оно успешно применялось в литературе для решения различных задач. В-третьих, получившееся апостериорное распределение является нормальным, и значит, легко можно получить значение любой функции параметров с помощью методов Монте-Карло. Однако существенным недостатком этого распределения является то, что оно не предполагает использования байесовской процедуры для оценки $\Sigma$.


Необходимости применять алгоритм Гиббса для априорного распределения Миннесоты нет. Алгоритм генерации случайной выборки непосредственно из апостериорного распределения прост:

\begin{enumerate}
\item На $j$-ом шаге сгенерировать очередную итерацию согласно:
\begin{equation}
\phi^{[j]}\sim \cN(\post \phi; \post \Xi)
\end{equation}
\item Увеличить $j$ на единицу и перейти к пункту один
\end{enumerate}

Если $\prior \Xi$ имеет структуру кронекерова произведения, $\prior \Xi = \Sigma \otimes \prior \Omega$, то вместо вектора $\phi^{[j]}$ можно генерировать матрицу $\Phi^{[j]}$ численно более простым алгоритмом:


\begin{enumerate}
\item Генерируют матрицу $V$ размера $k\times m$  из независимых стандартных нормальных величин
\item Считают матрицу $\Phi^{[j]}$ по формуле:
\[
\Phi^{[j]} = \post \Phi + \chol(\post\Omega) \cdot V \cdot \chol(\Sigma^{[j]})'
\]
\end{enumerate}


\subsection{Сопряженное нормальное-обратное Уишарта априорное распределение}

Указанного недостатка априорного распределения Миннесоты можно избежать, если рассматривать сопряженное априорное распределение, т.е. распределение, при котором  априорное распределение, функция правдоподобия и апостериорное распределение принадлежат одному классу. Т.к. функция правдоподобия может быть разбита на две части, одна из которых соответствует нормальному распределению (при условии известной ковариационной матрицы остатков), а другая --- обратному распределению Уишарта, то и~сопряженным априорным распределением для рассматриваемой модели будет также нормальное-обратное Уишарта распределение.


Априорное нормальное-обратное Уишарта распределение может быть записано как:
\begin{equation}
\begin{cases}
\Sigma\sim \cIW(\prior S, \prior \nu) \\
\phi|\Sigma\sim \cN (\prior \phi, \Sigma\otimes\prior \Omega)
\end{cases}
\end{equation}


Можно показать, что с учетом функции правдоподобия \eqref{likelihood} апостериорное распределение принадлежит к тому же классу (см, например, \cite{zellner_1996_introduction}):
\begin{equation}
\begin{cases}
\Sigma|Y \sim \cIW(\post S, \post \nu) \\
\Phi|\Sigma,Y\sim \cN (\post \Phi, \Sigma\otimes\post \Omega) 
\end{cases}
\end{equation}

где
\begin{align*}
&\post\nu =\prior \nu+T\\
&\post{\Omega}=(\prior \Omega^{-1}+X'X)^{-1}\\
&\post \Phi=\post{\Omega}\cdot (\prior \Omega^{-1}\prior \Phi+X'Y)\\
&\post S=\prior S +\hat E'\hat E+\hat \Phi X'X \hat \Phi +\prior \Phi'\prior\Omega^{-1}\prior \Phi-\post \Phi'\post\Omega^{-1}\post \Phi\\
&\hat\Phi=(X'X)^{-1}X'Y\\%\quad\text{и }
&\hat E=Y-X\hat\Phi
\end{align*}


Существует достаточно популярный альтернативный подход для подсчёта гиперпараметров апостериорного распределения.


Мы обнуляем матрицы  $\prior S$ и $\prior \Omega^{-1}$, а чтобы компенсировать разницу добавляем дополнительные наблюдения в матрицу $X$ и в матрицу $Y$: 

\[
X^*=\begin{bmatrix}
X^{+} \\
X 
\end{bmatrix}, \;
Y^*=\begin{bmatrix}
Y^{+} \\
Y 
\end{bmatrix}
\]

Отметим, что матрицы $X$ и $Y$ входят в гиперпараметры апостериорного распределения только в составе матриц $X'X$, $X'Y$ и $Y'Y$, поэтому абсолютно не важно, в каком порядке добавлять искусственные наблюдения. Можно их добавить в конец матриц $X$ и $Y$, можно в начало, можно в крапинку посередине.

Получим новые формулы для апостериорных гиперпараметров:


ВЫВЕРИТЬ!

\begin{align*}
&\post\nu =\prior \nu+T\\
&\post{\Omega}=(X^{*\prime}X^*)^{-1}=(X^{+\prime}X^+ + X'X)^{-1}\\
&\post \Phi=\post{\Omega}\cdot (X^{*\prime}Y^*)=\post{\Omega}\cdot (X^{+\prime}Y^++X'Y)\\
&\post S=\hat E'\hat E+\hat \Phi X'X \hat \Phi -\post \Phi'\post\Omega^{-1}\post \Phi\\
&\hat\Phi=(X'X)^{-1}X'Y\\%\quad\text{и }
&\hat E^*=Y^*-X^*\hat\Phi
\end{align*}

%\begin{align*}
%&\post\nu = \prior \nu  + T \\
%&\post S= E^{*\prime}E^*, \text{ где } E^*=Y^*-X^*\Phi \\
%&\post \Xi = ( \Sigma^{-1}\otimes X^{*\prime}X^*)^{-1} \\
%&\post \phi = \post \Xi \cdot ( \vec(X^{*\prime}Y^*\Sigma^{-1}))
%\end{align*}

Фокус заключается в том, что наблюдения добавляются так, что гиперпараметры апостериорных наблюдений не изменяются. Заметим, что $X^{*\prime}X^*=X^{+\prime}X^+ + X'X$, $E^{*\prime}E^*=E^{+\prime}E^+ + E'E$.

Чтобы новые формулы совпадали со старыми необходимо, чтобы

\begin{equation}
\begin{cases}
X^{+\prime}X^+=\prior \Omega^{-1} \\
X^{+\prime}Y^+=\prior \Omega^{-1}\prior \Phi \\
...
\end{cases}
\end{equation}







Заметим, что новые формулы позволяют трактовать:
\begin{itemize}
\item $\post \Phi$ --- как результат построения регрессий $Y^*$ на $X^*$:
\[
\post \Phi=(X^{*\prime}X^*)^{-1}\cdot (X^{*\prime}Y^*)
\]
\item $\post S$ --- как ...
\end{itemize}




Эти условия будут выполнены если добавить наблюдения по схеме:

В работах  \cite{banbura_al_2010_large}, \cite{berg_henzel_2013_point}) соответствующее априорное распределение вводится путем добавления искусственных наблюдений:
\begin{equation}
y^{+}=\begin{bmatrix}
\diag(\delta_1\sigma_1,\ldots, \delta_n\sigma_n)\\
0_{n(p-1)\times n}\\
\diag(\sigma_1,\ldots,\sigma_n)\\
0_{1\times n}
\end{bmatrix}
\qquad
%
x^{+}=\begin{bmatrix}
\diag(1,2,\ldots, p)\otimes \diag(\sigma_1,\ldots,\sigma_n)/\lambda & 0_{np\times 1} \\
0_{n\times np}&0_{n\times 1}\\
0_{1\times np}&\epsilon
\end{bmatrix}
\end{equation}



При этом математические ожидания и дисперсии априорного распределения параметров могут быть заданы по тому же принципу, что и в априорном распределении Миннесоты (см. \eqref{minnesota_exp}-\eqref{minnesota_variance}).
%При этом априорные распределения параметров, указанные в \eqref{minnesota_exp}-\eqref{minnesota_variance}), предполагают наличие  нулевой априорной корреляции между коэффициентами.

...


В работах \cite{doan_litterman_sims_1984_forecasting} and \cite{sims_1993_nine} было предложено добавить к этим априорным распределениям дополнительную характеристику, введение которой обуславливается возможным наличием во временных рядах единичных корней и коинтеграционных  соотношений. Это позволяет исключить появление неправдоподобно большой доли внутривыборочной дисперсии, объясняемой экзогенными переменными \cite{carriero_al_2015_bayesian}. %%% ???




Как и в случае распределения Миннесоты, необходимости использовать алгоритм Гиббса нет, можно генерировать случайную выборку непосредственно из апостериорного распределения. Например, можно применять такой алгоритм:


\begin{enumerate}
\item На $j$-ом шаге сгенерировать очередную итерацию согласно:
\begin{align*}
&\Sigma^{[j]} \sim \cIW(\post S, \post \nu) \\
&\phi^{[j]}\sim \cN(\post \phi; \Sigma^{[j]} \otimes \post \Omega)
\end{align*}
\item Увеличить $j$ на единицу и перейти к пункту один
\end{enumerate}

На практике вместо генерирования вектора $\phi^{[j]}$ генерируют сразу матрицу $\Phi^{[j]}$ в два шага:

\begin{enumerate}
\item Генерируют матрицу $V$ размера $k\times m$  из независимых стандартных нормальных величин
\item Считают матрицу $\Phi^{[j]}$ по формуле:
\[
\Phi^{[j]} = \post \Phi + \chol(\post\Omega) \cdot V \cdot \chol(\Sigma^{[j]})'
\]
\end{enumerate}




\subsection{Соответствие гиперпараметров в разных работах}


\begin{center}
\begin{tabular}{cccc}
\toprule
A &  CCM15 &  BGR10, BH13 & Формула? \\
\midrule
% тут базовые %
$\lambda_{tight}$ & $\lambda_1$ & $\lambda$ & \\
$\lambda_{kron}$ & $\lambda_2=1$ & $\vartheta=1$ & \\
$\lambda_{power}$ & $1$ & $1$ & \\
$\lambda_{const}$  & $\lambda_0$ & $\infty$ & \\
$\lambda_{exo}$ & NA & NA & \\
\midrule
$\lambda_{sc}$ & $\lambda_3$ & $\tau$ & \\
$\lambda_{io}$ & $\lambda_4$ & NA & \\
\bottomrule
\end{tabular}
\end{center}

Доступные реализации кода: (или в предыдущую): список реализаций и что какое распределение они реализуют







\subsection*{Модификация априорных распределений}

Априорное распределение суммы коэффициентов\footnote{sum-of-coefficints prior} было предложено в работе  \cite{doan_litterman_sims_1984_forecasting}. Это распределение отражает следующую идею:
когда среднее значение лагированных значений какой либо переменной находится на некотором уровне $\post y_{0i}$, то это же самое значение  $\post y_{0i}$, является хорошим прогнозом  для будущих значений этой переменной. В качестве  $\post y_{0i}$ мы используем среднее значение переменной  $\post y_{i}$ по первым  $p$ наблюдениям. Внедрение этого априорного распределения производится путем добавления искусственных дамми-наблюдений по следующей схеме:
\begin{gather*}
y_d(i,j)=\begin{cases}
\post y_{0i}/\lambda_3, \quad \text{если }i=j\\
0 \quad \text{в обратном случае}
\end{cases}
x_d(i,s)=\begin{cases}
\post y_{0i}/\lambda_3,\quad \text{если }i=j, s<km\\
0 \quad \text{в обратном случае,}
\end{cases}
\end{gather*}
где $i,j=1,\ldots,m$, $s=1, \ldots, km$. Когда $\lambda_3\to 0$, модель стремится к виду, предполагающему запись в разностях, т.е. единичных корней становится столько же, сколько переменных, и нет коинтеграции.

Другими словами/ другой вариант введения (оставить нужно тот, который будет понятнее):

\begin{equation}
y^{++}=\begin{bmatrix}\diag(\mu_1,\ldots,\mu_n)/\tau\end{bmatrix}\qquad
x^{++}=\begin{bmatrix}(1,2\ldots p)\otimes \diag(\mu_1,\ldots,\mu_n)/\tau &0_{n\times 1}\end{bmatrix}
\end{equation}


Априорное распределение изначального наблюдения\footnote{dummy initial observation prior }, предложенное в работе \cite{sims_1993_nine} означает, что исследователь вводит единственное дамми-наблюдение, такое, что все значения всех переменных равны соответствующему среднему начальных условий с точностью до коэффициента масштаба $1/\lambda_4$. Это происходит  путем добавления в систему дамми-наблюдений следующего вида:
\begin{gather*}
y_d(j)=\begin{cases}\post y_{0i}/\lambda_4 \qquad
\post y_{0i}/\lambda_4\quad \text{если }s<km\\
1/\lambda_4 \quad \text{в обратном случае,}
\end{cases}
\end{gather*}
где $j=1,\ldots m, s=1,\ldots km$. Когда $\lambda_4\to 0 $ модель принимает вид, в котором либо все переменные стационарны со средним, равным выборочному среднему начальных условий, либо существуют коинтегрированные ряды с  единичным корнем но без дрейфа.

\subsection{Выбор гиперпараметров и числа лагов}

Как было показано в работе \cite{demol_al_2008_forecasting} и подтверждено в других более поздних работах,использование сравнительно большого количества временных рядов требует уменьшения параметра $\lambda_1$ с увеличением размерности выборки, что означает наложение более жесткого априорного распределения. На данный момент в литературе используется два подхода к определению оптимальной величины $\lambda_1$. В своей работе мы используем оба и сравниваем качество прогноза.

Первый алгоритм был предложен в работе \cite{banbura_al_2010_large} и он основан на идее о том, что регуляризация должна быть настолько жесткой, чтобы не исключить возможность избыточной параметризации модели, при этом предполагается, что трехмерная VAR - достаточно простая (parsimonious) модель, не содержащая слишком большого количества параметров. Процедура выбора $\lambda$ состоит в том, что что средний внутривыборочный прогноз для реального ВВП и индекса цен тот же самый, как на первой выборке (на которой происходит) оценивание. Т.е. каждая модель регуляризуется до размера простой VAR. При этом  референтной моделью является та, для которой апостериорное распределение не зависит от функции правдоподобия, т. е. для которой $\lambda=0$. Это означает, что дисперсии всех параметров $\phi$ равны нулю, т. е. переменные описываются моделью случайного блуждания (RW) со смещением $(y_{i,t}=c+y_{i,t-1 +\varepsilon_t}, i=1,\ldots,m$. Обозначим эту модель индексом 0,  т.к. $\lambda=0$.
Схема выбора $\lambda$ состоит из следующих этапов:
\begin{enumerate}
\item На первом этапе  строятся внутривыборочные однопериодные прогнозы на обучающей выборке и рассчитывается среднеквадратичная ошибка прогноза выпуска $(MSFE_y^{0})$ и инфляции $(MSFE_{\pi}^{0})$.
\item Оценивается трехмерная VAR для $\lambda\to \infty$ \footnote{При $\lambda\to \infty$ оценки BVAR совпадают с оценками VAR методами OLS или ML, т. к. апостериорное распределение параметров в этом случае совпадает с функцией правдоподобия. Считается, что трехмерная VAR содержит достаточно маленькое число параметров, и байесовская регуляризация не требуется.} и рассчитываются среднеквадратичная ошибка прогноза выпуска $(MSFE_y^{\infty})$ и инфляции $(MSFE_{\pi}^{\infty})$ и показатель $FIT^{\infty}$:
\begin{equation}
FIT^{\infty}=\frac{1}{2}\cdot\frac{MSFE_y^{\infty}}{MSFE^0_y} + \frac{1}{2}\cdot\frac{MSFE_{\pi}^{\infty}}{MSFE^0_{\pi}}
\end{equation}
\item Оцениваются BVAR модели для $m$ переменных и для большого числа различных $\lambda$ рассчитываются среднеквадратичные ошибки прогноза для выпуска $(MSFE_y^{\lambda,m})$ и инфляции $(MSFE_{\pi}^{\lambda,m})$ и показатель $FIT^{{\lambda,m}}$: 
\begin{equation}
FIT^{\lambda,m}=\frac{1}{2}\cdot\frac{MSFE_y^{\lambda,m}}{MSFE^0_y} + \frac{1}{2}\cdot\frac{MSFE_{\pi}^{\lambda,m}}{MSFE^0_{\pi}}
\end{equation}
\item Оптимальное $\lambda$ рассчитывается как значение, при котором минимизируется отклонение $FIT^{\lambda,m}$ от $FIT^{\infty}$:
\begin{equation}
\lambda^*=\argmin |FIT^{\lambda, m}-FIT^{\infty}| 
\end{equation}
\end{enumerate}
После того как выбрано оптимальное $\lambda$ для каждой модели, происходит построение вневыборочных прогнозов на оценивающей выборке.


Второй алгоритм предложен в работе (Carriero et al, 2012) и представляет собой выбор такого параметра $\lambda_1$, который бы максимизировал функцию предельной плотности:
\begin{equation}
\lambda_{1t}^*=\argmax_{\lambda_1} \ln p(Y)
\end{equation}

При этом функция предельной плотности может быть получена путем интегрирования коэффициентов модели: \footnote{Т.к. интегрирование происходит по  всем коэффициентам, но не по гиперпараметрам априорного распределения ($\lambda_1,\lambda_2,\lambda_3$)  и не по числу лагов $p$, то предельная плотность является функцией $\lambda_j,j=1\ldots 3$ и $p$.}
\begin{equation}
p(Y)=\int p(Y|\phi)p(\phi) d \phi
\end{equation}
Если априорное распределение является нормальным - обратным Уишарта, то предельная плотность $p(Y)$ может быть посчитана аналитически (\cite{zellner_1996_introduction}; Bauwens et al,1999; Carriero et al,2012):
%\begin{equation}
\begin{multline}
p(Y)=\pi^{-\frac{Tm}{2}}\times +\left |(I+X\prior{\Omega} X')^{-1}\right|^{\frac{N}{2}}\times |\prior{S}|^{\frac{\prior{\nu}}{2}}\times \frac{\Gamma_N(\frac{\prior{\nu}+T}{2})}{\Gamma_N(\frac{\prior{\nu}}{2})}\times \\ \times
\left|\prior{S}+(Y-X\prior{\Phi})'(I+X\prior{\Omega} X')^{-1}(Y-X\prior{\Phi})\right|^{-\frac{\prior{\nu}+T}{2}},\label{marginal_density}
\end{multline}
где $\Gamma_N(\cdot)$ обозначает $N$-мерную гамма функцию.
Выбор числа лагов происходит аналогично путем максимизации по $p$ функции предельной плотности \eqref{marginal_density}: 
\begin{equation}
p^*=\argmax_{p} \ln p(Y)
\end{equation} 


\subsection{Особенности кодирования}

При практической реализации алгоритма Гиббса или Монте-Карло часто приходится обращать положительно определённые симметричные матрицы. Для этого можно использовать следующий способ:

\begin{enumerate}
\item Получить разложение Холецкого для заданной матрицы $A$
\[
A=U'U,
\]
где $U$ --- верхнетреугольная матрица
\item Обратить матрицу $U$. Существуют специальные алгоритмы обращения верхнетреугольных матриц.
\item Получить $A^{-1}$ по формуле
\[
A^{-1}=U^{-1}U^{-1\prime}
\]
\end{enumerate}

Однако даже данный способ сопряжён с численными трудностями, если обращаемая матрица плохо обусловлена. Например, при большом количестве эндогенных переменных и большом количестве лагов матрица $X'X$ является плохо обусловленной и для неё может быть численно трудно получить разложение Холецкого. В таком случае мы использовали псевдообратную матрицу Мура-Пенроуза.


\section{Данные}

Для расчетов мы используем 24 временных ряда с января 1995г. по апрель 2015г. Границы выборки обусловлены доступностью данных, исходная выборка содержит 244 наблюдения. Полный список взятых временных рядов указан в Приложении 2. После устранения сезонности в рядах, демонстрирующих сезонные колебания, мы логарифмируем всех ряды кроме процентной ставки. Далее происходит проверка на стационарность, для чего используются ADF и KPSS тесты. Такая проверка необходима, для того чтобы определить матожидание априорного  распределения для параметров $\Phi_1$. Следуя методологии других работ, посвященных прогнозированию с помощью BVAR (например ()()), мы назначаем $(\Phi_1)_{ii}=1$  для  нестационарных рядов и $(\Phi_1)_{ii}=0$ для стационарных. 
На втором этапе мы оцениваем три rolling VAR модели для разного набора переменных и строим по ним прогнозы. Период  оценивания составляет всегда 120 месяцев, Прогноз строится на 1, 3, 6 и 12 месяцев.  Мы строим VAR  для разного количества переменных: для 3, 6 и 24\footnote{Обычная VAR для сравнения качества прогноза строится для 3 и 6 переменных}. Модель с тремя переменными рассматривается как наиболее простая модель, содержащая только самые важные  переменные: показатель деловой активности (индекс промышленного производства),  индекс цен (подсчитанной с помощью ИПЦ) и  инструмента монетарной политики (в качестве прокси для которого мы берем процентную ставку межбанковского рынка). Модель с шестью переменными специфицируется по аналогии со многими монетарными моделями, использовавшимися для структурного анализа различных экономик (Sims, 1992; Kim and Roubini (2000); Bjornland (2008); Uhlig and Scholl (2008) и включает  в дополнение к уже указанным переменным валютный курс, денежный агрегат М2 и цены на нефть (последний показатель включен для отражения экспортоориентированности российской экономики). В модель с 24 переменными строится с использованием всех доступных временных рядов. 
При определении параметра жесткости распределения по методу Banbura et al.()необходимо выделить период, на котором происходит определение $\lambda$. Мы определяем $\lambda$ на самом раннем доступном промежутке: с января 1996г. по декабрь 2005г (первые 12 месяцев используются в качестве лаговых значений переменных для этой регрессии). Количество лагов определяется путем минимизации информационных критериев для обычной VAR на той же самой подвыборке. (что делать с вар по 24 рядам??) Далее количество лагов и параметр жесткости фиксируются и используются для построения прогнозов на всех остальных подвыборках. 
При определении параметра жесткости путем максимизации предельной плотности данных (marginal data density) при оценке каждой BVAR происходит совместный поиск на сетке по $\lambda$ и по $p$, и выбираются такие значения, для которых \eqref{marginal_density} максимально.  
\newpage

%\begin{appendices}
\section*{Приложения}
\subsection*{Данные}
\begin{center}
\begin{table}[h!]
\begin{tabular}{lccr}
\toprule
Название временного ряда& Тип данных &  База (если есть) & Источник \\
\midrule
Индекс промышленного производства & Базисный индекс & 2010 & IFS \\
Индекс потребительских цен & Базисный индекс & 2010 & IFS \\
Индекс занятости в промышленности & Базисный индекс & 2010 & IFS \\
Процентная ставка межбанковского рынка & В процентах годовых &  & IFS \\
Процентная ставка по кредитам & В процентах годовых &  & IFS \\
Индекс реальных денежных доходов & Базисный индекс & январь 1992 & ФСГС\\
Уровень безработицы & В процентах &  & IFS \\
Индекс цен на нефть марки Brent & Базисный индекс & 2010 & IFS \\
Индекс цен производителей & Цепной индекс &  & IFS \\
Ввод в действие новых жилых домов & В тыс.кв.м. &  & ФСГС \\
Индекс реальных инвестиций в основной капитал& Базисный индекс & январь 1994 & ЦАД \\
Индекс реальных зарплат & Базисный индекс & январь 1993 & ФСГС \\
Денежный агрегат М2 & в млрд. руб.  &  & ЦБ \\
Реальный эффективный валютный курс & Базисный индекс & 2010 & IFS \\
Цена натурального газа & Долл. за млн. БТЕ & 2010 & IFS \\
Международные резервы за исключением золота & Млрд. долл. &  & IFS \\
Номинальный валютный курс & руб. за долл. &  & IFS \\
Заявленная потребность в работниках & Тыс. чел. &  & ЦАД \\
Индекс реального объема сельхозпроизводства & Базисный индекс & январь 1993 & ЦАД\\
Индекс реального объема розничной торговли & Базисный индекс & январь 1994 & ЦАД\\
Сальдо консолидированного бюджета &  &  & ЦАД \\
Экспорт товаров & млн. долл. &  & IFS \\
Импорт товаров & млн. долл. &  & IFS \\


\end{tabular}
\end{table}
\end{center}
%\end{appendices}


\printbibliography

\end{document} 
