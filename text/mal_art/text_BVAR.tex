% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{article} % use larger type; default would be 10pt


%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or\ldots .
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information


% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{url}
\usepackage{soulutf8}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another\ldots


%%% HEADERS & FOOTERS
%\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
%\pagestyle{fancy} % options: empty , plain , fancy




%%My Additional Packages
\usepackage{mathtext}          % русские буквы в формулах
\usepackage[T2A]{fontenc}            % внутренняя кодировка  TeX
\usepackage[utf8]{inputenc}         % кодировка исходного текста
% \usepackage{cmap}          % русский поиск в pdf
\usepackage[english,russian]{babel} % локализация и переносы
\usepackage{amsmath} % Математические окружения AMS
\usepackage{amsfonts} % Шрифты AMS
\usepackage{amssymb} % Символы AMS
\usepackage{graphicx} % Вставить pdf- или png-файлы

\usepackage{euscript} % Красивый шрифт

\usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

\usepackage{indentfirst} % Отступ в первом абзаце.
\usepackage{pdflscape} %Переворачивает страницы, удобно для широких таблиц
\usepackage{footnote} %сноски в таблицах
\makesavenoteenv{tabular}
\makesavenoteenv{table}

%\usepackage[backend=biber, style=authoryear, citestyle=authoryear]{biblatex}
\usepackage[backend=biber, style=bwl-FU, citestyle=bwl-FU]{biblatex}


\addbibresource{bibliobase2.bib}

\usepackage{wrapfig} % Обтекание рисунков текстом

\usepackage{hyperref} % Гиперссылки

\DeclareMathOperator{\etr}{etr}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Var}{\mathbb{V}\mathrm{ar}}
\DeclareMathOperator{\chol}{chol}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cIW}{\mathcal{IW}}
\newcommand{\lag}{\EuScript{L}}

\newcommand{\prior}{\underline}
\newcommand{\post}{\overline}

\let\vec\relax
\DeclareMathOperator{\vec}{vec}


%\usepackage[toc,page]{appendix}


%\renewcommand{\appendixtocname}{Приложения}
%\renewcommand{\appendixpagename}{Приложения}
%\renewcommand{\appendixname}{Приложение}
%%% END Article customizations

%%% The "real" document content comes below\ldots

\title{BVAR in Russia}
\author{я и Боря}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed

\begin{document}
\maketitle
\section{Введение для Карты BVAR}

Значение точных прогнозов для проведения макроэкономической политики трудно переоценить. Существование лагов политики приводит к тому, что решения, принятые сегодня, повлияют на 
экономику только через некоторое время, поэтому фискальным и монетарным властям при принятии решений приходится ориентироваться не на текущие, а на ожидаемые показатели. Точный прогноз макроэкономических показателей, таким образом, является одним из ключевых факторов успешной политики. На важность точных прогнозов при проведении полититики первым обратил внимание  Theil(1958)(нужно найти его работу и посмотреть, есть ли там эти пункты. На данный момент они взяты из Wieland). В частности, он писал, что власти  должны (1) предстказть будущее состояние экономики (2) оценить эффект изменения инструмента политики (3) на базе полученных прогнозов разработать план действий. (нужно ли это вообще?) 
В настоящее время основной моделью для прогнозирования макроэкономических временных рядов является модель векторной  авторегрессии (VAR). Использование векторных авторегрессий в макроэкономическом анализе явилось следствием критики активно использовавшихся прежде традиционных эконометрических моделей. В частности, Sims(1980) обратил внимание на необоснованность ограничений, вводимых в рамках традиционных моделей, и предложил использовать более простую по построению динамическую модель,основанную на разложении Вольда и не требующую введения никаких ограничений на взаимную динамику переменных - VAR. Модели этого класса стали широко использоваться как для прогнозирования, так и для структурного анализа благодаря своей логичности и относительной простоте.   Однако для того чтобы правильно отражать динамику фактических временных рядов, VAR часто требуется большое количество лагов, что, в свою очередь, может снизить эффективность оценивания(?) на коротких выборках и привести к высоким ошибкам/высокой неопределенности прогноза. Проблема усугубляется тем, что в реальности при проведении политики центральные банки развитых стран ориентируются на большое количество показателей, и VAR малой размерности не может отразить всей информации, доступной центральным банкам. При этом увеличение числа переменных в VAR приводит к тому, что количество оцениваемых параметров, растет нелинейно. Это усугубляет проблему неэффективности оценивания и высоких ошибок прогноза. 
Одим из решений этой проблемы стало использование априорной информации относительно распределения параметров и ковариационной матрицы ошибок, т. е. переход от обычных\footnote{В англоязычной литературе обычные VAR (без наложения априорных распределений) называются частотными --- frequentist.} VAR к Байесовским (Bayesian VAR, BVAR). 
  Исследователи выделяют два преимущества BVAR по отношению к обычным. Во-первых, этот класс моделей предлагают решение проблемы избыточной параметризации и благодаря этому позволяет включать в модель большее количество переменных. При этом априорные веры позволяют снизить неопределенность в распределении параметров модели и улучшить ее прогнозные способности. Во-вторых, распространенные в настоящее время  
априорные распределения отражают современные представления о  долгосрочной динамике переменных, не проявляющиеся в коротких выборках, обычно используемых для анализа. Это, в свою очередь, также улучшает точность полученных прогнозов. Нельзя также не отметить, что современные компьютеры осуществляют симуляции настолько быстро, что исследователи более не ограниченны необходимостью использования только сопряженных распределений, позволяющих получить явное аналитическое решение, что, безусловно, увеличивает привлекательность байесовского подхода и способствует его быстрому распространению, в частности, в макроэкономическом анализе.  (\cite{karlsson_2012}). 
Наиболее часто цитируемый недостаток байесовского подхода -- субъективность, с нашей точки зрения, не является существенным. Действительно, изменение априорного распределения влияет на  результаты анализа \footnote{Т.к. плотность апостериорного распределения представляет собой комбинацию плотности априорного распределения и функции правдоподобия. Этот вопрос подробно освещен в следующем разделе.}. Однако обычная небаесовская VAR (как и любая эконометрическая модель) является в неменьшей степени отражением субъективных представлений исследователя. Выбор весьма ограниченного набора переменных в модели, разделение их на эндогенные и экзогенные, определение числа лагов в модели\footnote{На практике для определения количества лагов в частотной VAR, как правило, исследователи ориентируются на информационные критерии. Однако довольно часто разные информационные критерии дают противоречивые результаты. В этом случае предпочтение определенного критерия всем остальным -- есть тоже субъективное решение.}  в любом случае отражают представления исследователя о правильной спецификации модели.    
К сожалению, несмотря на широкое распространение BVAR в академических статьях, количество практических обзоров этого метода  весьма ограничено. Существующие обзоры \cite{karlsson_2012}, \cite{delnegro_schorfheide_2011} и изложение в учебнике \cite{canova_2007} сильно математизированы и едва ли доступны для новичков без специальной математической подготовки.
 При этом ни в одном из них не содержится достаточно подробной классификации априорных распределений и ни к одному из них не прилагается инструкций для реализации предложенных методов в эконометрическом пакете. 
Исключениями являются обзоры \cite{koop_korobilis} и \cite{blake_mumtaz}.
Однако \cite{koop_korobilis_2010} не рассматривают ставший весьма популярным метод задания априорного распределения через добавление дополнительных наблюдений, в т. ч. априорное распределение суммы коэффициентов (sum-of-coefficients prior) и априорное распределение начального наблюдения (initial observation prior).  
\cite{blake_mumtaz_2012} используют терминологию, несколько отличающуюся от других работ, а код фактически содержит только пример построения BVAR только с одним типом распределения. Кроме того, сам обзор не лишен опечаток (стоит ли это писать? можем ли мы быть уверены, что наш -лишен??). При этом ни в одном из указанных обзор не рассмотрен достаточно подробно вопрос о прогнозировании с помощью BVAR (а именно это и является обычно целью их построения, по крайней мере, BVAR в сокращенной форме). На русском языке обзоров BVAR на данный момент, насколько нам  известно, вообще нет. 
Данный обзор нацелен на новичков в Байесовском анализе и содержит подробную классификацию априорных распределений, наиболее популярных при проведении макроэкономических исследований. Кроме того, к обзору прилагается код в R, в котором используются те же обозначения, что и в тексте работы, и который может быть использован как в учебных, так и в научных целей. 
Мы оставляем за рамками данного обзора построение структурных BVAR (SBVAR), BVAR c меняющимися параметрами (TVP-BVAR), BVAR со стохастической волатильностью, а также проблемы выбора переменных при построении BVAR. К этим вопросам мы вернемся в своих будущих работах. 
 План обзора следующий: ...










\section{Обзор литературы}

Построение точных  макроэкономических прогнозов является ключевым условием проведения верной политики центральными  банками.  Хорошо известно, что центральные банки развитых государств опираются на большое число макроиндикаторов при проведении политики (\cite{beckner_1996} или (\cite{bernanke_boivin_2003}) для США., какая-нибудь аналогичная работа для ЕЗ). Однако обычная векторно-авторегрессионная модель, ставшая наиболее часто встречающимся инструментом для построения прогнозов, не может учесть большое количество переменных, так как количество параметров, подлежащих оценке, растет нелинейно с увеличением числа уравнений.  При этом неучтенная при построении VAR информация может приводить к смещенным оценкам и  неверным выводам  как относительно прогнозируемых значений, так и виде функций импульсных откликов.  Основные способы учета большого числа переменных – это использование DF и Байсовских VAR.
Динамические факторы были предложены в работах \cite{forni_al_2000} и \cite{stock_watson_2002}.  В указанных работах предполагается, что дисперсия большого количества временных рядов может быть описана с помощью нескольких, искусственно построенных (common factors)  с помощью метода главных компонент.  Расширением метода    \cite{stock_watson_2002} служит FAVAR, предложенная в статье \cite{bernanke_al_2005}. В рамках FAVAR несколько динамических факторов  добавляются как дополнительные переменные в обычную VAR.
Цель данной работы состоит в построении  прогноза основных макроиндикаторов (выпуска, инфляции и др.) для российской экономики.  Задача осложняется отсутствием  большого количества длинных временных рядов, что не позволяет провести построение DFM.
Байесовские модели зарекомендовали себя как хороший инструмент построения прогнозов. В ряде работ было показано, что они обеспечивают более низкую ошибку прогноза, чем, например, обычные VAR и VECM.
(?) две стратегии – использовать shrinkage, который становится уже с ростом числа переменных
По Bloor and Matheson надо бы переписать.


Модификация Litterman prior была предложена в работах \cite{doan_al_1984}  (sum of coefficients prior) и \cite{sims_1993}    (co-persistence prior).  Комбинация этих трех априорных распределений была использована в работе \cite{robertson_tallman_1999}  для предсказания безработицы, темпа роста  и инфляции. В работе показано, что смешанное априорное распределение получает получить более точные прогнозы, чем априорные распределения \cite{litterman_1986} и \cite{sims_zha_1998} (про эту работу пока ничего не писала).


Ключевую роль в развитии подхода сыграла статья \cite{de_mol_al_2008}. В этой работе на основе асимптотического анализа было показано, что если данные характеризуются высокой мультиколлинеарностью (что характерно для выборок макрорядов большой размерности) сужение априорного распределения при увеличении количества переменных дает больший вес нескольким первым главным компонентам.  Это означает, что если данные характеризуются факторной структурой, то наложение более узких априорных распределений с увеличением размерности модели  не  приводит к потере важной информации, т.к. для описания данных достаточно небольшого количества первых факторов.  Эта точка зрения  была подтверждена и развита в статье \cite{banbura_al_2010}, в которой авторы  строят VAR модели для 3, 7, 20 и 131 переменных и показывают, что модели с большей размерностью демонстрируют лучшие прогнозные способности, чем модели малой размерности и даже FAVAR (? Проверить, об этом пишет Бошеман). Интересно отметить, что хорошая прогнозная способность достигается уже в модели с 20  переменными, поэтому как для прогнозирования, так и для структурного анализа достаточно сконцентрироваться на агрегированных данных.
Аналогичная модель для Новой Зеландии была построена в работе \cite{bloor_matheson_2010}, в которой они использовали метод условного прогнозного оценивания (\cite{waggoner_zha_1999}), что позволило им сравнить сценарии , основанные на различной условной информации.  Строят три модели (с 9, 13 и 35 переменными), делают вывод, что BVAR обладает более высокой предсказательной способностью, чем AR и обычная VAR модель.  Хотя результаты варьируют по разным переменным, в общем и целом, BVAR c большим числом переменных характеризуется более высокой точностью прогноза.
Из Beauchemin
\cite{koop_2010} расширил результаты \cite{banbura_al_2010}  и показал, что BVAR c большой размерностью обладают лучшей прогнозной способностью даже по отношению  к более сложным моделям (???)
Тот же метод построения априорного распределения (естественно-сопряженная версия Миннесоты-распределения (\cite{kadiyala_karlsson_1997},  \cite{sims_zha_1998}- проверить, они ли предложили? ), что в работах \cite{banbura_al_2010} , \cite{bloor_matheson_2010} и \cite{koop_2010} был применен в работе \cite{beauchemin_zaman_2011}. Они показывают, что BVAR с 16 переменными может быть с успехом использована как для прогнозов, так и для структурного анализа (трансмиссии монетарного шока (?)/cтруктурного анализа монетарной политики. Аналогичное построение априорного распределения используется в работе  \cite{alessandri_mumtaz_2014}, где с помощью линейной и нелинейной BVAR (?) модели показано, что учет финансовых индикаторов позволяет улучшить прогноз выпуска и инфляции, в т. ч. в кризисные периоды.

Во всех работах гиперпараметр, контролирующий жесткость (?) , выбирается таким образом, чтобы максимизировать функцию правдоподобия  данных ( это максимизирует точность вневыборочного прогноза ?). В работе \cite{geweke_whiteman_2006} было показано, что такой выбор гиперпараметра минимизирует ошибки прогноза на один период.


Сам Литтерман в своей работе показал, что использование априорного распределения (Bayesian shrinkage) в BVAR с  не менее  чем шестью переменными  улучшает прогнозную силу модели. Однако до~последнего времени считалось, что при использовании достаточно большого числа временных рядов уточнения правдоподобия только с помощью априорного распределения недостаточно.  Это приводило к необходимости задавать дополнительные ограничения.

\section{Методология}




\subsection{Удобная табличка}

\begin{center}
\begin{tabular}{ccp{6cm}l}
\toprule
Буква & Размер &  Описание & Формула \\
\midrule
$p$ & скаляр & количество лагов & \\
$m$ & скаляр & количество эндогенных переменных & \\
$d$ & скаляр & количество экзогенных переменных & \\
$k$ & скаляр & количество параметров в одном уравнении & $k=mp+d$ \\
$T$ & скаляр & количество наблюдений &  \\
\midrule
$z_t$ & $d \times 1$ & вектор экзогенных переменных (считая константу) & \\
$y_t$ & $m \times 1$ & вектор эндогенных переменных  & $y_t=\Phi' x_t+\varepsilon_t$ \\
$x_t$ & $k \times 1$ & вектор всех регрессоров & $x_t=[ y'_{t-1} \ldots  y'_{t-p} \; z'_t ]'$ \\
$\varepsilon_t$ & $m \times 1$ & вектор случайных ошибок  & \\
$Y$ & $T \times m$ & все эндогенные переменные & $Y=[y_1, y_2,\ldots, y_T]'$ \\
$X$ & $T \times k$ & матрица регрессоров& $X=[x_1, x_2,\ldots, x_T]'$ \\
$E$ & $T \times m$ & матрица ошибок & $E=[\varepsilon_1, \varepsilon_2,\ldots, \varepsilon_T]'$ \\
$y$&$mT\times 1$& векторизация $Y$&$y=\vec(Y)$\\
$\varepsilon$&$mT\times 1$ &векторизация $E$&$\varepsilon=\vec(E)$\\
\midrule
$\Phi_1$, \ldots & $m \times m$ & коэффициенты VAR & $y_t= \Phi_1 y_{t-1} + \ldots + \Phi_{ex} z_t +\varepsilon_t$ \\
$\Phi_{ex}$ & $m \times d$ & коэффициенты при экзогенных переменных & \\
$\Phi$ & $k \times m$ & упаковка матриц $\Phi_1$, \ldots & $\Phi=[ \Phi_1 \ldots \Phi_p \; \Phi_{ex}]'$ \\
$\phi$ & $km \times 1$ & вектор из матрицы $\Phi$ & $\phi = \vec{\Phi}$ \\
\midrule % здесь к независимому без кронекерова произведения
$\prior \Xi $ & $km \times km$  & Априорная ковариационная матрица $\Phi$& \\
$\prior \Phi$ & $k \times m$ & априорное математическое ожидание $\Phi$ & \\
$\prior \phi$ & $km \times 1$ & вектор из матрицы $\prior \Phi$  & $\prior \phi = \vec{ \prior \Phi}$ \\
$\post \Xi $ & $km \times km$  & Апостериорная ковариационная матрица $\Phi$& \\
$\post \Phi$ & $k \times m$ & апостериорное математическое ожидание $\Phi$ & \\
$\post \phi$ & $km \times 1$ & вектор из матрицы $\post \Phi$  & $\post \phi = \vec{ \post \Phi}$ \\
$\prior \nu $ & скаляр & & \\
$\post \nu $ & скаляр & & $\post \nu = T + \prior \nu$ \\
\midrule % здесь к сопряженному с кронекеровым произведением
$\prior \Omega$ & $k \times k$ & Матрица априорных масштабирующих коэффициентов ковариационной матрицы $\Phi$& $\prior \Xi = \Sigma \otimes \prior \Omega$ \\
$\post \Omega$ & $k \times k$ & Матрица апостериорных масштабирующих коэффициентов ковариационной матрицы $\Phi$&  $\post \Omega = (\prior\Omega^{-1}+ X'X)^{-1}$,  $\post \Xi = \Sigma \otimes \post \Omega$\\
$\Sigma$ & $m \times m$ &Ковариационная матрица ошибок& $\E\varepsilon_t \varepsilon _t'=\Sigma$\\
\bottomrule
\end{tabular}
\end{center}




\subsection{Байесовская VAR}
Рассмотрим переменные $y_{it}$, объединенные в~вектор  $y_{t}=(y_{1t},y_{2t},\ldots, y_{mt})'$ размерности $m$. Векторная авторегрессия в сокращенной форме записывается в~виде:
\begin{equation}
y_t =\Phi_{ex}+ \Phi_1 y_{t-1} + \Phi_2 y_{t-2} +\ldots + \Phi_p y_{t-p} + \varepsilon_t,\quad \varepsilon_t\sim \cN(0,\Sigma)
\end{equation}
где $\Phi_{ex}=(c_1,\ldots ,c_m)'$ --- вектор констант размерности $m$, $\Phi_l$ --– авторегрессионные матрицы размерности $m\times m$ при $l=1, \ldots, p$. Вектор $\varepsilon_t$ --- $m$-мерный вектор ошибок с ковариационной матрицей $\E\varepsilon_t \varepsilon _t'=\Sigma$, некоррелированный с объясняющими переменными.
Группируя матрицы параметров в общую матрицу $\Phi=[\Phi_1 \ldots \Phi_p \; \Phi_{ex}]'$ % здесь нужно транспонирование!
и определяя новый вектор $x_t=[ y'_{t-1} \ldots  y'_{t-p} \; 1]'$, получаем VAR записанную в более компактном виде:

\begin{equation}
y_t=\Phi' x_t+\varepsilon_t
\end{equation}
Если же сгруппировать переменные  и шоки следующим образом: $Y=[y_1, y_2,\ldots, y_T]'$, $X=[x_1, x_2,\ldots, x_T]'$, $E=[\varepsilon_1, \varepsilon_2,\ldots, \varepsilon_T]'$ то VAR можно записать как:
\begin{equation}
Y=X\Phi+E\label{var}
\end{equation}

Эта же модель может быть записано в векторизованном виде\footnote{Третье уравнение системы следует из тождества: $\vec(ABC)=(C\otimes A)\vec (B)$}:
\begin{gather}
Y=X\Phi+E\\
\vec(Y)=\vec(X\Phi I)+\vec(E)\Leftrightarrow \\
%\vec(Y)=(I_m\otimes X)\vec(Phi)+\vec(E
%\intertext{либо}
y=(I_M\otimes X)\phi + \varepsilon \label{var2}
\end{gather}
где $\varepsilon  \sim \cN(0,\Sigma\ \otimes\ I_T)$ и вектор $\phi=\vec{\Phi}$ имеет размерность $km\times 1$.

Задача байесовского оценивания заключается в~поиске апостериорных распределений параметров $p(\Phi, \Sigma|Y)$ с~использованием функции максимального правдоподобия, $L(Y|\Phi, \Sigma)$, и~заданного априорного распределения, $p(\Phi, \Sigma|Y)$. Для этого используется правило Байеса:
\begin{equation}
p(\Phi, \Sigma|Y)=\frac{p(\Phi,\Sigma) L(Y|\Phi,\Sigma)}{p(Y)}
\end{equation}
Т.к. $p(Y)$ не зависит от $\Phi$ и $\Sigma$, то можно записать: 
\begin{equation}
p(\Phi, \Sigma|Y)\propto p(\Phi,\Sigma) L(Y|\Phi,\Sigma)
\end{equation}\\
Так как $\varepsilon_t\sim \cN(0,\Sigma)$, то функция правдоподобия задается как:\footnote{ Другая форма записи функции правдоподобия: $L(Y|\Phi, \Sigma) \propto |\Sigma|^{-T/2}\etr\left\lbrace -\frac{1}{2}  \left[\Sigma^{-1} \hat E' \hat E\right]\right\rbrace \times \etr\left\lbrace -\frac{1}{2}  \left[\Sigma^{-1}(\Phi-\hat\Phi)'X'X(\Phi-\hat\Phi)\right]\right\rbrace$, где $\hat E=Y-X\hat\Phi$ и $\hat \Phi=(X'X)^{-1}X'Y$. Здесь $\etr()=\exp(\tr())$. }
\begin{equation}
L(Y|\Phi, \Sigma) \propto |\Sigma|^{-T/2}\etr\left\lbrace -\frac{1}{2}  \left[\Sigma^{-1}(Y-X\Phi)'(Y-X\Phi)\right]\right\rbrace\label{likelihood}
\end{equation}

В двух следующих разделах будут разобраны наиболее известные априорные и построенные на их основе апостериорные распределения.  


\subsection{Классификация популярных априорных распределений}



Детально каждое из априорных распределений будет описано отдельно, а в
этом разделе будет дана общая схема связи априорных распределений.
Среди самых популярных априорных распределений можно назвать:

\begin{enumerate}

\item Независимое нормальное-обратное Уишарта распределение


\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma \sim \cIW(\prior S; \prior\nu) \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}


В общем случае выборку из  апостериорного распределения можно получить
по схеме Гиббса.


Частными случаями независимого нормального-обратного Уишарта являются:


\begin{enumerate}

\item Распределение Миннесоты


\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma = const \\
% \phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}


Получается из независимого нормального-обратного Уишарта при $\prior
S=(\prior \nu-m-1) \cdot \Sigma$ и $\prior \nu \to \infty$.
Апостериорное распределение выписывается в явной форме. Можно
использовать алгоритм Монте-Карло, сразу генерирующий случайную
выборку из апостериорного распределения, без необходимости периода
«прожига».


Более того, алгоритм симуляции упрощается если матрица $\prior \Xi$
имеет структуру кронекерова произведения $\prior \Xi =  \Sigma \otimes
\prior \Omega $. В этом случае распределение Миннесоты становится
частным случаем сопряжённого нормального-обратного Уишарта.



\item Независимое нормальное-Джеффри


\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma \sim |\Sigma|^{-(m+1)/2} \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}




Получается из независимого нормального-обратного Уишарта при $\prior S
= \prior \nu^{1/m}\cdot I$ и  $\prior \nu \to 0$.


как тут с плотностью растущей к необратимым матрицам ???????????

\url{http://www.tc.umn.edu/~nydic001/docs/unpubs/Wishart_Distribution.pdf}
--- тут пишут, что при $\nu <...$ получается необратимая матрица, а
тут вроде как обратимая????



Для получения выборки из~апостериорного распределения можно
использовать схему Гиббса. Необходимые формулы для гиперпараметров
апостериорного распределения получаются из общего случая просто
подстановкой $\prior S=0$, $\prior \nu =0$.



Распределение Миннесоты и независимое нормальное-Джеффри являются
противоположными крайностями независимого-обратного Уишарта. В
распределении Миннесоты матрица $\Sigma$ предполагается известной, а в
нормальном-Джеффри матрица $\Sigma$ имеет «размытое» неинформативное
распределение.



Частным случаем независимого нормального-Джеффри распределения является:


\begin{enumerate}

\item Неинформативное-Джеффри

\begin{equation}
\begin{cases}
\phi \sim 1 \\
\Sigma \sim |\Sigma|^{-(m+1)/2} \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}

\end{enumerate}


Априорное распределение для ленивых. Не нужно указывать ни одного
гиперпараметра!


Получается из независимого нормального-Джеффри при $\prior\phi=0$ и
$\prior\Xi= a\cdot I$ и $a\to \infty$. Является также частным случаем
сопряженного нормального-Джеффри априорного распределения.


Для получения выборки из~апостериорного распределения можно
использовать прямое симулирование по схеме Монте-Карло без алгоритма
Гиббса. % Необходимые формулы для гиперпараметров апостериорного
распределения получаются из общего случая просто подстановкой $\prior
S=0$, $\prior \nu =0$, $\prior \Xi^{-1}=0$, $\prior\phi=0$. При этом
формулы существенно упрощаются, в частности исчезает необходимость
обращать матрицу размера $km\times km$.


\end{enumerate}

\item Сопряженное нормальное-обратное Уишарта распределение


\begin{equation}
\begin{cases}
\Sigma\sim \cIW(\prior S, \prior \nu) \\
\phi|\Sigma\sim \cN (\prior \phi, \Sigma\otimes\prior \Omega)
\end{cases}
\end{equation}


Для сопряженного нормального-обратного Уишарта распределения нет
необходимости использовать алгоритм Гиббса, так как есть явные формулы
для апостериорных распределений. Можно использовать алгоритм
Монте-Карло, сразу генерирующий случайную выборку из апостериорного
распределения. Исчезает период «прожига», необходимый для сходимости
алгоритма Гиббса.


Частным случаем сопряжённого нормального-обратного Уишарта
распределения оказывается:


\begin{enumerate}

\item Распределение Миннесоты при  $\prior \Xi =  \Sigma \otimes
\prior \Omega $.



При $\prior \Xi =  \Sigma \otimes \prior \Omega $ распределение
Миннесоты является и частным случаем независимого
нормального-обратного Уишарта, и сопряженного нормального-обратного
Уишарта. Для получения выборки из апостериорного распределения можно
использовать и схему Гиббса для независимого нормального-обратного
Уишарта, и алгоритм Монте-Карло без «прожига».



\item Сопряжённое нормальное-Джеффри


\begin{equation}
\begin{cases}
\Sigma \sim |\Sigma|^{-(m+1)/2} \\
\phi | \Sigma \sim \cN(\prior\phi; \Sigma \otimes \prior\Omega ) \\
\end{cases}
\end{equation}




Получается из сопряжённого нормального-обратного Уишарта при $\prior S
= \prior \nu^{1/m}\cdot I$ и  $\prior \nu \to 0$.  Формулы для
гиперпараметров апостериорного распределения получаются подстановкой
$\prior\nu=0$, $\prior S=0$.


Частным случаем сопряжённого нормального-Джеффри является

\begin{enumerate}

\item Неинформативное-Джеффри


Получается из сопряженного нормального-Джеффри при $\prior\phi=0$ и
$\prior\Omega= a\cdot I$ и $a\to \infty$. Формулы для гиперпараметров
апостериорного распределения получаются подстановкой $\prior\phi=0$,
$\prior\Omega^{-1}=0$, $\prior\nu=0$, $\prior S=0$.

\end{enumerate}



\end{enumerate}


\end{enumerate}



????? Где в этой схеме \st{Березовский} априорное распределение Sim-Zha?



В нашей работе мы всегда явно указываем, идёт ли речь о независимом
или сопряжённом априорном распределении. Однако при чтении многих
других работ надо быть внимательным, зачастую авторы говорят о
«нормальном-обратном Уишарта» распределении, не уточняя, какое имеется
ввиду.

\subsection{Априорное распределение Миннесоты}


Одно из решений этой проблемы было предложено в работе \cite{litterman_1979}, автор которой показал, что введение ограничений в форме априорных распределений параметров увеличивает точность оценок и прогнозов. Априорное распределение, позже(?) получившее название «априорное распределение Миннесоты>> было предложено в работе  \cite{litterman_1986} и (с некоторыми модификациями) в \cite{doan_litterman_sims_1984}.

%Хотя априорное распределение Миннесоты и является частным случаем независимого нормального-обратного Уишарта распределения, имеет смысл рассмотреть его отдельно. Оно было предложено в работе  \cite{litterman_1986_forecasting} и \cite{doan_litterman_sims_1984_forecasting}.

%Изначально \cite{litterman_1986_forecasting} предложил использовать априорное распределение Миннесоты, при котором в явном виде учитывается предпосылка о нестационарности большинства макрорядов.

Априорное распределение параметров предполагается многомерным нормальным, зависящим от нескольких гиперпараметров. Параметры предполагаются независимыми, следовательно, их ковариационная матрица диагональна. Ковариационная матрица вектора $\varepsilon_t$ также предполагается диагональной и постоянной.
Тогда вектор $\Phi$ не зависит от $\Sigma$:
\begin{equation}
\phi\sim \cN(\prior \phi, \prior \Xi)
\end{equation}

Априорная плотность распределения $\phi$ также не зависит от $\Sigma$ и может быть записана как:
\begin{equation}
p(\phi)=\frac{1}{(2\pi)^{km/2}|\prior\Xi|^{1/2}} \exp \left\lbrace-\frac{1}{2}(\phi-\prior \phi)'\prior\Xi^{-1}(\phi-\prior \phi ) \right\rbrace.
\end{equation}

Комбинируя её с функцией правдоподобия \eqref{likelihood}, получаем, что апостериорное распределение параметров задаются в следующем виде:
\begin{equation}
\phi|Y\sim \cN(\post{\phi},\post \Xi)
\end{equation}

где
\begin{align*}
&\post \Xi=[\prior \Xi^{-1}+\Sigma^{-1}\otimes(X'X)]^{-1}\\
& \post \phi=\post{\Xi}[\prior \Xi^{-1}\prior \phi+(\Sigma^{-1}\otimes X')y].
\end{align*}

Если $\prior\Xi$ имеет структуру кронекерова произведения, $\prior\Xi = \Sigma \otimes \prior{\Omega}$, то формулы можно существенно упростить и обойтись обращением матриц меньшей размерности:

\begin{multline}
\post \Xi=[\prior \Xi^{-1}+\Sigma^{-1}\otimes(X'X)]^{-1}=[(\Sigma \otimes \prior\Omega)^{-1}+\Sigma^{-1}\otimes(X'X)]^{-1}=\\
=[\Sigma^{-1}\otimes  \prior\Omega^{-1} +\Sigma^{-1}\otimes(X'X)]^{-1}=\Sigma \otimes (\prior\Omega^{-1}+ X'X)^{-1} = \Sigma \otimes \post \Omega
\end{multline}

В результате получаем 

\[
\Phi | Y \sim \cN (\post\Phi, \Sigma \otimes \post\Omega)
\]

На практике в качестве матрицы $\Sigma$ используют её оценку $\hat\Sigma$, диагональные элементы которой равны: $\hat\sigma_{1}^2, \hat\sigma_{2}^2,\ldots,\hat\sigma_{m}^2 $, где  $\hat\sigma_{i}^2$ --- оценка дисперсии случайной составляющей в $AR(p)$ модели для ряда $i$. \footnote{Некоторые авторы для подсчета оценки дисперсии используют AR(1) модель, даже если сама VAR имеет большее количество лагов.}

Математическое ожидание априорного распределения параметров может быть записано с помощью матрицы $\prior \Phi = \E (\Phi)$ размерности $k\times m$, где $\prior\Phi=[\prior\Phi_1 \ldots \prior\Phi_p \; \prior\Phi_{ex}]'$ и $\prior \phi = \vec{ \prior \Phi}$.


\begin{equation}
(\prior\Phi_l)_{ij}=
\begin{cases}
\delta_i\; i=j, l=1;\\
0,\;\text{ в остальных случаях }
\end{cases}\label{minnesota_exp}
\end{equation}

%%% здесь про дельта i и кто их как берет

Априорное распределение Миннесоты было задумано таким образом, чтобы учесть нестационарность многих макроэкономических временных рядов. В этом случае $\delta_{i}$, принимают значение единица.\footnote{В настоящее время широкое распространение получила практика назначать $\delta_{i}=1$ для нестационарных рядов и $\delta_{i}<1$ для стационарных.}

Априорное распределение Миннесоты предполагает, что априорная ковариационная матрица параметров $\prior \Xi$ диагональна. Пусть $\prior \Xi_i$ обозначает блок $\prior \Xi$, размера $k\times k$, связанный с коэффициентами уравнения $i$, т.е.  

\begin{equation*}
\prior \Xi=\begin{pmatrix}
\prior \Xi_1&0_{k\times k}&\cdots&0_{k\times k}&0_{k\times k}\\
0_{k\times k}& \prior \Xi_2& \cdots &0_{k\times k}&0_{k\times k}\\
\vdots &\vdots& \ddots&\vdots& \vdots\\
0_{k\times k}&0_{k\times k}&\cdots&\prior\Xi_{m-1} & 0_{k\times k}\\
0_{k\times k}&0_{k\times k}&\cdots&0_{k\times k}&\prior \Xi_m
\end{pmatrix}
\quad
\prior \Xi_i=\begin{pmatrix}
\prior \Xi_{i,lag=1}&0_{m\times m}&\cdots&0_{m\times m}&0_{m\times 1}\\
0_{m\times m}& \prior \Xi_{i,lag=2}& \cdots &0_{m\times m}&0_{m\times 1}\\
\vdots &\vdots& \ddots& \vdots&\vdots\\
0_{m\times m}&0_{m\times m}&\cdots&\prior \Xi_{i,lag=p}&0_{m\times 1}\\
0_{1\times m}&0_{1\times m}&\cdots&0_{1\times m}&\prior \Xi_{i,const}
\end{pmatrix}
\end{equation*}
Тогда  диагональные элементы  $\prior \Xi_{i,lag=l}$ определяются по формулам:
\begin{equation}
(\prior \Xi_{i,lag=l})_{jj}
=\begin{cases}
\left(\frac{\lambda_{tight}}{l^{\lambda_{lag}}}\right)^2,\ j=i\\
\left(\frac{\lambda_{tight}\cdot\lambda_{kron}\sigma_i}{l^{\lambda_{lag}}\sigma_j}\right)^2, \; j\ne i
\end{cases}\\
\prior \Xi_{i,const}
=\lambda_{const}^2\sigma_i^2, 
\label{minnesota_variance}
\end{equation}

%\begin{itemize}
%\item $\frac{\lambda_1}{l^2}$ --- для переменной $i$ и лага $l$
%\item $\frac{\lambda_1\lambda_2\sigma^2_i}{l^2\sigma^2_j}$ --- для переменной $j\neq i$ и лага $l$
%\item $\lambda_3\sigma_i^2 \label{minnesota_variance}$ --- для коэффициентов при экзогенных переменных
%\end{itemize}

Как можно видеть из приведенной выше формулы \eqref{minnesota_variance} априорная дисперсия параметров зависит от нескольких гиперпараметров, задаваемых исследователем. Гиперпараметры имеют следующую интерпретацию: $\lambda_{tight}$ (параметр регуляризации) отражает общую <<жесткость>> априорного распределения. Если $\lambda_{tight}\to 0$, то априорное распределение полностью определяет апостериорное распределение, и данные не играют никакой роли при оценке параметров. Наоборот, если $\lambda_{tight}\to \infty$, то априорное распределение перестает влиять и оценка параметров сходится к обычной оценке МНК. Параметр $\lambda_{kron}$ (параметр кросс-регуляризации) добавляет дополнительную жесткость лагам других переменных по сравнению с лагами зависимой переменной. Если  $\lambda_{kron}<1$, то собственные лаги зависимой переменной помогают предсказывать значение переменной лучше, чем лаги других переменных, поэтому коэффициенты при лагах других переменных оказываются жестче регуляризованы к нулю.   Параметр $\lambda_{const}$ отражает относительную жесткость распределения константы.

При $\lambda_{kron}=1$ матрица $\prior \Xi$ имеет структуру кронекерова произведения и представима в виде:
\[
\prior \Xi = \Sigma \otimes \prior \Omega,
\]
где $\prior \Omega$ --- матрица размера $k\times k$, соответствующая отдельному уравнению. Кронекерово домножение слева на матрицу $\Sigma$ для $i$-го уравнения означает домножение дисперсий, указанных в матрице $\prior \Omega$, на коэффициент $\sigma^2_i$. Сама  $\prior \Omega$ представима в виде: 


\begin{equation}
\prior \Omega=\begin{pmatrix} \label{prior_omega1}
\prior \Omega_{lag=1}&0_{m\times m}&\cdots&0_{m\times m}&0_{m\times 1}\\
0_{m\times m}& \prior\Omega_{lag=2}& \cdots &0_{m\times m}&0_{m\times 1}\\
\vdots &\vdots& \ddots&\vdots& \vdots\\
0_{m\times m}&0_{m\times m}&\cdots&\prior\Omega_{lag=p} & 0_{m\times 1}\\
0_{1\times m}&0_{1\times m}&\cdots&0_{1\times m}&\prior \Omega_{const}
\end{pmatrix} 
\end{equation}

При этом матрица $\prior \Omega_{lag=l}$ имеет размерность $m\times m$, и ее диагональные элементы определяются по формулам:
\begin{equation}
(\prior \Omega_{lag=l})_{jj} \label{prior_omega2}
=\left(\frac{\lambda_{tight}}{l^{\lambda_{lag}}\sigma_j}\right)^2
\quad
\prior \Omega_{const}=\lambda_{const}^2 
\end{equation} 

При использовании априорного распределения Миннесоты, нет необходимости применять алгоритм Гиббса для получения апостериорного распределения. Алгоритм генерации случайной выборки непосредственно из апостериорного распределения происходит методом Монте-Карло:

\begin{enumerate}
\item На $s$-ом шаге сгенерировать очередную итерацию согласно:
\begin{equation}
\phi^{[s]}\sim \cN(\post \phi; \post \Xi)
\end{equation}
\item Увеличить $s$ на единицу и перейти к пункту один
\end{enumerate}

Если $\prior \Xi$ имеет структуру кронекерова произведения, $\prior \Xi = \Sigma \otimes \prior \Omega$, то вместо вектора $\phi^{[s]}$ можно генерировать матрицу $\Phi^{[s]}$ численно более простым алгоритмом:


\begin{enumerate}
\item Генерируют матрицу $V$ размера $k\times m$  из независимых стандартных нормальных величин
\item Считают матрицу $\Phi^{[s]}$ по формуле:
\[
\Phi^{[s]} = \post \Phi + \chol(\post\Omega) \cdot V \cdot \chol(\Sigma^{[s]})'
\]
\end{enumerate}

Преимущества априорного распределения Миннесоты хорошо известны. Прежде всего, оно просто задается, Кроме того, оно успешно применялось в литературе для решения различных задач. И наконец, получившееся апостериорное распределение является нормальным, а значит, легко можно получить значение любой функции параметров с помощью методов Монте-Карло. Однако существенным недостатком этого распределения является то, что оно не предполагает использования байесовской процедуры для оценки $\Sigma$.




\subsection{Сопряженное нормальное-обратное Уишарта априорное распределение}

Указанного недостатка априорного распределения Миннесоты можно избежать, если рассматривать сопряженное априорное распределение, т.е. распределение, при котором  априорное распределение, функция правдоподобия и апостериорное распределение принадлежат одному классу. Т.к. функция правдоподобия может быть разбита на две части, одна из которых соответствует нормальному распределению (при условии известной ковариационной матрицы остатков), а другая --- обратному распределению Уишарта, то и~сопряженным априорным распределением для рассматриваемой модели будет также нормальное-обратное Уишарта распределение.


Априорное нормальное-обратное Уишарта распределение может быть записано как:
\begin{equation}
\begin{cases} \label{conjugate_prior}
\Sigma\sim \cIW(\prior S, \prior \nu) \\
\phi|\Sigma\sim \cN (\prior \phi, \Sigma\otimes\prior \Omega)
\end{cases} 
\end{equation}

Здесь нужно отметить, что система \eqref{conjugate_prior} записана для случая, когда ковариационная матрица параметров имеет кронекерову структуру, что означает, что $\lambda_{kron}$ полагается равной единице. Хотя строго, говоря, это предположение не обязательно, оно существенно ускоряет расчеты, поэтому, как правило, используется в макроэкономических  приложениях. 

Гиперпараметры вектора математического ожидания $(\prior \phi)$ и ковариационной матрицы $\prior \Omega$ условного априорного распределения  могут быть заданы точно так же, как и в случае распределения Миннесоты для  случая $\lambda_{kron} =1$ (см. \eqref{minnesota_exp},\eqref{prior_omega1} и\eqref{prior_omega2}). 
$\prior S$ выбирается так, чтобы среднее $\Sigma$ совпадало с фиксированной ковариационной матрицей в априорном распределении Миннесоты. Т.к. безусловное распределение параметров имеет вид:
\begin{equation}
\E(\phi)=\prior \phi \quad \Var(\phi)=(\prior \nu - m -1)^{-1}(\prior S\otimes \prior \Omega),
\end{equation}
то диагональные элементы $\prior S$ выбираются следующим образом:
\begin{equation}
(\prior S)_{ii}= (\prior \nu- m- 1) \hat\sigma^2_{i}
\end{equation}
Выбор степеней свободы обратного Уишарта распределения $\prior\nu$ в соответствии с: 
\begin{equation}
\prior \nu \geq\max\lbrace m+2, m+2h-T\rbrace
\end{equation}
обеспечивает существование как априорной дисперсии параметров, так и апостериорной дисперсии прогнозов на горизонте $h$ (см. \cite{kadiyala_karlsson_1997}).

Можно показать, что с учетом функции правдоподобия \eqref{likelihood} апостериорное распределение принадлежит тому же классу (см, например, \cite{zellner_1996}):
\begin{equation}
\begin{cases}
\Sigma|Y \sim \cIW(\post S, \post \nu) \\
\Phi|\Sigma,Y\sim \cN (\post \Phi, \Sigma\otimes\post \Omega) 
\end{cases}
\end{equation}

где
\begin{align*}
&\post\nu =\prior \nu+T\\
&\post{\Omega}=(\prior \Omega^{-1}+X'X)^{-1}\\
&\post \Phi=\post{\Omega}\cdot (\prior \Omega^{-1}\prior \Phi+X'Y)\\
&\post S=\prior S +\hat E'\hat E+\hat \Phi'
 X'X \hat \Phi +\prior \Phi'\prior\Omega^{-1}\prior \Phi-\post \Phi'\post\Omega^{-1}\post \Phi\\
&\hat\Phi=(X'X)^{-1}X'Y\\%\quad\text{и }
&\hat E=Y-X\hat\Phi
\end{align*}


Существует достаточно популярный альтернативный подход для подсчёта гиперпараметров апостериорного распределения.


Мы обнуляем матрицы  $\prior S$ и $\prior \Omega^{-1}$, а чтобы компенсировать разницу добавляем дополнительные наблюдения в матрицу $X$ и в матрицу $Y$: 

\[
X^*=\begin{bmatrix}
X^{+} \\
X 
\end{bmatrix}, \;
Y^*=\begin{bmatrix}
Y^{+} \\
Y 
\end{bmatrix}
\]

Отметим, что матрицы $X$ и $Y$ входят в гиперпараметры апостериорного распределения только в составе матриц $X'X$, $X'Y$ и $Y'Y$, поэтому абсолютно не важно, в каком порядке добавлять искусственные наблюдения и каким образом по отношению к матрицам $X$ и $Y$. Можно их добавить в конец матриц $X$ и $Y$, можно в начало, можно посередине.

Получим новые формулы для апостериорных гиперпараметров:
ВЫВЕРИТЬ!

\begin{align*}
&\post\nu =\prior \nu+T\\
&\post{\Omega}=(X^{*\prime}X^*)^{-1}=(X^{+\prime}X^+ + X'X)^{-1}\\
&\post \Phi=\post{\Omega}\cdot (X^{*\prime}Y^*)=\post{\Omega}\cdot (X^{+\prime}Y^++X'Y)\\
&\post S=\hat E'\hat E+\hat \Phi X'X \hat \Phi -\post \Phi'\post\Omega^{-1}\post \Phi\\
&\hat\Phi=(X'X)^{-1}X'Y\\%\quad\text{и }
&\hat E^*=Y^*-X^*\hat\Phi
\end{align*}

%\begin{align*}
%&\post\nu = \prior \nu  + T \\
%&\post S= E^{*\prime}E^*, \text{ где } E^*=Y^*-X^*\Phi \\
%&\post \Xi = ( \Sigma^{-1}\otimes X^{*\prime}X^*)^{-1} \\
%&\post \phi = \post \Xi \cdot ( \vec(X^{*\prime}Y^*\Sigma^{-1}))
%\end{align*}

Другими словами, наблюдения добавляются так, что гиперпараметры апостериорных наблюдений не изменяются. Заметим, что $X^{*\prime}X^*=X^{+\prime}X^+ + X'X$, $E^{*\prime}E^*=E^{+\prime}E^+ + E'E$.

Чтобы новые формулы совпадали со старыми необходимо, чтобы

\begin{equation}
\begin{cases}
X^{+\prime}X^+=\prior \Omega^{-1} \\
X^{+\prime}Y^+=\prior \Omega^{-1}\prior \Phi \\
...
\end{cases}
\end{equation}







Заметим, что новые формулы позволяют трактовать:
\begin{itemize}
\item $\post \Phi$ --- как результат построения регрессий $Y^*$ на $X^*$:
\[
\post \Phi=(X^{*\prime}X^*)^{-1}\cdot (X^{*\prime}Y^*)
\]
\item $\post S$ --- как ...
\end{itemize}




Эти условия будут выполнены если добавить наблюдения по схеме:

По аналогии с  работами  \cite{banbura_al_2010}, \cite{berg_henzel_2013} соответствующее априорное распределение вводится путем добавления искусственных наблюдений\footnote{Формулы, приведенные в самих работах \cite{banbura_al_2010}, \cite{berg_henzel_2013} и отражающие введение новых наблюдений, являются частным случаем \eqref{NIW_observations} для $\lambda_{lag}=1$ и $\lambda_{const}\to\infty$.}:
\begin{equation}
Y^{NIW}=\begin{bmatrix}
\frac{\diag(\delta_1\sigma_1,\ldots, \delta_m\sigma_m)}{\lambda_{tight}}\\
0_{m(p-1)\times m}\\
\diag(\sigma_1,\ldots,\sigma_m)\\
0_{1\times m}
\end{bmatrix}
\quad
%
X^{NIW}=\begin{bmatrix}
\frac{\diag(1,2^{\lambda_{lag}},\ldots, p^{\lambda_{lag}})\otimes \diag(\sigma_1,\ldots,\sigma_m)}{\lambda_{tight}} & 0_{mp\times 1} \\
0_{m\times mp}&0_{m\times 1}\\
0_{1\times mp}&\frac{1}{\lambda_{const}}
\end{bmatrix}\label{NIW_observations}
\end{equation}



%При этом математические ожидания и дисперсии априорного распределения параметров могут быть заданы по тому же принципу, что и в априорном распределении Миннесоты (см. \eqref{minnesota_exp}-\eqref{minnesota_variance}).
%При этом априорные распределения параметров, указанные в \eqref{minnesota_exp}-\eqref{minnesota_variance}), предполагают наличие  нулевой априорной корреляции между коэффициентами.


В работах \cite{doan_al_1984} and \cite{sims_1993} было предложено добавить к этим априорным распределениям дополнительную характеристику, введение которой обуславливается возможным наличием во временных рядах единичных корней и коинтеграционных  соотношений. Это позволяет исключить появление неправдоподобно большой доли внутривыборочной дисперсии, объясняемой экзогенными переменными \cite{carriero_al_2015}. %%% ???



\subsubsection*{Модификации априорного распределения}

Априорное распределение суммы коэффициентов\footnote{sum-of-coefficints prior} было предложено в работе  \cite{doan_al_1984}. Это распределение отражает следующую идею: если переменные в VAR имеют единичный корень, то можно учесть эту информацию, задав априорное распределении, в котором сумма коэффициентов при всех лагах зависимой переменной равна единице (см. \cite{robertson_tallman_1999} и \cite{blake_mumtaz_2012} Другими словами, когда среднее значение лагированных значений какой либо переменной находится на некотором уровне , то это же самое значение, является хорошим прогнозом  для будущих значений этой переменной. 



Внедрение этого априорного распределения производится путем добавления искусственных дамми-наблюдений по следующей схеме:
%\begin{gather*}
%y_d(i,j)=\begin{cases}
%\post y_{0i}/\lambda_3, \quad \text{если }i=j\\
%0 \quad \text{в обратном случае}
%\end{cases}
%x_d(i,s)=\begin{cases}
%\post y_{0i}/\lambda_3,\quad \text{если }i=j, s<km\\
%0 \quad \text{в обратном случае,}
%\end{cases}
%\end{gather*}
%где $i,j=1,\ldots,m$, $s=1, \ldots, km$. Когда $\lambda_3\to 0$, модель стремится к виду, предполагающему запись в разностях, т.е. единичных корней становится столько же, сколько переменных, и нет коинтеграции.


\begin{gather}
Y^{SC}=\frac{1}{\lambda_{sc}}\begin{bmatrix}\diag(\delta_1\mu_1,\ldots,\delta_m\mu_m)\end{bmatrix}\\
X^{SC}=\frac{1}{\lambda_{sc}}\begin{bmatrix}(1_{1\times p})\otimes \diag(\delta_1\mu_1,\ldots,\delta_m\mu_m) &0_{m\times 1}\end{bmatrix},
\end{gather}

где $(1_{1\times p})$ - вектор-строка из единиц длиной $p$, $\mu_i$ есть $i$-ая компонента вектора $\mu$, который состоит из средних начальных значений всех переменных\footnote{Некоторые авторы для  расчета $mu$ рассчитывают среднее по всем наблюдениям, т. е. $\mu=\frac{1}{T}\sum_{t=1}^T y_t$ (см. \cite{banbura_al_2010} и \cite{carriero_al_2015}). Однако, в соответствии с работой \cite{sims_zha_1998} для расчета среднего используются только первые $p$ наблюдений.}: $\mu=\frac{1}{p}\sum_{t=1}^p y_t$  

Априорное распределение начального наблюдения\footnote{dummy initial observation prior }, предложенное в работе \cite{sims_1993} отражает априорную веру в то, что переменные имеют общий стохастический тренд. Для этого вводится единственное дамми-наблюдение такое, что все значения всех переменных равны соответствующему среднему начальных значений $\mu_i$ с точностью до коэффициента масштаба: $\lambda_{io}$. Это происходит  путем добавления в систему дамми-наблюдения следующего вида:
\begin{gather}
Y^{IO}=\frac{1}{\lambda_{io}}\begin{bmatrix}
\delta_1\mu_1,\ldots,\delta_m\mu_m
\end{bmatrix}\\
X^{IO}=\frac{1}{\lambda_{io}}\begin{bmatrix}
(1_{1\times p})\otimes (\delta_1\mu_1,\ldots,\delta_m\mu_m) &1
\end{bmatrix},
\end{gather}

(?) Это априорное распределение приводит к тому, что среднее по каждой переменной есть линейная комбинация всех остальных средних.
%%% Непонятно, нужны ли здесть дельты. У Карьеро их нет, но у него и в sum-of-coefficients их нет, а у Банбуры в sum-of-coefficients есть, f initiфд ibservation они не вводят



%\begin{gather*}
%y_d(j)=\begin{cases}\post y_{0i}/\lambda_4 \qquad
%\post y_{0i}/\lambda_4\quad \text{если }s<km\\
%1/\lambda_4 \quad \text{в обратном случае,}
%\end{cases}
%\end{gather*}
%где $j=1,\ldots m, s=1,\ldots km$. 

Гиперпараметр $\lambda_{io}$ отражает жесткость указанного априорного распределения. Когда $\lambda_{io}\to 0 $,  модель принимает вид, в котором либо все переменные стационарны со средним, равным выборочному среднему начальных условий, либо нестационарны (без дрейфа) и коинтегрированны. 


Как и в случае распределения Миннесоты, необходимости использовать алгоритм Гиббса нет, можно генерировать случайную выборку непосредственно из апостериорного распределения. Например, можно применять такой алгоритм:


\begin{enumerate}
\item На $s$-ом шаге сгенерировать очередную итерацию согласно:
\begin{align*}
&\Sigma^{[s]} \sim \cIW(\post S, \post \nu) \\
&\phi^{[s]}\sim \cN(\post \phi; \Sigma^{[s]} \otimes \post \Omega)
\end{align*}
\item Увеличить $j$ на единицу и перейти к пункту один
\end{enumerate}

На практике вместо генерирования вектора $\phi^{[s]}$ генерируют сразу матрицу $\Phi^{[s]}$ в два шага:

\begin{enumerate}
\item Генерируют матрицу $V$ размера $k\times m$  из независимых стандартных нормальных величин
\item Считают матрицу $\Phi^{[s]}$ по формуле:
\[
\Phi^{[s]} = \post \Phi + \chol(\post\Omega) \cdot V \cdot \chol(\Sigma^{[s]})'
\]
\end{enumerate}

Два ключевых недостатка априорного распределения Миннесоты (отсутствие байесовского оценивания $\Sigma$ обязательная апостериорная независимость отдельных уравнений системы) решаются при использовании сопряженного нормального  - обратного Уишарта распределения. Однако в этом случае параметры априорного распределения для разных уравнений выбираются симметрично. В частности, все коэффициенты при первом лаге зависимой переменной априорно имеют одну и ту же дисперсию $\lambda_{tight}^2$  Хотя обычно эта предпосылка не является слишком ограничивающей, в реальности легко встретиться с задачами, в которых ковариационная матрица априорного распределения не должна быть симметрично сформирована для разных уравнений. Например, довольно известным в литературе является следующий пример (см, \cite{kadiyala_karlsson_1997}). Положим, исследователь хочет  учесть в VAR наличие нейтральности денег. При построении модели эта предпосылка может быть учтена наложением такого априорного распределения, в котором все коэффициенты при лагах денег в уравнении для выпуска имеют нулевое матожидание и низкую дисперсию. Однако это означает, что и в других уравнениях дисперсия коэффициентов в априорном распределении будет относительно низкой. Это характеристика может быть нежелательной, и, чтобы этого избежать,   априорное распределение можно задать как независимое - обратное Уишарта. 

\subsection{Независимое нормальное-обратное Уишарта распределение}

Этот тип распределений предполагает, что ковариационная матрица параметров может быть произвольной формы: 

\begin{equation}
\begin{cases}
\phi \sim \cN(\prior\phi; \prior\Xi ) \\
\Sigma \sim \cIW(\prior S; \prior\nu) \\
\phi \text{ и } \Sigma \text{ независимы }
\end{cases}
\end{equation}


В этом случае можно показать (ссылка karlsson? - но тут нет доказательства), что условные апостериорные распределения имеют вид:

\begin{equation}
\begin{cases}
\phi | \Sigma, Y \sim \cN(\post \phi; \post \Xi ) \\
\Sigma | \phi, Y \sim \cIW(\post S; \post\nu) \\
\end{cases}
\end{equation}
где\footnote{Другой вариант записи для $\post\phi$ имеет вид: $\post \phi = \post \Xi \cdot (\prior \Xi^{-1} \prior \phi + (\Sigma^{-1}\otimes(X'X))\vec{\hat\Phi})$ } 
\begin{align*}
&\post\nu = \prior \nu + T \\
&\post S=\prior S + E'E, \text{ где } E=Y-X\Phi \\
&\post \Xi = (\prior \Xi^{-1} + \Sigma^{-1}\otimes X'X)^{-1} \\
&\post \phi = \post \Xi \cdot (\prior \Xi^{-1} \prior \phi + \vec(X'Y\Sigma^{-1}))
\end{align*}


Гиперпараметры априорного распределения могут быть выбраны точно так же, как и в априорном распределении Миннесоты (см. \eqref{minnesota_exp} и \eqref{minnesota_variance}). При необходимости неинформативное априорное распределение для коэффициентов при детерминированных переменных переменных можно задать, обнулив соответствующее значение в матрице $\prior\Xi^{-1}$. 
Использование произвольной ковариационной матрицы приводит к тому, что исследователю оказываются известны только условные апостериорные распределения для $\phi$ и $\Sigma$. Это обуславливает необходимость использования алгоритма Гиббса для получения реализаций из совместного апостериорного распределения. 

Получить Марковскую цепь, сходящуюся к апостериорному распределению можно, например, так:
\begin{enumerate}
\item Сгенерировать произвольно стартовую матрицу $\Sigma^{[0]}$, например, единичную
\item На $s$-ом шаге сгенерировать очередную итерацию согласно:

\begin{align}
&\phi^{[s]}\sim \cN(\post \phi^{[s-1]}; \post \Xi^{[s-1]}), 
\text{ где }  \post \phi^{[s-1]} \text{ и } \post \Xi^{[s-1]} \text{ рассчитываются через  } \Sigma^{[s-1]}\\
&\Sigma^{[s]} \sim \cIW(\post S^{[s]}; \post\nu), 
\text{ где } \post S^{[s]}  \text{ рассчитываются через } \phi^{[s]}
\end{align}

\item Увеличить $s$ на единицу и перейти к пункту два
\end{enumerate}




\subsection{Соответствие гиперпараметров в разных работах}


\begin{center}
\begin{tabular}{ccccc}
\toprule
DM16 &  CCM15 &  BGR10, BH13 & KK97 &Формула? \\
\midrule
% тут базовые %
$\lambda_{tight}$ & $\lambda_1$ & $\lambda$ & $\sqrt{\pi_1}$ &\\
$\lambda_{kron}$ & $\lambda_2=1$ & $\vartheta=1$ &  $\sqrt{\frac{\pi_2}{\pi_1}}$&\\
$\lambda_{lag}$ & $1$ & $1$ & $0.5$&\\
$\lambda_{const}$  & $\lambda_0$ & $\infty$ &$ \sqrt{\pi_3}$& \\
$\lambda_{exo}$ & NA & NA & NA&\\
\midrule
$\lambda_{sc}$ & $\lambda_3$ & $\tau$ && \\
$\lambda_{io}$ & $\lambda_4$ & NA && \\
\bottomrule
\end{tabular}
\end{center}




\subsection{Выбор гиперпараметров и числа лагов}

Как было показано в работе \cite{demol_al_2008} и подтверждено в других более поздних работах,использование сравнительно большого количества временных рядов требует уменьшения параметра $\lambda_1$ с увеличением размерности выборки, что означает наложение более жесткого априорного распределения. На данный момент в литературе используется два подхода к определению оптимальной величины $\lambda_1$. В своей работе мы используем оба и сравниваем качество прогноза.

\subsection{Метод регуляризации в соответствии с \cite{banbura_al_2010}}

Первый алгоритм был предложен в работе \cite{banbura_al_2010} и он основан на идее о том, что регуляризация должна быть настолько жесткой, чтобы не исключить возможность избыточной параметризации модели, при этом предполагается, что трехмерная VAR - достаточно простая (parsimonious) модель, не содержащая слишком большого количества параметров. Процедура выбора $\lambda$ состоит в том, что что средний внутривыборочный прогноз для реального ВВП и индекса цен тот же самый, как на первой выборке (на которой происходит) оценивание. Т.е. каждая модель регуляризуется до размера простой VAR. При этом  референтной моделью является та, для которой апостериорное распределение не зависит от функции правдоподобия, т. е. для которой $\lambda=0$. Это означает, что дисперсии всех параметров $\phi$ равны нулю, т. е. переменные описываются моделью случайного блуждания (RW) со смещением, $y_{i,t}=c+y_{i,t-1} +\varepsilon_t, i=1,\ldots,m$. Обозначим эту модель индексом 0,  т.к. $\lambda=0$.
Схема выбора $\lambda$ состоит из следующих этапов:
\begin{enumerate}
\item На первом этапе  строятся внутривыборочные однопериодные прогнозы на обучающей выборке и рассчитывается среднеквадратичная ошибка прогноза выпуска $(MSFE_y^{0})$ и инфляции $(MSFE_{\pi}^{0})$.
\item Оценивается трехмерная VAR для $\lambda\to \infty$ \footnote{При $\lambda\to \infty$ оценки BVAR совпадают с оценками VAR методами OLS или ML, т. к. апостериорное распределение параметров в этом случае совпадает с функцией правдоподобия. Считается, что трехмерная VAR содержит достаточно маленькое число параметров, и байесовская регуляризация не требуется.} и рассчитываются среднеквадратичная ошибка прогноза выпуска $(MSFE_y^{\infty})$ и инфляции $(MSFE_{\pi}^{\infty,3})$ и показатель $FIT^{\infty,3}$:
\begin{equation}
FIT^{\infty,3}=\frac{1}{2}\cdot\frac{MSFE_y^{\infty,3}}{MSFE^0_y} + \frac{1}{2}\cdot\frac{MSFE_{\pi}^{\infty,3},}{MSFE^0_{\pi}}
\end{equation}
\item Оцениваются BVAR модели для $m$ переменных и для большого числа различных $\lambda$ рассчитываются среднеквадратичные ошибки прогноза для выпуска $(MSFE_y^{\lambda,m})$ и инфляции $(MSFE_{\pi}^{\lambda,m})$ и показатель $FIT^{{\lambda,m}}$: 
\begin{equation}
FIT^{\lambda,m}=\frac{1}{2}\cdot\frac{MSFE_y^{\lambda,m}}{MSFE^0_y} + \frac{1}{2}\cdot\frac{MSFE_{\pi}^{\lambda,m}}{MSFE^0_{\pi}}
\end{equation}
\item Оптимальное $\lambda$ рассчитывается как значение, при котором минимизируется отклонение $FIT^{\lambda,m}$ от $FIT^{\infty,3}$:
\begin{equation}
\lambda^*_m=\argmin_{\lambda} |FIT^{\lambda, m}-FIT^{\infty,3}| 
\end{equation}
\end{enumerate}
После того как выбрано оптимальное $\lambda$ для каждой модели, происходит построение вневыборочных прогнозов на оценивающей выборке.


\subsection{Метод регуляризации в соответствии с \cite{carriero_al_2012}}
Второй алгоритм предложен в работе (Carriero et al, 2012) и представляет собой выбор такого параметра $\lambda_1$, который бы максимизировал функцию предельной плотности:
\begin{equation}
\lambda^*=\argmax_{\lambda} \ln p(Y)
\end{equation}

При этом функция предельной плотности может быть получена путем интегрирования коэффициентов модели: \footnote{Т.к. интегрирование происходит по  всем коэффициентам, но не по гиперпараметрам априорного распределения ($\lambda_1,\lambda_2,\lambda_3$)  и не по числу лагов $p$, то предельная плотность является функцией $\lambda_j,j=1\ldots 3$ и $p$.}
\begin{equation}
p(Y)=\int p(Y|\phi)p(\phi) d \phi
\end{equation}
Если априорное распределение является нормальным "--- обратным Уишарта, то предельная плотность $p(Y)$ может быть посчитана аналитически (\cite{zellner_1996}; Bauwens et al,1999; Carriero et al,2012):
%\begin{equation}
\begin{multline}
p(Y)=\pi^{-\frac{Tm}{2}}\times +\left |(I+X\prior{\Omega} X')^{-1}\right|^{\frac{N}{2}}\times |\prior{S}|^{\frac{\prior{\nu}}{2}}\times \frac{\Gamma_N(\frac{\prior{\nu}+T}{2})}{\Gamma_N(\frac{\prior{\nu}}{2})}\times \\ \times
\left|\prior{S}+(Y-X\prior{\Phi})'(I+X\prior{\Omega} X')^{-1}(Y-X\prior{\Phi})\right|^{-\frac{\prior{\nu}+T}{2}},\label{marginal_density}
\end{multline}
где $\Gamma_N(\cdot)$ обозначает $N$-мерную гамма функцию.
Выбор числа лагов происходит аналогично путем максимизации по $p$ функции предельной плотности \eqref{marginal_density}: 
\begin{equation}
p^*=\argmax_{p} \ln p(Y)
\end{equation} 

\subsection{Построение прогнозов и сравнение результатов}

	Оценка байесовских VAR с заданным $\lambda$ происходит с «rolling window» по 120 наблюдениям, начиная с января 1996 г и заканчивая апрелем 2015г. (первые 12 наблюдений используются для построения лаговых значений в нескольких наиболее ранних подвыборках. Обозначим последнее наблюдение первой выборки (декабрь 2005 г.) как $T_0$ и последнее доступное наблюдение (апрель 2015г.) за $T_1$, а последнее наблюдение каждой оцениваемой выборки за $T$. Мы строим вневыборочные прогнозы на 1,3,6,9 и 12 месяцев вперед.   т.е. на дату $T+h$, где $h=1,3,6,9, 12$. Таким образом количество прогнозов на один шаг оказывается на единицу меньше, чем прогнозов на два шага и т.д. \footnote{Альтернативный метод состоит в том, чтобы посчитать одинаковое количество прогнозов для каждого горизонта $h$, начиная с $T_0+11$, однако это означает определенную потерю информации о прогнозах.}
Для каждой модели ($m$)  и каждого прогнозного окна ($h$) рассчитываются вневыборочные среднеквадратичные  ошибки прогноза для индикатора деловой активности $(OMSFE_{ip,h}^{\lambda,m})$, ИПЦ $(OMSFE_{p,h}^{\lambda,m})$ и процентной ставки $(OMSFE_{r,h}^{\lambda,m})$:
\begin{equation}
OMSFE_{var,h}^{\lambda,m}=\frac{1}{2+T_1-T_0-h}\sum_{T=T_0-1}^{T_1} (y_{var,T+h|T}^{\lambda,m}-y_{var,T+h|T})^2,\quad var=\lbrace{ip,p,r\rbrace}
\end{equation}

\subsection{Особенности кодирования}

При практической реализации алгоритма Гиббса или Монте-Карло часто
приходится обращать положительно определённые симметричные матрицы.
Для этого можно использовать следующий способ:


\begin{enumerate}

\item Получить разложение Холецкого для заданной матрицы $A$

\[
A=U'U,
\]

где $U$ --- верхнетреугольная матрица

\item Обратить матрицу $U$. Существуют специальные алгоритмы обращения
верхнетреугольных матриц.

\item Получить $A^{-1}$ по формуле

\[
A^{-1}=U^{-1}U^{-1\prime}
\]

\end{enumerate}


Однако даже данный способ сопряжён с численными трудностями, если
обращаемая матрица плохо обусловлена. Например, при большом количестве
эндогенных переменных и большом количестве лагов матрица $X'X$
является плохо обусловленной и для неё может быть численно трудно
получить разложение Холецкого. В таком случае мы использовали
псевдообратную матрицу Мура-Пенроуза.


\section{Данные}

Для расчетов мы используем 24 временных ряда с января 1995г. по апрель 2015г. Границы выборки обусловлены доступностью данных, исходная выборка содержит 244 наблюдения. Полный список взятых временных рядов указан в Приложении 2. После устранения сезонности в рядах, демонстрирующих сезонные колебания, мы логарифмируем всех ряды кроме процентной ставки. Далее происходит проверка на стационарность, для чего используются ADF и KPSS тесты. Такая проверка необходима, для того чтобы определить матожидание априорного  распределения для параметров $\Phi_1$. Следуя методологии других работ, посвященных прогнозированию с помощью BVAR (например ()()), мы назначаем $(\Phi_1)_{ii}=1$  для  нестационарных рядов и $(\Phi_1)_{ii}=0$ для стационарных. 
На втором этапе мы оцениваем три rolling VAR модели для разного набора переменных и строим по ним прогнозы. Период  оценивания составляет всегда 120 месяцев, Прогноз строится на 1, 3, 6 и 12 месяцев.  Мы строим VAR  для разного количества переменных: для 3, 6 и 24\footnote{Обычная VAR для сравнения качества прогноза строится для 3 и 6 переменных}. Модель с тремя переменными рассматривается как наиболее простая модель, содержащая только самые важные  переменные: показатель деловой активности (индекс промышленного производства),  индекс цен (подсчитанной с помощью ИПЦ) и  инструмента монетарной политики (в качестве прокси для которого мы берем процентную ставку межбанковского рынка). Модель с шестью переменными специфицируется по аналогии со многими монетарными моделями, использовавшимися для структурного анализа различных экономик (Sims, 1992; Kim and Roubini (2000); Bjornland (2008); Uhlig and Scholl (2008) и включает  в дополнение к уже указанным переменным валютный курс, денежный агрегат М2 и цены на нефть (последний показатель включен для отражения экспортоориентированности российской экономики). В модель с 24 переменными строится с использованием всех доступных временных рядов.




 
При определении параметра жесткости распределения по методу \cite{banbura_al_2010} необходимо выделить период, на котором происходит определение $\lambda$. Мы определяем $\lambda$ на самом раннем доступном промежутке: с января 1996г. по декабрь 2005г (первые 12 месяцев используются в качестве лаговых значений переменных для этой регрессии). Количество лагов определяется путем минимизации информационных критериев для обычной VAR на той же самой подвыборке. (что делать с вар по 24 рядам??) Далее количество лагов и параметр жесткости фиксируются и используются для построения прогнозов на всех остальных подвыборках. 
При определении параметра жесткости путем максимизации предельной плотности данных (marginal data density) при оценке каждой BVAR происходит совместный поиск на сетке по $\lambda$ и по $p$, и выбираются такие значения, для которых \eqref{marginal_density} максимально.  







\newpage

\printbibliography
\newpage
%\begin{appendices}
\begin{landscape}
\section*{Приложения}
\subsection*{Данные}
\begin{center}
\begin{table}[h!]
\begin{tabular}{lccr}
\toprule
Название временного ряда& Тип данных &  База (если есть) & Источник \\
\midrule
Индекс промышленного производства & Базисный индекс & 2010 & IFS \\
Индекс потребительских цен & Базисный индекс & 2010 & IFS \\
Индекс занятости в промышленности & Базисный индекс & 2010 & IFS \\
Процентная ставка межбанковского рынка & В процентах годовых &  & IFS \\
Процентная ставка по кредитам & В процентах годовых &  & IFS \\
Индекс реальных денежных доходов & Базисный индекс & январь 1992 & ФСГС\\
Уровень безработицы & В процентах &  & IFS \\
Индекс цен на нефть марки Brent & Базисный индекс & 2010 & IFS \\
Индекс цен производителей & Цепной индекс &  & IFS \\
Ввод в действие новых жилых домов & В тыс.кв.м. &  & ФСГС \\
Индекс реальных инвестиций в основной капитал& Базисный индекс & январь 1994 & ЦАД \\
Индекс реальных зарплат & Базисный индекс & январь 1993 & ФСГС \\
Денежный агрегат М2 & в млрд. руб.  &  & ЦБ \\
Реальный эффективный валютный курс & Базисный индекс & 2010 & IFS \\
Цена натурального газа & Долл. за млн. БТЕ & 2010 & IFS \\
Международные резервы за исключением золота & Млрд. долл. &  & IFS \\
Номинальный валютный курс & руб. за долл. &  & IFS \\
Заявленная потребность в работниках & Тыс. чел. &  & ЦАД \\
Индекс реального объема сельхозпроизводства & Базисный индекс & январь 1993 & ЦАД\\
Индекс реального объема розничной торговли & Базисный индекс & январь 1994 & ЦАД\\
Сальдо консолидированного бюджета &  &  & ЦАД \\
Экспорт товаров & млн. долл. &  & IFS \\
Импорт товаров & млн. долл. &  & IFS \\
\end{tabular}
\end{table}
\end{center}
\end{landscape}
%\end{appendices}
\newpage

\subsection*{Доступные реализации кода}

\begin{center}
\begin{tabular}{p{4cm}cccccc}
\toprule
Источник&  Среда&Min &  Conj N-IW & Ind N-IW & SoC &IO\\
\midrule
% тут базовые %
Carriero&Matlab&-&?&-&+&+ \\
Blake Mumtaz&Matlab&-&+&+&+&+ \\
Koop Korobilis&Matlab  & + & +  & + &- &- \\
Zha&Matlab&&?&&+&+\\
Le Sage&Matlab&?&&&&\\
Sims&Matlab&&?&?&+&+\\
Canova&Matlab&&?&&&\\
BMR&R& + & - & + & - & -  \\
MSBVAR&R & - & + & - & + & +  \\
bvarr&R  & + & + & + & + & +  \\
Sims&R&&?&?&+&+\\
Встроенная функция&EViews& + & + & - &+ &+ \\
Встроенная функция&Gauss& ? & ?& ?& ?&?  \\
Встроенная функция&Dynare& ? & +& ?& +&+  \\
\bottomrule
\end{tabular}
\end{center}
\begin{enumerate}
\item Carriero: Дамми-наблюдения вводятся как для conj NIW, при этом делает Gibbs sampling, при этом $\post\Phi$ одно и то же, а $\Sigma$ пересчитывается на каждом шаге в зависимости от предыдущего $\Phi$. При плохо обусловленной матрице $X'X$ используется псевдо-обратная. Имеется странный хак, повторно генерирующий VAR коэффициенты, если собственные числа за пределами единичного круга. \url{http://cremfi.econ.qmul.ac.uk/efp/info.php} 
\item Blake Mumtaz: Называет Миннесотой Ind NIW.  Код для conj NIW построен так же, как у Carriero через Gibbs Sampling: разница - добавляет 2 один. строки для дамми при опредеделеннии коэффициента при константе. \url{http://www.bankofengland.co.uk/education/Pages/ccbs/technical_handbooks/techbook4.aspx}
\item Koop Korobilis: Код крайне негибкий. Без правки кода нет возможности прогнозировать больше чем на один шаг, базовые conj N-IW и ind N-IW априорное распределения не содержит  гиперпараметров и задаются через фиксированные матрицы.\url{https://sites.google.com/site/dimitriskorobilis/matlab}\\
\item Zha: в соответствии со статьей ограничения накладываются на структурную форму VAR, соответственно дается другая интерпретация для некоторых гиперпараметров
\item Sims Недостаточно подробное описание. Нужно прочитать весь код, чтобы что-то модифицировать.\url{http://sims.princeton.edu/yftp/VARtools/}
\item BMR: Симуляции реализованы в C++.  оценивает DSGE.  Реализует TVPBVAR. Отличная документация. \url{ http://bayes.squarespace.com/bmr/} 
\item MSBVAR: Симуляции реализованы на фортране и C++. Реализует также марковские BVAR с переключением.  \url{https://cran.r-project.org/web/packages/MSBVAR/ }
\item bvarr: Сопряжённое нормальное-Уишарта реализовано максимально гибко.  При плохо обусловленной матрице $X'X$ используется псевдо-обратная. Код для Миннесоты и независимого нормального Уишарта является переводом кода Koops-Korobilis и потому крайне негибкий. \url{https://github.com/bdemeshev/bvarr }
\item Eviews: «EViews just ignores the fact that the coefficients were estimated using Bayesian methods, and forecasts the same way as it would a classical model» . Коэффициент $\lambda_{kron}$ равен по умолчанию 0,99 и не может быть изменен. Большую свободу представляет прямое задание ковариационной матрицы. Матожидания коэффициентов при первых лагах могут быть заданы только одинаковыми для всех переменных. 
\item Gauss: 
\item Dynare: Функция позиционируется как BVAR \`a la Sims. Оценка возможна только <<комплектом>>, но можно поменять априорное распределение для априорного распределения ковариационной матрицы  \url{http://www.dynare.org/}
\end{enumerate}

\end{document} 
