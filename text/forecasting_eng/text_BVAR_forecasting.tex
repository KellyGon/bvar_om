% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[12pt]{article} % use larger type; default would be 10pt


%%% PAGE DIMENSIONS
\usepackage[left=3cm,right=1.5cm,top=2cm,bottom=2cm]{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or\ldots .
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information


% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{url}
\usepackage{caption}
\usepackage{soulutf8}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another\ldots


%%% HEADERS & FOOTERS
%\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
%\pagestyle{fancy} % options: empty , plain , fancy
\setcounter{page}{3}



%%My Additional Packages
\usepackage{mathtext}          % русские буквы в формулах
\usepackage[T2A]{fontenc}            % внутренняя кодировка  TeX
\usepackage[utf8]{inputenc}         % кодировка исходного текста
% \usepackage{cmap}          % русский поиск в pdf
%\usepackage[english,russian]{babel} % локализация и переносы
\usepackage[russian,english]{babel} % локализация и переносы

%\usepackage[english]% локализация и переносы

\usepackage{amsmath} % Математические окружения AMS
\usepackage{amsfonts} % Шрифты AMS
\usepackage{amssymb} % Символы AMS
\usepackage{graphicx} % Вставить pdf- или png-файлы

\usepackage{euscript} % Красивый шрифт

\usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

\usepackage[colorinlistoftodos]{todonotes} % [colorinlistoftodos,prependcaption,textsize=tiny]

\usepackage{indentfirst} % Отступ в первом абзаце.
\usepackage{pdflscape} %Переворачивает страницы, удобно для широких таблиц


\usepackage{footnote} %сноски в таблицах
\makesavenoteenv{tabular}
\makesavenoteenv{table}

%\usepackage[backend=biber, style=authoryear, citestyle=authoryear]{biblatex}
\usepackage[backend=biber, style=bwl-FU, citestyle=bwl-FU]{biblatex}
%\usepackage{microtype}

\addbibresource{bibliobase2.bib}

\usepackage{wrapfig} % Обтекание рисунков текстом

\usepackage{hyperref} % Гиперссылки

\DeclareMathOperator{\etr}{etr}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Var}{\mathbb{V}\mathrm{ar}}
\DeclareMathOperator{\chol}{chol}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cIW}{\mathcal{IW}}
\newcommand{\lag}{\EuScript{L}}

\newcommand{\prior}{\underline}
\newcommand{\post}{\overline}

\let\vec\relax
\DeclareMathOperator{\vec}{vec}
\hyphenation{re-si-du-als}

%\usepackage[toc,page]{appendix}


%\renewcommand{\appendixtocname}{Приложения}
%\renewcommand{\appendixpagename}{Приложения}
%\renewcommand{\appendixname}{Приложение}
%%% END Article customizations

%%% The "real" document content comes below\ldots

\title{Forecasting Russian macroeconomic indicators with BVAR\footnote{The authors are grateful to Svetlana Bryzgalova (Stanford University), Dean Fantazzini (MSU),  Hubert Kempf (ENS de Cachan), Sergey Pekarsky (NRU HSE) and all participants of \textit{II International Conference: Modern Econometric Tools and Applications.} for valuable comments, questions and discussion. All remaining errors are authors’ responsibility.}\footnote{The study was implemented in the framework of the Basic Research Program at the National Research University Higher School of Economics in 2015.
}}
\author{Boris Demeshev,$^{1}$   Oxana Malakhovskaya$^{1}$\\
\\
\normalsize{$^{1}$National Research University Higher School of Economics}\\
}    
\date{}          

\begin{document}


%\maketitle



%\begin{abstract}
%This paper evaluates the forecast performance of Bayesian vector autoregressions (BVARs) on Russian data. We estimate BVARs of different sizes and compare the accuracy of their out-of-sample forecasts with those obtained with unrestricted vector autoregressions and random walk with drift. We show that many Russian macroeconomic indicators can be forecast by BVARs more accurately than by competing models. However, contrary to several other studies, we do not confirm that the relative forecast error monotonically decreases with increasing the cross-sectional dimension of the sample. In half of those cases where a BVAR appears to be the most accurate model, a small-dimensional BVAR outperforms its high-dimensional counterpart.
%\end{abstract}

%\smallskip
%\noindent JEL-Classification: C11, C13, C53\\
%\smallskip
%\noindent Keywords: VAR, forecasting, Bayesian estimation

\section{Introduction}

Accurate macroeconomic forecasts are extremely important for policy making. Central banks and government bodies monitor a large set of macroeconomic indicators to determine the policy (\cite{beckner_1996}, \cite{bernanke_boivin_2003}). Therefore, a model used for forecasting must be suitable for data-rich samples because large models might outperform low-dimensional ones by taking into account more potentially relevant information. This explains the recent resurgence in interest from academics, central bankers and private sector experts for macroeconomic forecasting in a data-rich environment.

In this paper, we forecast Russian macroeconomic indicators with Bayesian vector autoregressions (BVARs) of different sizes. Our goal is twofold.  First, we compare the forecast accuracy of BVAR with that of unrestricted vector autoregressions (VARs) and random walk with drift models for 23 important macroeconomic indicators. Second, we  question whether a high-dimensional model always outperforms a low-dimensional one in terms of forecasting accuracy.

For the last 30 years, VARs introduced by \cite{sims_1980} have become a  widely-used tool for forecasting. However, unrestricted VARs bear the risk of over-parametrization even for samples of moderate size. This risk stems from the fact that the number of  parameters to be estimated  increases nonlinearly with the number of equations. For this reason, in economic applications unrestricted VARs usually contain  only up to eight variables, and this may potentially lead  to the loss of some relevant information and undermine the forecast accuracy. 

To deal with a data-rich environment researchers modify VARs and impose restrictions on the covariance structure. One strand of the literature focuses on  dynamic factor models (DFM, \cite{forni_al_2000} and \cite{stock_watson_2002})and Panel VARs and Global VARs (PVARs, GVARs, \cite{pesaran_al_2004} and \cite{dees_guntner_2014}).  DFM are based on the idea that a relatively small set of indices extracted from a high-dimensional set of variables can summarize the information from this set.
These factors are treated as variables in a VAR model either separately or in conjunction with several time series from the original information set in a factor-augmented VAR (FAVAR) model. For data sets with a panel structure a suitable choice is a PVAR or a GVAR with shrinkage done by exclusion, exogeneity or homogeneity restrictions.

Another method of shrinkage is the Bayesian one and we follow this approach. The shrinkage is done by imposing restrictions on the parameters in the form of prior distributions.  While BVARs in a low-dimensional space were widely used for macroeconomic analysis, their use for data-rich environments was limited until recently. The reason was a general agreement that Bayesian shrinkage is insufficient to solve the over-parametrization problem in high cross-sectional dimension samples.

However, in their influential paper,  \cite{demol_al_2008} show that Bayesian methods can be successfully applied to a data-rich environment if the degree of shrinkage is set relative to the cross-sectional dimension of the sample. \cite{banbura_al_2010} confirm and develop this assertion for BVARs applied to a large set of US time-series. Their main result is that high-dimensional models have better forecasting performance than small-dimensional models and even FAVARs. They also show that accurate forecasts can be already obtained using a medium-sized BVAR (20 variables in their case).

Several authors have recently shown that, in terms of forecasting accuracy, medium and large BVARs outperform their low-dimensional counterparts. For example,  \cite{beauchemin_zaman_2011} present a medium BVAR with a good forecasting performance applied to the US data.  \cite{bloor_matheson_2010} compare univariate autoregresions (ARs), unrestricted VARs and BVARs and show evidence that high-dimensional BVARs, in general demonstrate better forecasting performance. \cite{koop_2013} demonstrates that  high-dimensional BVARs outperform factor models in terms of forecasting performance. Moreover, he argues that more complicated priors than those that are usually applied may not lead to more precise forecasts. \cite{alessandri_mumtaz_2014} underline the importance of financial factors for an accurate forecast of output and inflation, especially <<for predicting <<tail>> macroeconomic outcomes>>.
\cite{carriero_al_2015} study some characteristics of  BVARs and find those providing the most accurate forecasts.

Our analysis delivers two important results. First, we show that most Russian macroeconomic indicators in our sample can be forecast by BVARs more accurately than by competing models. However, contrary to other studies (for example, \cite{bloor_matheson_2010}, \cite{banbura_al_2010})   we do not confirm that relative forecast error monotonically decreases with the dimension of the sample. In almost half of those cases where a BVAR is the most accurate model, a small-dimensional BVAR outperforms its high-dimensional counterpart. 
 
The paper is structured as follows. Section 2 presents our model and the prior distribution we apply. In Section 3 we describe our sample and the data transformations we use. Section 4 contains the results and their interpretation.  Section 5 concludes.


\section{Model}

\subsection{BVAR}
Let $y_{it}$ be variables\footnote{For the convenience of the reader, all the notations are also shown in Appendix 1.} stacked in a  $m\times 1$ vector $y_{t}=(y_{1t},y_{2t},\ldots, y_{mt})'$. The reduced form VAR can be written as:
\begin{equation}
y_t =\Phi_{const}+ \Phi_1 y_{t-1} + \Phi_2 y_{t-2} +\ldots + \Phi_p y_{t-p} + \varepsilon_t,\quad \varepsilon_t\sim \cN(0,\Sigma)\label{reduced_var}
\end{equation}
where $\Phi_{const}=(c_1,\ldots ,c_m)'$ is a $m\times 1$  vector of constants, $\Phi_l$ are autoregression   $m\times m$ - dimensional matrices where $l=1, \ldots, p$. Vector $\varepsilon_t$ is a $m$-dimensional vector of errors with  covariance matrix $\E\varepsilon_t \varepsilon _t'=\Sigma$, and is uncorrelated with regressors.
By grouping parameter matrices into one matrix  $\Phi=[\Phi_1 \ldots \Phi_p \; \Phi_{const}]'$ % здесь нужно транспонирование!
and defining new vector $x_t=[ y'_{t-1}\;  \ldots  \; y'_{t-p} \; 1]'$, the equation \eqref{reduced_var} can be written in a more compact form:

\begin{equation}
y_t=\Phi' x_t+\varepsilon_t
\end{equation}
If the variables and shocks are grouped in the following way: $Y=[y_1, y_2,\ldots, y_T]'$, $X=[x_1, x_2,\ldots, x_T]'$, $E=[\varepsilon_1, \varepsilon_2,\ldots, \varepsilon_T]'$, the VAR can be written as:
\begin{equation}
Y=X\Phi+E\label{var}
\end{equation}

%We will also use vector $\phi=\vec{\Phi}$ of  $km\times 1$ dimension in what follows.

The Bayesian estimate combines a likelihood function $L(Y|\Phi, \Sigma)$ with a prior distribution $p(\Phi, \Sigma)$ and results in a posterior distribution of parameters $p(\Phi, \Sigma|Y)$: %To find the distribution, a corollary from the rule of Bayes is applied:
\begin{equation}
p(\Phi, \Sigma|Y)\propto p(\Phi,\Sigma) L(Y|\Phi,\Sigma)
\end{equation}
%where $ L(Y|\Phi,\Sigma)$ is a likelihood function.


\subsection{Сonjugate normal --- inverse Wishart prior}

Our benchmark model for estimation and forecasting purposes is a BVAR with a conjugate normal --- inverse Wishart prior.  The prior can be written as: 

\begin{equation}
\begin{cases} \label{conjugate_prior}
\Sigma\sim \cIW(\prior S, \prior \nu) \\
\Phi|\Sigma\sim \cN (\prior \Phi, \Sigma\otimes\prior \Omega)
\end{cases} 
\end{equation}

The prior mean of the coefficient matrices is  written with a $k\times m$ matrix  $\prior \Phi = \E (\Phi)$, where $\prior\Phi=[\prior\Phi_1 \ldots \prior\Phi_p \; \prior\Phi_{const}]'$.% и $\prior \phi = \vec{ \prior \Phi}$. 
The matrices $\prior\Phi_l$ are defined as follows:


\begin{equation}
(\prior\Phi_l)_{ij}=
\begin{cases}
\delta_i\; i=j, l=1;\\
0,\;\text{ otherwise }
\end{cases}\label{minnesota_exp}
\end{equation}

A matrix $\prior \Omega$  is assumed to be diagonal and it depends on several hyperparameters:
\begin{gather}
\prior \Omega=\diag\lbrace{\prior \Omega_{lag=1},\ldots, \prior \Omega_{lag=p},\prior \Omega_{const} \rbrace}\\ 
(\prior \Omega_{lag=l})_{jj} 
=\left(\frac{\lambda}{l^{\lambda_{lag}}\hat\sigma_j}\right)^2
\quad
\prior \Omega_{const}=\lambda_{const}^2 \label{prior_omega2}
\end{gather}

The  hyperparameters have the following interpretation: $\lambda$ determines the overall tightness of the prior and it is responsible for the relative weight of the prior with respect to the information incorporated in the data, $\lambda_{lag}$  controls the velocity of the decrease of the prior variance with increasing the lag length, and $\lambda_{const}$ governs the relative tightness of the prior for the constant terms.
 
The scale matrix $\prior S$ is diagonal and its non-zero elements assure that the mean of $\Sigma$ is equal to the fixed covariance matrix of the standard Minnesota prior: 

\begin{equation}
(\prior S)_{ii}= (\prior \nu- m- 1) \hat\sigma^2_{i}
\end{equation}
The scale parameter $\sigma^2_{i}$ is usually set to be equal to the variance estimate of residuals in a univariate $AR$ model.
The choice of degrees of freedom of inverse Wishart distribution  $\prior\nu$  greater than or equal to than $\max\lbrace m+2, m+2h-T\rbrace$ guarantees the existence of the  prior variance of the regression parameters and the posterior variances of the forecasts at horizon $h$ (\cite{kadiyala_karlsson_1997}).

It is possible to show that the posterior distribution formed by combining this prior distribution with a likelihood function is also normal --- inverse Wishart (see, for example, \cite{zellner_1996}):
\begin{equation}
\begin{cases}
\Sigma|Y \sim \cIW(\post S, \post \nu) \\
\Phi|\Sigma,Y\sim \cN (\post \Phi, \Sigma\otimes\post \Omega) 
\end{cases}
\end{equation}
with the following parameters:
%где
%\footnote{
%Эквивалентная формула (карлсон p15) для $\post S$, 
%$\post S=\prior S +\hat E'\hat E + (\prior \Phi - \hat \Phi)'(\prior \Omega $+ (X'X)^{-1})^{-1}(\prior \Phi - \hat \Phi)$}
\begin{align*}
\post\nu &=\prior \nu+T\\
\post{\Omega}&=(\prior \Omega^{-1}+X'X)^{-1}\\
\post \Phi&=\post{\Omega}\cdot (\prior \Omega^{-1}\prior \Phi+X'Y)\\
\begin{split}
\post S&=\prior S +\hat E'\hat E+\hat \Phi'
 X'X \hat \Phi \\&+\prior \Phi'\prior\Omega^{-1}\prior \Phi-\post \Phi'\post\Omega^{-1}\post \Phi
 \end{split}\\
\hat\Phi&=(X'X)^{-1}X'Y\\%\quad\text{и }
\hat E&=Y-X\hat\Phi
\end{align*}

There is a popular alternative approach to calculate hyperparameters of the posterior distribution. 
We set $\prior S$ and $\prior \Omega^{-1}$ to be zero matrices and to compensate the difference we add supplementary observations into  $X$ and  $Y$ matrices according to:

\begin{equation}\label{star_sample}
Y^*=\begin{bmatrix}
Y^{NIW} \\
Y 
\end{bmatrix}
\quad
X^*=\begin{bmatrix}
X^{NIW} \\
X 
\end{bmatrix},
\end{equation}
where matrices $Y^{NIW}$ and $X^{NIW}$ are defined as follows\footnote{The similar formulae provided in \cite{banbura_al_2010}, \cite{berg_henzel_2013} can be regarded as special cases of  \eqref{NIW_observations} for $\lambda_{lag}=1$ и $\lambda_{const}\to\infty$.}:
%Можно показать, что подсчет параметров апостериорного распределения  соответствии с указанной схемой эквивалентен добавлению дополнительных искусственных наблюдений по следующей схеме:
%\footnote{The similar formulae provided in \cite{banbura_al_2010}, \cite{berg_henzel_2013} for defining Normal-inverted Wishart distribution can be regarded as special cases of  \eqref{NIW_observations} for $\lambda_{lag}=1$ и $\lambda_{const}\to\infty$.}:

%По аналогии с  работами  \cite{banbura_al_2010}, \cite{berg_henzel_2013} соответствующее априорное распределение вводится путем добавления искусственных наблюдений
\begin{equation}
Y^{NIW}=\begin{bmatrix}
\frac{\diag(\delta_1\sigma_1,\ldots, \delta_m\sigma_m)}{\lambda}\\
0_{m(p-1)\times m}\\
\diag(\sigma_1,\ldots,\sigma_m)\\
0_{1\times m}
\end{bmatrix}
\quad
%
X^{NIW}=\begin{bmatrix}
\frac{\diag(1,2^{\lambda_{lag}},\ldots, p^{\lambda_{lag}})\otimes \diag(\sigma_1,\ldots,\sigma_m)}{\lambda} & 0_{mp\times 1} \\
0_{m\times mp}&0_{m\times 1}\\
0_{1\times mp}&\frac{1}{\lambda_{const}}
\end{bmatrix}\label{NIW_observations}
\end{equation}

This method permits the calculation $\post \Phi$ as an OLS estimate of the regression of $Y^*$ on $X^*$: $\post \Phi=(X^{*\prime}X^*)^{-1} X^{*\prime}Y^*$ and $\post S$ as a sum of the squared residuals for this regression: 
$\post S =(Y^* - \post \Phi X^*)'(Y^* - \post \Phi X^*)$.

%При этом математические ожидания и дисперсии априорного распределения параметров могут быть заданы по тому же принципу, что и в априорном распределении Миннесоты (см. \eqref{minnesota_exp}-\eqref{minnesota_variance}).
%При этом априорные распределения параметров, указанные в \eqref{minnesota_exp}-\eqref{minnesota_variance}), предполагают наличие  нулевой априорной корреляции между коэффициентами.


\subsection{Prior modifications}

\cite{doan_al_1984} and \cite{sims_1993} propose complementing this prior distribution with additional information in form of two other priors. This modification reflects the belief that time series may have unit roots and cointegration relations. These elements in the prior allow avoiding an unreasonably large share of the variation in the data which is accounted for by deterministic components (\cite{sims_1993}).


A sum-of-coefficients prior was introduced by \cite{doan_al_1984}. If all the time-series in a sample have a unit root, this information can be taken into account with a prior where a sum of all the lag parameters for each dependent variable is equal to one (\cite{robertson_tallman_1999}, \cite{blake_mumtaz_2012}). In other words, when the mean of the lagged values of a variable is at a certain level, this level is a good forecast for future observations of this dependent variable.  
We implement this prior by combining the dataset given in \ref{star_sample} with artificial dummy-observations according to the following scheme:
%\begin{gather*}
%y_d(i,j)=\begin{cases}
%\post y_{0i}/\lambda_3, \quad \text{если }i=j\\
%0 \quad \text{в обратном случае}
%\end{cases}
%x_d(i,s)=\begin{cases}
%\post y_{0i}/\lambda_3,\quad \text{если }i=j, s<km\\
%0 \quad \text{в обратном случае,}
%\end{cases}
%\end{gather*}
%где $i,j=1,\ldots,m$, $s=1, \ldots, km$. Когда $\lambda_3\to 0$, модель стремится к виду, предполагающему запись в разностях, т.е. единичных корней становится столько же, сколько переменных, и нет коинтеграции.


\begin{gather}
Y^{SC}=\frac{1}{\lambda_{sc}}\begin{bmatrix}\diag(\delta_1\mu_1,\ldots,\delta_m\mu_m)\end{bmatrix}\\
X^{SC}=\frac{1}{\lambda_{sc}}\begin{bmatrix}(1_{1\times p})\otimes \diag(\delta_1\mu_1,\ldots,\delta_m\mu_m) &0_{m\times 1}\end{bmatrix},
\end{gather}

\noindent where $(1_{1\times p})$  is a unitary $[1\times p]$ vector, $\mu_i$ is $i$-th component of vector   $\mu$, which contains the average values of initial observations of all variables in the sample\footnote{Some authors calculate $\mu$  using the average values of all observations in a sample, so that   $\mu=\frac{1}{T}\sum_{t=1}^T y_t$ (\cite{banbura_al_2010} and \cite{carriero_al_2015}). However, following \cite{sims_zha_1998} we calculate $\mu$ using only initial $p$ observations.}: $\mu=\frac{1}{p}\sum_{t=1}^p y_t$.  

The dummy initial observation prior proposed by  \cite{sims_1993} expresses the belief that the variables have a common stochastic trend. Only one observation is added so that the values of all variables are equal to the average value of initial observations $\mu_i$ normalized to a scale coefficient $\lambda_{io}$. Therefore, this extra observation is defined as follows:
\begin{gather}
Y^{IO}=\frac{1}{\lambda_{io}}\begin{bmatrix}
\delta_1\mu_1,\ldots,\delta_m\mu_m
\end{bmatrix}\\
X^{IO}=\frac{1}{\lambda_{io}}\begin{bmatrix}
(1_{1\times p})\otimes (\delta_1\mu_1,\ldots,\delta_m\mu_m) &1
\end{bmatrix},
\end{gather}

This prior distribution reflects the belief that the average value for a variable is a linear combination of average values of all the other variables.
%%% Непонятно, нужны ли здесть дельты. У Карьеро их нет, но у него и в sum-of-coefficients их нет, а у Банбуры в sum-of-coefficients есть, f initiфд ibservation они не вводят
%\begin{gather*}
%y_d(j)=\begin{cases}\post y_{0i}/\lambda_4 \qquad
%\post y_{0i}/\lambda_4\quad \text{если }s<km\\
%1/\lambda_4 \quad \text{в обратном случае,}
%\end{cases}
%\end{gather*}
%где $j=1,\ldots m, s=1,\ldots km$. 

The hyperparameter $\lambda_{io}$ controls the tightness of this prior. When $\lambda_{io}\to 0 $, the model implies that either all variables are stationary with the mean equal to sample mean of the initial observations or non-stationary without drift and cointegrated. 


\subsection{Choice of tightness hyperparameter: the algorithm of shrinkage}

As shown by \cite{demol_al_2008} and confirmed in several other recent studies, a sample with a larger cross-sectional dimension requires a lower  $\lambda$, so the prior must be tighter for a larger sample than for a smaller one. In this paper, we use the approach introduced by \cite{banbura_al_2010} to determine the optimal $\lambda$ for every model. 

This algorithm is based on the idea that the shrinkage should be sufficiently tight  to avoid over-parametrization. Moreover, it is assumed that a three-variable unrestricted VAR is parsimonious enough and does not require any additional shrinkage. This implies that the hyperparameter $\lambda$ can be chosen so that the model has the same in-sample fit as a three-variable VAR.  In other words, a BVAR model of any dimension is shrunk to the size of a small unrestricted VAR.  A detailed description of the procedure is laid out below. We denote  the actual value of a variable $var$ at moment $T+h$ by $y_{var,T+h}$, and  a forecast of the variable $var$ at moment $T$ for a  horizon $h$ in a model with $m$ variables and an overall tightness parameter $\lambda$ by  $y_{var,T+h|T}^{\lambda,m}$. The algorithm for choosing $\lambda$ has the following steps. 
\begin{enumerate}
\item We make in-sample one-period forecasts with BVAR on a training sample and calculate the mean squared forecast error for the set  of $M$ variables of central interest\footnote{Our benchmark set of variables of central interest $(\EuScript{M})$ includes the industrial production index, consumer price index and interbank interest rate so that $M=3$. As a  robustness check we excluded the interest rate from this set and there was almost no change in the vector of the optimal $\lambda$.}:
\begin{equation}
MSFE_{var,1}^{\lambda,m}=\frac{1}{T_0 - p}\sum_{t=p}^{T_0-1} \left(y_{var,t+1|t}^{\lambda,m}-y_{var,t+1}\right)^2,
%\quad var=\lbrace{ip,p,r\rbrace},
\end{equation}
\noindent where  the BVAR coefficients are obtained using the training sample: $t=p+1,\ldots, T_0$ and $T_0$ is the last observation of the training sample: $T_0=p+120$. 
\item In a similar way we calculate one-period forecasts according to the random walk model\footnote{We normalize MSFE for the BVAR and VAR models by MSFE obtained with the random walk model to take into account the different scales of the series.
We use a superscript 0 for the random walk model as random walk may be considered as a special case of BVAR if $\lambda=0$ and $\delta_i=1, i=1,\ldots,k$.} for the same variables $\left(MSFE_{var,1}^0\right)$ and a new indicator $FIT^{\lambda,m}$:
% We estimate models for  $m$ variables with many different $\lambda$ and calculate MSF errors  for output  $(MSFE_y^{\lambda,m})$ and prices $(MSFE_{\pi}^{\lambda,m})$  as well as $FIT^{{\lambda,m}}$: 
\begin{equation}
FIT^{\lambda,m}=\frac{1}{M} \sum_{var\in \EuScript{M}} \frac{MSFE_{var,1}^{\lambda,m}}{MSFE_{var,1}^{0}}
%FIT^{\lambda,m}=\frac{1}{2}\cdot\frac{MSFE_y^{\lambda,m}}{MSFE^0_y} + \frac{1}{2}\cdot\frac{MSFE_{\pi}^{\lambda,m}}{MSFE^0_{\pi}}
\end{equation}
\item We estimate  VARs for the same set of $M$ variables of interest\footnote{We denote all results from VAR by a superscript $\infty$ as unrestricted VAR is a special case of BVAR with
$\lambda\to \infty$. In this case the posterior coincides with the likelihood function.} and calculate MSFEs and an indicator $FIT^{\infty,M}$:
\begin{equation}
FIT^{\infty,M}=\frac{1}{M} \sum_{var\in \EuScript{M}} \frac{MSFE_{var,1}^{\infty,M}}{MSFE_{var,1}^{0}}
%FIT^{\infty,M}=\frac{1}{2}\cdot\frac{MSFE_y^{\infty,3}}{MSFE^0_y} + \frac{1}%{2}\cdot\frac{MSFE_{\pi}^{\infty,3},}{MSFE^0_{\pi}}
\end{equation}

\item The optimal lambda is the value minimizing the difference between $FIT^{\lambda,m}$ and $FIT^{\infty,M}$:
\begin{equation}
\lambda^*_m=\argmin_{\lambda} |FIT^{\lambda, m}-FIT^{\infty,M}| 
\end{equation}
\end{enumerate}
After the optimal $\lambda$ is chosen for every $m$, we keep it fixed  and make out-of-sample forecasts on the evaluation sample.  


\subsection{Out-of-sample forecasting}

	We estimate BVARs with the optimal  $\lambda$ on <<rolling window>> containing 120 observations, starting from observation $p+1$ and continuing until March 2015. The first $p$ observations are used as a pre-sample and the subsample $[p+1, p+120]$ is a training sample to determine the optimal  $\lambda$ on a grid. We denote the last available observation as  $T_1$, and the last observation of each evaluation subsample as $\tau$. The number of forecasts is equal to $T_1 - T_0 -h +1 $ where $h$ is the forecasting horizon ($h = 1,3,6,9,12)$.  Therefore, the number of one-period forecasts is greater than the number of three-period forecasts by two, etc.\footnote{An alternative method is to calculate an equal number of forecasts for each horizon $h$, starting  from $T_0+12$. However this implies the loss of some information about the forecasts and we do not proceed with this method here.}
For every model $m$  and forecasting horizon $h$ we calculate the out-of-sample MSFE  for all $m$ variables included in the model:
\begin{equation}
OMSFE_{var,h}^{\lambda,m}=\frac{1}{T_1-T_0-h+1}\sum_{\tau=T_0}^{T_1-h} \\\left(y_{var,\tau+h|\tau}^{\lambda,m}-y_{var,\tau+h}\right)^2,
%\quad var=\lbrace{ip,p,r\rbrace},
\end{equation}
%where $y_{var,T+h}$ - actual values of a variable $var$ at$T+h$,  $y_{var,T+h|T}^{\lambda,m}$ - is a forecast of a variable $var$ at a horizon $h$ in a model with $m$ variables and an overall tightness parameter $\lambda$.

Then we calculate the MSFE of out-of-sample forecasts obtained with random walk with drift $(OMSFE_{var,h}^{0})$ and unrestricted VAR $(OMSFE_{var,h}^{\infty,m})$:
	 
%We measure the out-of-sample forecast accuracy in terms of mean squared forecast error: 
%\begin{equation}
%OMSFE_{var,h}^{\lambda,m}=\frac{1}{2+T_1-T_0-h}\sum_{\tau=T_0-1}^{T_1} (y_{var,\tau+h|\tau}^{\lambda,m}-y_{var,\tau+h|\tau})^2
%\end{equation}

\begin{gather}
OMSFE_{var,h}^{\infty,m}=\frac{1}{T_1-T_0-h+1}\sum_{\tau=T_0}^{T_1-h} \left(y_{var,\tau+h|\tau}^{\infty,m}-y_{var,\tau+h}\right)^2\\
OMSFE_{var,h}^{0}=\frac{1}{T_1-T_0-h+1}\sum_{\tau=T_0}^{T_1-h} \left(y_{var,\tau+h|\tau}^{0}-y_{var,\tau+h}\right)^2,
\end{gather}

To compare the forecast accuracy of different models we report the relative MSFE, that is, the ratio of the MSFE of the model in question by the MSFE of a reference model (random walk in our case): 

\begin{equation}\label{rmsfe}
RMSFE_{var}=\frac{OMSFE_{var,h}^{\lambda,m}}{OMSFE_{var,h}^0}
\end{equation}
%where $var$ is any variable in the dataset


\section{Data and Estimations}

Our dataset consists  of 23 time series running from January 1996 to April 2015. Our sample containing 232 observations is limited by data availability. The full list of the series and their sources is displayed in Appendix 2. We seasonally adjust data which demonstrate seasonal fluctuations with TRAMO/SEATS option in EViews and apply logarithms to the series, with the exception of those already expressed in rates. 

We estimate models of different cross-sectional dimension. The industrial production index, CPI and interbank interest rate are forecast with three-variable, six-variable and 23-variable models.  Monetary aggregate, the real effective exchange rate and the oil price index are forecast with four-variable, six-variable and 23-variable models. All the other series are forecast  with four-variable, seven-variable and 23-variable models. For all models with dimension less than eight we estimate both unrestricted VARs and BVARs. We estimate only a BVAR on the sample with 23 variables.
The three-variable VAR is the simplest specification that can be justified by a textbook version of a New Keynesian model.  
A model with six variables is specified in line with many monetary models used previously for the structural analysis of different economies (\cite{sims_1992}, \cite{kim_roubini_2000}, \cite{bjornland_2008}, \cite{scholl_uhlig_2008}) and it contains the real effective exchange rate, monetary aggregate M2, and the oil price index in addition to three variables included in the smallest VAR. The oil price index is used as a variable in the model to reflect the belief that oil price index is an important explanatory factor for the other variables in the sample as Russia has a petroleum export-based economy. To forecast variables outside of these core sets we estimate four-variable and seven-variable VARs containing three or six-variable samples described above plus an additional variable of interest. We include all available time series in our 23-variable model. In totally, after the optimal $\lambda$ is chosen, we estimate 79 models. In a compact form the models used for forecasting are presented in  Table \ref{tab:models}. 
For VARs and BVARs we take all possible lags from 1 to 12. 


\begin{table}
\begin{center}
\caption{List of models and variable sets}
\begin{tabular}{p{2.5cm}l}
\toprule
VAR3/BVAR3&$Y=\lbrace IP, CPI, R \rbrace$\\
VAR4/BVAR4 &$Y=\lbrace IP, CPI, R, Z\rbrace$ \\
VAR6/BVAR6& $Y=\lbrace IP, CPI, R, M2, REER, OPI \rbrace$ \\
VAR7/BVAR7&$Y=\lbrace IP, CPI, R, M2, REER, OPI, W \rbrace$\\
BVAR23&$Y$ includes all 23 variables from the dataset\\
\bottomrule
\end{tabular}
\vspace{5mm}
\caption*{where $IP$ is the industrial production index, $CPI$ is the consumer price index, $R$ is the nominal interbank rate, $M2$ is the monetary aggregate M2, $REER$ is the real effective exchange rate, $OPI$ is the Brent oil price index. $Z$ is any variable from the dataset besides $IP$, $CPI$ and $R$.  $W$ is any variable from the dataset besides $IP$, $CPI$, $R$, $M2$,$REER$, and $OPI$.}
\label{tab:models}
\end{center}
\end{table}

\section{Results}

For every variable and every forecasting horizon we find a model with the lowest RMSFE. We compare 60 specifications for each variable and each forecasting horizon as we have 5 models (a VAR and a BVAR with 3 or 4 variables, a VAR and a BVAR with 6 or 7 variables, and a BVAR with 23 variables) and 12 lags for each of them. 
We visualize our results with color tables (Figures \ref{fig:rmsfe1}-\ref{fig:rmsfe2} ). 
The two tables in these Figures differ by the hyperparameter sets used for the BVAR priors. For models depicted in Figure 1  we take $\delta_i =1$ for nonstationary series and $\delta_i =0.5$ for stationary series while constructing the prior. We use the KPSS test to split the series into two groups. The parameters $\sigma_i$ are taken to be equal to the standard deviations of the residuals in the univariate $AR(p)$ model. This hyperparapeter set will be referenced as set A in what follows. Figure 2 is related to BVARs with the prior determined by the univariate $AR(1)$ model. We take $\delta_i$ as equal to the OLS estimates of the first lag parameter and  $\sigma_i$ as equal to the standard deviations of residuals in AR(1) models. This set will be referenced as set B. 


\begin{figure}[!h]
\begin{center}
\includegraphics[scale=0.80]{hyper3.jpg}
\caption{RMSFE of the best forecasting accuracy models, parameter set A: $\sigma_i$ are std of $AR(p)$ residuals, $\delta_i=1$ for nonstationary series, $\delta_i=0.5$ for stationary series}
\label{fig:rmsfe1}
\end{center}
\end{figure}

\begin{figure}[!h]
%\begin{figure}
\begin{center}
\includegraphics[scale=0.80]{hyper4.jpg}
\caption{RMSFE of the best forecasting accuracy models, parameter set B: $\sigma_i$ are std of $AR(1)$ residuals, $\delta_i=1$ are first lag $AR(1)$ estimates}
\label{fig:rmsfe2}
\end{center}
\end{figure}


The color of a cell corresponds to the model that appears to outperform the others in terms of forecasting accuracy for a given variable and a given forecasting horizon. Most of cells are green (either light green or bright green) reflecting that a BVAR  provides the most accurate forecast for the corresponding  variables and forecasting horizons. An unrestricted VAR gives the most accurate forecast for variables and horizons indicated by yellow and orange cells. The procedure for choosing $\lambda$ is such that the BVAR and the unrestricted VAR  necessarily coincide  for the smallest sample (3 or 4 variables). This explains why orange represents both of these models.  Blue means that neither BVAR nor VAR beat the random walk in terms of forecast accuracy.

The forecast accuracy is measured with RMSFE calculated according to (\ref{rmsfe}) and is also shown in Figures \ref{fig:rmsfe1}-\ref{fig:rmsfe2}. 
The numbers less than one indicate that the a VAR or a BVAR model provides a better forecast than the random walk and the smaller the number is, the more accurate the forecasts are relative to the random walk.  We see that in most cases we have at least one model that provides a forecast much better than the reference model. 

Despite different prior parameter sets, the two tables are very similar both in terms of the best forecasting models and  the relative accuracy  with respect to the random walk. 

We  interpret our results as follows. First, for many variables and forecasting horizons, BVARs outperforms the random walk and unrestricted VARs. Out of the 115 forecasting cases highlighted in the paper (23 variables times 5 forecasting horizons) BVARs appear to be best in terms of forecast accuracy in 71 cases for the prior hyperparameter set A and in 77 cases for the prior hyperparameter set B.  There are variables in our sample that are forecast more accurately by BVARs for all forecasting horizons we try  (such as employment, import and lending rate). For several variables BVARs are the best option for the shortest horizons (for example, monetary aggregate M2 and the real effective exchange rate). On the contrary, for the agricultural production index a BVAR model has the lowest forecast error only for a one-year horizon.  

Second, among all cases where BVARs show their forecasting accuracy, a high-dimensional BVAR is the best option  in about half of the cases (35 of 71 for set A and 39 of 77 for set B).  In other cases it is beaten by a low-dimensional BVAR (6 or 7 variables). 

Third, for some variables and some forecasting horizons neither unrestricted VARs nor BVARs outperform  the random walk. For example, in all specifications we consider the nominal exchange rate cannot be forecast by either VARs or BVARs better than by the random walk, and it is a long-held consensus in economics remounting to \cite{meese_rogoff_1983}. However, we question another wide-spread belief that the price of oil is a random walk process. We show that the oil price index can be forecasted by BVARs much better than by the random walk and the result is robust for different prior settings.

\section{Conclusion}

This paper evaluates the forecasting performance of BVARs on Russian data. We estimate BVARs of different sizes and compare the accuracy of their out-of-sample forecasts with those obtained with unrestricted VARs and random walk.   Our sample consists of 23 variables and we forecast at 5 different horizons up to 12 months. We show that for the majority of the variables  BVARs  outperform the competing models in terms of forecasting accuracy. 
However, we cannot confirm the conclusion drawn in some other studies (for example, \cite{bloor_matheson_2010}, \cite{banbura_al_2010}), where Bayesian methods were applied to data from developed countries, claiming that high-dimensional BVARs forecast better than low-dimensional models. Our results implies that a 23-variable BVAR performs most accurately in only about a half of cases where a BVAR is considered as a better forecasting tool with respect to its competitors. For the rest of those cases a BVAR with a relatively small size (6 or 7 variables in our case)  can outperform a 23-variable BVAR in terms of forecasting accuracy.


\newpage

\printbibliography

\newpage

%\begin{appendices}

\section*{Appendices}


\subsection*{Appendix 1. Table of notations}

\begin{center}
\begin{tabular}{ccp{6cm}l}
\toprule
Notation & Dimension &  Description & Formula \\
\midrule
$p$ & scalar & number of lags & \\
$m$ & scalar & number of endogenous variables & \\
$d$ & scalar & number of exogenous variables & \\
$k$ & scalar & number of parameters in an equation & $k=mp+d$ \\
$h$ & scalar & forecast horizon &  \\
\midrule
$y_t$ & $m \times 1$ &  vector of endogenous variables   & $y_t=\Phi' x_t+\varepsilon_t$ \\
$x_t$ & $k \times 1$ & vector of all regressors & $x_t=[ y'_{t-1} \ldots  y'_{t-p} \; z'_t ]'$ \\
$\varepsilon_t$ & $m \times 1$ & vector of random errors & $y_t=\Phi' x_t+\varepsilon_t$\\
$Y$ & $T \times m$ & all endogenous variables & $Y=[y_1, y_2,\ldots, y_T]'$ \\
$X$ & $T \times k$ & matrix of regressors& $X=[x_1, x_2,\ldots, x_T]'$ \\
$E$ & $T \times m$ & matrix of errors & $E=[\varepsilon_1, \varepsilon_2,\ldots, \varepsilon_T]'$ \\
\midrule
$\Phi_1$, \ldots & $m \times m$ & coefficients of VAR & $y_t= \Phi_1 y_{t-1} + \ldots + \Phi_{const} +\varepsilon_t$ \\
$\Phi_{const}$ & $m \times d$ & matrix of constants & $y_t= \Phi_1 y_{t-1} + \ldots + \Phi_{const} +\varepsilon_t$\\
$\Phi$ & $k \times m$ & grouping of matrices $\Phi_1$, \ldots & $\Phi=[ \Phi_1 \ldots \Phi_p \; \Phi_{ex}]'$ \\
\midrule % здесь к независимому без кронекерова произведения
$\prior \Phi$ & $k \times m$ & prior mean $\Phi$ & $\prior \Phi = \E (\Phi)$ \\
$\post \Phi$ & $k \times m$ & posterior mean $\Phi$ & $\post \Phi=\post{\Omega}\cdot (\prior \Omega^{-1}\prior \Phi+X'Y)$\\
$\prior \nu $ & scalar & prior degrees of freedom& $\nu\ge\max\lbrace m+2, m+2h-T\rbrace$ \\
$\post \nu $ & scalar & posterior degrees of freedom & $\post \nu = T + \prior \nu$ \\
$\prior S $ & $m \times m$& prior scale matrix& $\prior S= (\prior \nu- m- 1)\diag(\sigma^2_{1},\ldots,\sigma^2_{m})$\\
$\post S $ & $m \times m$ & posterior scale matrix&  $\post S=\prior S +\hat E'\hat E+\hat \Phi'
 X'X \hat \Phi+$\\&&&$ +\prior \Phi'\prior\Omega^{-1}\prior \Phi-\post \Phi'\post\Omega^{-1}\post \Phi$\\
\midrule % здесь к сопряженному с кронекеровым произведением
$\prior \Omega$ & $k \times k$ & matrix of prior scaling coefficients of covariance matrix $\Phi$& $\prior \Xi = \Sigma \otimes \prior \Omega$ \\
$\post \Omega$ & $k \times k$ & matrix of posterior scaling coefficients of covariance matrix $\Phi$&  $\post \Omega = (\prior\Omega^{-1}+ X'X)^{-1}$,  $\post \Xi = \Sigma \otimes \post \Omega$\\
$\Sigma$ & $m \times m$ &covariance matrix of errors& $\E\varepsilon_t \varepsilon _t'=\Sigma$\\
\bottomrule
\end{tabular}
\end{center}
\newpage
%\begin{landscape}
\subsection*{Appendix 2. Data}
\begin{center}
\begin{table}[h!]
\begin{tabular}{lccr}
\toprule
Name of serie& Type of series &  Base period (if any) & Source \\
\midrule
Industrial production index & base index & 2010 & IFS \\
Consumer price index & base index  & 2010 & IFS \\
Employment in manufacturing index & base index  & 2010 & IFS \\
Interbank interest rate & perc. per ann. &  & IFS \\
Lending interest rate & perc. per ann.&  & IFS \\
Real income index & base index  & 01:1992 & FSSS\\
Unemployment rate & percent &  & IFS \\
Crude oil (Brent) price index & base index  & 2010 & IFS \\
Producer price index & chain index &  & IFS \\
New houses commissioning & thous. of sq. met. &  & FSSS\\
Real fixed investment  index & base index  & 01:1994 & UAESD\\
Real wage rates index & base index  & 01:1993 & FSSS \\
Monetary aggregate М2 & bln. rub.  &  & CBR \\
Real effective exchange rate & base index  & 2010 & IFS \\
Natural gas price & US\$ for bln BTU & 2010 & IFS \\
International reserves excluding gold & Bln US\$ &  & IFS \\
Nominal exchange rate & rub. per US\$. &  & IFS \\
Declared need in workers  & thous. of people &  & UAESD \\
Real agricultural production index & base index  & 01:1993 & UAESD\\
Real retail output index & base index  & 01:1994 & UAESD\\
Total government budgetary balance & bln. rub. &  & UAESD \\
Export of goods & mln US\$ &  & IFS \\
Import of goods &  mln US\$ &  & IFS \\
\bottomrule
\end{tabular}
\vspace{5mm}\\
IFS - International Financial Statistics of IMF \url{http://www.imf.org/en/Data}\\
FSSS - Federal State Statistical Service\url{http://www.gks.ru/}\\
CBR - Central Bank of Russia \url{http://cbr.ru/}\\
UAESD - United Archive of Economic and Sociological Data \url{http://sophist.hse.ru/rstat/}
\end{table}
\end{center}



%\end{landscape}
%\end{appendices}
\end{document} 
