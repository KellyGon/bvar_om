\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\newcommand{\cN}{\mathcal{N}}
\newcommand{\cW}{\mathcal{W}}


\begin{document}

\section{Байесовские VAR}
%\section{Байесовские SVAR и SVECM}

%\subsection{Отличия байесовского подхода}

%\subsection{Байесовские VAR}

На английском языке существует большое количество статей, посвященных теории и практике оценивания байесовских VAR-моделей (далее BVAR). На русском же языке ситуация с BVAR-моделями просто ужасна. Поэтому в данном разделе мы попытаемся одновременно изложить и теорию, и практику оценивания. 

Практику оценивания BVAR моделей мы изложим в статистической среде R. Здесь нам будут полезны два пакета:

\begin{enumerate}
\item Пакет MSBVAR, выложенный на официальном репозитории, устанавливается командой:
<<eval=FALSE>>=
install_packages("MSBVAR")
@
\item Пакет bvarr, выложенные на github, устанавливается почти также легко:
<<eval=FALSE>>=
library("devtools")
install_github("bdemeshev/bvarr")
@
\end{enumerate}

Загружаем установленные пакеты и делаем доступным набор данных 
<<warning=FALSE, message=FALSE>>=
library("MSBVAR")
library("bvarr")
library("ggplot2")
data(Yraw)
@

Рассмотрим стандартную VAR(p) модель

\begin{equation}
y_t=a_0+\sum_{j=1}^p A_j y_{t-j} + \varepsilon_t 
\end{equation}
% + B w_t

Векторы $y_t$, $a_0$ и $\varepsilon_t$ имеют размер $M\times 1$, матрицы $A_i$ --- размер $M\times M$. % В модель могут входить экзогенные регрессоры $w_t$. 
% Если имеется $k_w$ экзогенных регрессоров, то $w_t$ --- это вектор размера $k_w \times 1$, а $B$ --- матрица размера $M\times k_w$.

Вектор $\varepsilon_t$ имеет многомерное нормальное распределение, $\mathcal{N}(0,\Sigma)$.

В байесовском подходе параметры $\Sigma$, $a_0$, $A_1$, \ldots, $A_p$ предполагаются случайными. При наличии априорных предположений об их распределении можно вывести апостериорные распределения.

Для описания априорных распределений нам будут удобны дополнительные обозначения. А именно, все параметры модели, кроме $\Sigma$, мы <<запихнём>> в матрицу $A$ и в вектор $\alpha$:

\begin{enumerate}
\item Матрица $A$. Справа от столбца $a_0$ выпишем друг за другом матрицы $A_1$, $A_2$, \ldots, $A_p$, а затем транспонируем полученный результат:
\[
A=(a_0 \, A_1\, A_2\, \ldots \, A_p)'
\]
Матрица $A$ имеет размер $(1+pM) \times M$. Величину $1+pM$ назовём буквой $K$.

\item Вектор $\alpha$. Расположим столбцы матрицы $A$ друг под другом:
\[
\alpha = vec(A)
\]
Вектор $\alpha$ состоит из $M+pM^2$ элементов.
\end{enumerate}

Поместим все переменные правой части в вектор-строку $X_t$, т.е. это вектор, который начинается с единицы, за ней находится вектор $y_{t-1}$, за ним --- вектор $y_{t-2}$ и так далее, $X_t=(1, y_{t-1}', y_{t-2}', \ldots, y_{t-p}')$.  Вектор $X_t$ имеет размеры $1\times K$, где $K=1+pM$.

Тогда VAR(p) модель можно также записать в виде:

\begin{equation}
y_t'=X_t A+\varepsilon_t'
\end{equation}
% !!! это правильная версия (1) в "Manual to accompany Matlab ..." by KoKo
% должно быть транспонирование над \varepsilon_t
% их Y_t=y_t'

или 

\begin{equation}
y_t=(I_M \otimes X_t) \alpha + \varepsilon_t
\end{equation}
где $I_M$ --- единичная матрица размера $M\times M$. 

Также можно записать нашу модель без индекса $t$.  Расположим вектор-строки $y_1'$, $y_2'$, \ldots, $y_T'$ одну под другой, полученную матрицу назовем буквой $Y$. Аналогично, с помощью $E$ обозначим вектор-строки $\varepsilon_1$, \ldots, $\varepsilon_T$, записанные одна под другой. В матрицу $X$ поместим строки $X_1$, $X_2$, \ldots, $ X_T$ одну под одной.

\begin{equation}
Y=XA+E
\end{equation}

Вектор $y=vec(Y)$ --- это вектор-столбец зависимых переменных, в котором сначала идут все наблюдения за первой эндогенной переменной, потом все наблюдения за второй эндогенной переменной и т.д. Аналогично, $\varepsilon=vec(E)$.

\begin{equation}
y=(I_M \otimes X) \alpha + \varepsilon
\end{equation}

где вектор ошибок $\varepsilon$ имеет многомерное нормальное распределение, $\varepsilon \sim \mathcal{N}(0,\Sigma \otimes I_T)$.





Для удобства введем обозначение $Z_t=(I_M \otimes X_t)$, а также обозначения для МНК-оценок вектора $\alpha$, матрицы $A$ и матрицы $\Sigma$. Здесь мы фактически предполагаем, что оцениваются отдельные модели для каждой компоненты $y_t$:

\begin{equation}
\hat{\alpha}=\left(\sum_t Z_t'Z_t\right)^{-1}\left(\sum Z_t' y_t\right) 
\end{equation}

\begin{equation}
\hat{A}=(X'X)^{-1}X'Y 
\end{equation}

Суммы квадратов остатков:
\begin{equation}
\hat{S}=(Y-X\hat{A})'(Y-X\hat{A})
\end{equation}

\begin{equation}
\hat{\Sigma} = \frac{\hat{S}}{T-K}
\end{equation}


Существует несколько популярных вариантов априорного распределение неизвестных параметров:

\begin{itemize}

\item Априорное распределение Джеффри:

Параметры $\alpha$ и $\Sigma$ предполагаются априорно независимыми,

\begin{equation}
\begin{array}{l}
p(\alpha) \propto 1 \\
p(\Sigma) \propto |\Sigma|^{-(M+1)/2}
\end{array}
\end{equation}

При таких предположениях апостериорное распределение имеет вид:


\begin{equation}
\begin{array}{l}
\alpha | \Sigma, y \sim \cN(\hat{a},\Sigma) \\
\Sigma | y \sim \cW^{-1} (\hat{S},T-K)
\end{array}
\end{equation}


В данном подходе исследователю вообще не требуется подбирать гиперпараметры априорного распределения. С одной стороны --- это плохо, нет возможности посмотреть, как меняются апостериорные выводы при смене априорного распределения. С другой стороны --- хорошо, так как подбирать ничего не нужно.

<<cache=TRUE>>=
model <- bvar(Yraw, prior="diffuse")
bvar.imp.plot(model)
@

Можно рассматривать как предельный частный случай сопряженного нормального-Уишарта априорного распределения, при $\underline{\alpha}=0$, ???? $\underline{\nu}=0$ ??? уточнить

\item Сопряженное нормальное-Уишарта априорное распределение

Здесь параметры $\alpha$ и $\Sigma$ предполагаются априорно зависимыми:


\begin{equation}
\begin{array}{l}
\Sigma \sim \cW^{-1} (\underline{S},\underline{\nu}) \\
\alpha | \Sigma \sim \cN( \underline{\alpha}, \Sigma \otimes \underline{V} ) 
\end{array}
\end{equation}


В этом случае условные апостериорные распределения будут иметь вид

\begin{equation}
\begin{array}{l}
\alpha | \Sigma, y \sim \cN(\overline{\alpha}, \Sigma \otimes \overline{V}) \\
\Sigma | y \sim \cW^{-1} (\overline{S},\overline{\nu})
\end{array}
\end{equation}
где $\overline{V}=(\underline{V}^{-1}+X'X)^{-1}$, $\overline{\alpha}=vec(\overline{A})$, $\overline{A}=\overline{V}(\underline{V}^{-1}\underline{A}+X'X\hat{A})$, $\overline{\nu}=T+\underline{\nu}$, $\overline{S}=S+\underline{S}+\hat{A}'X'X\hat{A}+\underline{A}'\underline{V}^{-1}\underline{A}-\overline{A}'(\underline{V}^{-1}+X'X)\overline{A}$


Мы видим, что априорное распределение описывается огромным количеством гиперпараметров, $\underline{S}$, $\underline{\nu}$, $\underline{\alpha}$ и $\underline{V}$. Обычно выбирают $\underline{\alpha}=0$, $\underline{\nu}=M+1$, $\underline{S}=I_M$ и матрицу $\underline{V}$ пропорциональной единичной матрице, например, $\underline{V}=10 I_K$


<<cache=TRUE>>=
model <- bvar(Yraw, prior="conjugate")
bvar.imp.plot(model)
@



\item Minnesota (Litterman)  Априорное распределение Литтермана (или априорное распределение Миннесоты)

Данная модель является гремучей смесью байесовского и частотного подходов. 
От байесовского подхода взято предположение об априорном распределении параметра $\alpha$:

\begin{equation}
\alpha \sim \cN(\underline{\alpha} , \underline{U})
\end{equation}

От частотного подхода взята трактовка матрицы $\Sigma$ как матрицы неизвестных констант, и поэтому используется некоторая оценка $\hat{\Sigma}$. На практике используют либо $\hat{\Sigma}=\hat{S}/T$, либо $\hat{\Sigma}=(s_1^2,s_2^2,\ldots,s_n^2)$, где $s_i^2$ --- оценка дисперсии ошибки $i$-го уравнения, полученная с помощью обычного МНК.

У данного распределения огромное количество гиперпараметров! Чтобы исследователю не приходилось специфицировать их все, используются следующие ограничения. 

Во-первых, предполагается, что матрица $\underline{A}'$ имеет вид:


\begin{equation}
\underline{A}'=[0_{M\times 1}, \, \rho \cdot I_M, \,  0_{M\times M}, \,  0_{M\times M}, \,  \ldots, \,  0_{M\times M}]
\end{equation}
то есть в математических ожиданиях мы априорно предполагаем, что каждый ряд является $AR(1)$ процессом с коэффициентом авторегрессии равным $\rho$. Например, можно взять $\rho=0.9$, что означает, что исследуемые ряды похожи на ряды с единичным корнем.

Во-вторых, описание матрицы $\underline{U}$ сокращается до нескольких гиперпараметров. Матрица $\underline{U}$ предполагается диагональной. Рассмотрим её диагональный элемент $u$, соответствующий уравнению для $y_{ti}$.

\begin{equation}
u=\begin{cases}
a_1/r^2, \text{ для коэффициента при } y_{(t-r),i} \\
\frac{a_2 \sigma_i^2}{r^2 \sigma_j^2}, \text{ для коэффициента при } y_{(t-r),j} \text{ и } j\neq i \\
a_3 \sigma_i^2 \text{ для коэффициента при константе и экзогенных переменных }
\end{cases}
\end{equation}

Скаляры $a_1$, $a_2$, $a_3$ выбирает исследователь. Убывание $u$ с ростом $r$ отражает идею, что ближайшие по времени прошлые значения переменных оказывают более сильное влияние, чем отдаленные во времени. Выбор $a_1>a_2$ будет означать, что наиболее сильное влияние на эндогенную переменную оказывают её собственные лаговые значения, а не лаговые значения других эндогенных переменных. На практике при выборе констант $\sigma_i^2$ снова примешивают частотную философию и полагают $\sigma_i^2=s_i^2$.

Таким образом, задача исследователя сводится к выбору четырех априорных гиперпараметров, $a_1$, $a_2$, $a_3$, $\rho$. Величина $a_1$ ограничивает величину коэффициентов при собственных лагах эндогенной переменной, величина $a_2$ --- коэффициента при лагах других эндогенных переменных, $a_3$ --- коэффициента при экзогенных переменных. Обычно значения $a_1$, $a_2$ небольшие, от нуля до единицы, а $a_3$ может быть довольно большим, т.к. априорно коэффициент при экзогенной переменной, например, при константе, может быть довольно большим. Выбор $a_1=a_2=0.5$ и $a_3=100$ может оказаться разумным. Параметр $\rho$ отвечает за априорное мнение исследователя о близости процессов к процессам с единичным корнем.

Существует много вариантов спецификации формулы для диагональных элементов матрицы $\underline{U}$. Например, (Kadiyala
and Karlsson, 1997) делят на $r$, а не на $r^2$. А Canova, 2007 предлагает более общий вид формулы,
\begin{equation}
u=\begin{cases}
a_1/d(r), \text{ для коэффициента при } y_{(t-r),i} \\
\frac{a_1 a_2 \sigma_j^2}{d(r) \sigma_i^2}, \text{ для коэффициента при } y_{(t-r),j} \text{ и } j\neq i \\
a_1 a_3 \text{ для коэффициента при константе и экзогенных переменных }
\end{cases}
\end{equation}
где в качестве $d(r)$ берут обычно либо степенную функцию $d(r)=r^{a_4}$, либо показательную, $d(r)=r^{1-a_4}$.



<<cache=TRUE>>=
model <- bvar(Yraw, prior="minnesota")
bvar.imp.plot(model)
@




\item Независимое нормальное-Уишарта априорное распределение

Параметры $\alpha$ и $\Sigma$ предполагаются независимыми,


\begin{equation}
\alpha \sim \cN(\underline{\alpha} , \underline{U}) \\
\Sigma \sim \cW^{-1} (\underline{S},\underline{\nu})
\end{equation}


Условные апостериорные распределения будут иметь вид
\begin{equation}
\alpha | y, \Sigma \sim \cN(\overline{\alpha} , \overline{U}) \\
\Sigma | y, \alpha \sim \cW^{-1} (\overline{S},\overline{\nu})
\end{equation}
где $\overline{\alpha}=\overline{U}(\underline{U}\underline{\alpha}+
\sum_{i=1}^T Z_t'\Sigma^{-1}y_t)$ и $\overline{U}=(\underline{U}+\sum_{i=1}^T Z_t'\Sigma^{-1}Z_t)^{-1}$, $\overline{\nu}=T+\underline{\nu}$, $\overline{S}=\underline{S}+\sum_{t=1}^T(y_t-Z_t\alpha)(y_t-Z_t\alpha)'$.

Один из возможных вариантов, взять $\underline{\alpha}=0$, $\underline{U}$ пропорционально единичной матрицы, так чтобы допускать значения коэффициентов из $\alpha$ в широком диапазоне, $\underline{\nu}=M+1$ и $\underline{S}=I_M$.


<<cache=TRUE>>=
model <- bvar(Yraw, prior="independent")
bvar.imp.plot(model)
@



\item Априорное нормальное распределение, спецификация Симс-Жа

Является еще более гремучей, чем модель Литтермана, смесью байесовского и частотного подхода. Помимо смеси двух философий здесь используется добавление искусственных наблюдений.

От байесовского подхода взято предположение об априорном распределении параметра $\alpha$:

\begin{equation}
\alpha \sim \cN(\underline{\alpha} , \underline{U})
\end{equation}

От частотного подхода взята трактовка матрицы $\Sigma$ как матрицы неизвестных констант,
её оценка $\hat{\Sigma}$ получается путем построения оценивания обычной VAR(p) модели с константой.

Изначально имеется  $T$ наблюдений, из которых $p$ стартовых наблюдений пропадает, так как у них невозможно посчитать лаговые значения порядка $p$. То есть к реальным $T-p$ наблюдения Симс и Жа добавляют в модель  $m+1$ искусственное наблюдение в начале временного ряда по следующей схеме:

$Y_{dummy}=\begin{pmatrix}
\mu_5\bar{y}_{1} & 0 & 0 & \ldots & 0\\
0 & \mu_5\bar{y}_{2} & 0 & \ldots  & 0\\
\ldots & & & & \\
0 & 0  & \ldots & 0 & \mu_5\bar{y}_{m}\\
\mu_6\bar{y}_1 & \mu_6\bar{y}_2 &  \ldots &  & \mu_6\bar{y}_m \\
\end{pmatrix}$

А соответствующий вектор переменных правой части можно в виде блоков записать как
$X_{dummy}=\begin{pmatrix}
c & Y_{dummy} & Y_{dummy} & \ldots & Y_{dummy} 
\end{pmatrix}$,
где $c$ --- вектор-столбец, $c=\begin{pmatrix}
0 & 0 & \ldots & 0 & \mu_6
\end{pmatrix}'$.

Большое значение коэффициента $\mu_5$ означает склонность исследователя считать, что при равенстве лаговых значений некоторой переменной её среднему $\bar{y}_i$, хорошим прогнозом будущего значения этой переменной также будет число $\bar{y}_i$. Используя $\mu_5 \to \infty$ исследователь предполагает, что каждый ряд является процессом с единичным корнем, а коинтеграция отсутствует.

Большое значение коэффициента $\mu_6$ означает склонность исследователя считать, что при равенстве лаговых значений всех переменных их средним $\bar{y}_i$, хорошим прогнозом будущего значения  переменных также будут средние $\bar{y}_i$. Используя $\mu_5 \to \infty$ исследователь предполагает, что либо каждый ряд является стационарным процессом, либо некоторые из рядов являются процессами с единичным корнем, коинтеграция при этом допускается.




Вектор $\underline{\alpha}$ определяется полностью аналогично модели Литтермана, то есть так чтобы матрица $\underline{A}$ приняла вид 
\begin{equation}
\underline{A}'=[0_{M\times 1}, \, \rho \cdot I_M, \,  0_{M\times M}, \,  0_{M\times M}, \,  \ldots, \,  0_{M\times M}]
\end{equation}

Матрица $\underline{U}$ предполагается диагональной, её диагональный элемент $u$, соответствующий уравнению для $y_{ti}$ определяется как:

\begin{equation}
u=\begin{cases}
\frac{\lambda_0 \lambda_1}{r^{\lambda_3} \sigma_j^2}, \text{ для коэффициента при } y_{(t-r),j} \text{ и любых } j \\
\lambda_0 \lambda_4 \text{ для коэффициента при константе } \\
\lambda_0 \lambda_5 \text{ для коэффициента при прочих экзогенных переменных }
\end{cases}
\end{equation}
где $\lambda_0 \in [0;1]$, а остальные гиперпараметры $\lambda_i$ и $\mu_j$ больше неотрицательны.


Величины $\lambda_2$, $\mu_1$, \ldots, $\mu_4$ в модели Симс-Жа не используются. Такой стиль обозначений выбран для большей сопоставимостью с моделью Литтермана.

Кратко гиперпараметры модели Симс-Жа можно описать так:


\begin{tabular}{ccc}
Коэффициент & Диапазон & Смысл \\ 
\hline 
$\lambda_0$ & $[0;1]$ & Масштаб ковариационной матрицы \\
$\lambda_1$ &  & Стандартное отклонение от $A_1$ \\
$\lambda_3$ &  & Скорость убывания дисперсии при росте лага \\
$\lambda_4$ &  & Масштаб для констант \\
$\lambda_5$ &  & Масштаб для коэффициентов при экзогенных переменных \\
$\mu_5$ &  & Компонента суммы коэффициентов  \\
$\mu_6$ &  & Компонента исходного дамми-наблюдения \\
\hline 
\end{tabular} 

<<cache=TRUE>>=
model <- bvar(Yraw, prior="independent")
bvar.imp.plot(model)
@


Спецификацией Симс-Жа можно воспользоваться и в независимом нормальном-Уишарта априорном распределении. 

<<cache=TRUE>>=
fit.BVAR <- szbvar(ts(Yraw), p=4,
                   lambda0=0.6, lambda1=0.1,
                   lambda3=2, lambda4=0.25, lambda5=0, mu5=0,
                   mu6=0, qm=4, posterior.fit=FALSE)
posterior.impulses <- mc.irf(fit.BVAR, nsteps=10, draws=5000)
plot(posterior.impulses) 
@

\end{itemize}

За пределами данного обзора остались 
\begin{itemize}
\item Модели VAR с параметрами меняющимися во времени
\item Модели  VAR со стохастическим отбором параметров
\end{itemize}




%\subsection{Байесовские SVAR}



\end{document}